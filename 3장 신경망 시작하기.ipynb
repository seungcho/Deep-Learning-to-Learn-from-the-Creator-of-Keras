{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3장 신경망 시작하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망이 가장 많이 사용되는 세 종류의 문제인 이진 분류, 다중 분류, 스칼라 값을 예측하는 회귀에 배운 것들을 적용한다.**\n",
    "\n",
    "> - **영화 리뷰를 긍정 또는 부정으로 분류하기(이진 분류)**\n",
    "\n",
    "> - **신문 기사 토픽으로 분류하기(다중 분류)**\n",
    "\n",
    "> - **부동산 데이터를 바탕으로 주택가격을 예측하기(회귀)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 신경망의 구조**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망 훈련에는 다음 요소들이 관련**\n",
    "\n",
    "> - **네트워크(또는 모델)를 구성하는 층: 연속된 층으로 구성된 네트워크가 입력 데이터에 예측으로 매핑한다.**\n",
    "\n",
    "> - **입력 데이터와 그에 상응하는 타깃**\n",
    "\n",
    "> - **학습에 사용할 피드백 신호를 정의하는 손실 함수: 손실함수는 예측과 타깃을 비교하여 네트워크의 예측이 기댓값에 얼마나 잘 맞는지 측정하는 손실 값을 만든다.**\n",
    "\n",
    "> - **학습 진행 방식을 결정하는 옵티마이저: 옵티마이저는 손실 값을 사용하여 네트워크 가중치 업데이트**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/네트워크.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1.1 층: 딥러닝의 구성 단위**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **신경망의 핵심적인 데이터 구조는 2장에서 소개한 층이다.**\n",
    "\n",
    "> - **어떤 종류의 층은 상태가 없지만 대부분의 경우 가중치라는 층의 상태를 가진다.**\n",
    "\n",
    "> - **가중치는 확률적 경사 하강법에 의해 학습되는 하나 이상의 텐서이며 여기에 네트워크가 학습한 지식이 담겨 있다.**\n",
    "\n",
    "- **층마다 적절한 텐서 포맷과 데이터 처리 방식이 다르다.**\n",
    "\n",
    "> - **예를 들어 (samples,features) 크기의 2D 텐서가 저장된 간단한 벡터 데이터는 완전 연결층(fully connected layer)이나 밀집 층(dense layer)라 불리는 밀집 연결 층(densely connected layer)에 의해 처리되는 경우가 많다.**\n",
    "\n",
    "> - **(samples, timesteps, feature) 크기의 3D 텐서로 저장된 시퀸스 데이터는 보통 LSTM같은 순환 층(recurrent layer)에 의해 처리된다.**\n",
    "\n",
    "> - **4D텐서로 저장되어 있는 이미지 데이터는 일반적으로 2D합성곱 층(convolution layer)에 의해 처리된다.**\n",
    "\n",
    "- **케라스에서는 호환 가능한 층들을 엮어 데이터 변환 파이프라인(pipeline)을 구성함으로써 딥러닝 모델을 만든다.**\n",
    "\n",
    "> - **여기에서 층 호환성은  각 층이 특정 크기의 입력 텐서만 받고 특정 크기의 출력 텐서를 반환한다는 사실을 말한다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Keras code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 32개의 유닛(=노드)으로 된 밀집층\n",
    "# 첫 번째 차원이 784인 2D 텐서만 입력으로 받는 층을 만들었다.\n",
    "# 배치 차원인 0번째 축은 지정하지 않기 때문에 어떤 배치 크기도 입력으로 받을 수 있다.\n",
    "from keras import layers\n",
    "\n",
    "layer = layers.Dense(32, input_shape = (784,)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**따라서 이 층에는 32차원의 벡터를 입력으로 받는 하위 층이 연결되어야 한다.**\n",
    "\n",
    "> - **케라스에서는 모델에 추가된 층을 자동으로 상위 층의 크기에 맞추어 주기 때문에 호환성을 걱정하지 않아도 된다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "#첫 번째 차원이 784인 2D 텐서만 입력으로 받는 층 -> 첫번 째 차원의 크기가 32로 변환된 텐서를 출력\n",
    "model.add(layers.Dense(32,input_shape=(784,))) \n",
    "#두 번째 층에는 input_shape 매개변수를 지정하지 않았지만 그 대신 앞선 층의 출력 크기(32)를 입력의 크기로 자동으로 채택\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1.2 모델: 층의 네트워크**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**딥러닝 모델은 층으로 만든 비순환 유향 그래프(Directed Acyclic Graph, DAG)이다. 가장 일반적인 예가 하나의 입력을 하나의 출력으로 매핑하는 층을 순서대로 쌓는 것이다.**\n",
    "\n",
    "> - **그래프 이론에서 비순환 유형 그래프는 에지(edge)에 방향이 있고 한 노드(node)에서 다시 자기 자신으로 돌아올 경로가 없는 그래프를 말한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**다양한 네트워크 구조들을 보게 될 것인데, 자주 등장하는 것들은 다음과 같다.**\n",
    "\n",
    "> - **가지가 2개인 네트워크**\n",
    "\n",
    "> - **출력이 여러 개인 네트워크**\n",
    "\n",
    "> - **인셉션 블록**\n",
    "\n",
    "**네트워크 구조는 가설 공간(hyothesis space)을 정의한다.**\n",
    "\n",
    "> - **네트워크 구조를 선택함으로써 가능성 있는 공간(가설공간)을 입력 데이터에서 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한하게 된다.**\n",
    "\n",
    "> - **우리가 찾아야 할 것은 이런 텐서 연산에 포함된 가중치 텐서의 좋은 값이다.**\n",
    "\n",
    "**신뢰할 만한 모범적인 사례와 원칙이 있지만 연습을 해야만 적절한 신경망을 설계할 수 있는 기술을 갖추게 될 것이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1.3 손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**네트워크 구조를 정의하고 나면 두 가지를 더 선택해야한다.**\n",
    "\n",
    "> - **손실 함수: 훈련하는 동안 최소화될 값이다. 주어진 문제에 대한 성공 지표가 된다.**\n",
    "\n",
    "> - **옵티마이저: 손실 함수를 기반으로 네트워크가 어떻게 업데이트될지 결정한다. 특정 종류의 확률적 경사 하강법(SGD)을 구현**\n",
    "\n",
    "**여러 개의 출력을 내는 신경망은 여러 개의 손실 함수를 가질 수 있다. 하지만 경사 하강법 과정은 하나의 스칼라 손실값을 기준으로 한다.**\n",
    "\n",
    "> - **따라서 손실이 여러 개인 네트워크에서는 모든 손실이 (평균을 내서) 하나의 스칼라 양으로 합쳐진다.**\n",
    "\n",
    "**문제에 맞는 올바른 목적 함수를 선택하는 것은 아주 중요하다.**\n",
    "\n",
    "> - **우리가 만든 모든 신경망은 단지 손실 함수를 최소화하기만 한다는 것을 기억**\n",
    "\n",
    "**올바른 손실 함수를 선택하는 간단한 지침**\n",
    "\n",
    "> - **2개의 클래스가 있는 분류 문제에는 이진 크로스엔트로피(binary cross_entropy)**\n",
    "\n",
    "> - **여러 개의 클래스가 있는 분류 문제에는 범주형 크로스엔트로피(categorical cross_entropy)**\n",
    "\n",
    "> - **회귀 문제에는 평균 제곱 오차(Mean square error)**\n",
    "\n",
    "> - **시퀸스 학습 문제에는 CTC(Connection Temporal Classification)등을 사용**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 케라스 소개**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**케라스는 거의 모든 종류의 딥러닝 모델을 간편하게 만들고 훈련시킬 수 있는 파이썬을 위한 딥러닝 프레임워크이다.**\n",
    "\n",
    "> - **동일한 코드로 CPU와 GPU에서 실행할 수 있다.**\n",
    "\n",
    "> - **사용하기 쉬운 API를 가지고 있어 딥러닝 모델의 프로토타입을 빠르게 만들 수 있다.**\n",
    "\n",
    "> - **(컴퓨터 비전을 위한)합성곱 신경망, (시퀀스 처리를 위한)순환 신경망을 지원하며 이 둘을 자유롭게 조합하여 사용할 수 있다.**\n",
    "\n",
    "> - **다중 입력이나 다중 출력 모델, 층의 공유, 모델 공유 등 어떤 네트워크 구조도 만들 수 있다. 이 말은 적대적 생성 신경망(Generative Adversarial Network, GAN)부터 뉴럴 튜링 머신(Neural Turing Machine)까지 케라스는 기본적으로 어떤 딥러닝 모델에도 적합하다는 뜻이다.**\n",
    "\n",
    "**케라스는 MIT 라이센스를 따르므로 상업적인 프로젝트에도 자유롭게 사용할 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**머신 러닝 경연 웹사이트인 캐글(Kaggle)에서도 케라스의 인기가 높다. 최근에 거의 모든 딥러닝 경연 대회의 우승자들이 케라스 모델을 사용하고 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2.1 케라스, 텐서플로, 씨아노, CNTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](./img/스택.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**케라스는 딥러닝 모델을 만들기 위한 고수준의 구성 요소를 제공하는 모델 수준의 라이브러리이다. 텐서 조작이나 미분 같은 저수준의 연산을 다루지 않는다. 그 대신 케라스의 백엔드 엔진(backend engine)에서 제공하는 최적화되고 특화된 텐서 라이브러리를 사용한다.**\n",
    "\n",
    "> - **텐서조작이나 미분 같은 저수준의 연산을 다루지 않는다.**\n",
    "\n",
    "**그 대신 케라스의 백엔트 엔진(backend engine)에서 제공하는 최적화되고 특화된 텐서 라이브러리를 사용한다.**\n",
    "\n",
    "> - **텐서플로우의 저수준 API는 처음에는 조금 번거로울 수 있지만 복잡한 모델을 개발할 수 있도록 자유도를 높여준다.**\n",
    "\n",
    "> - **1.14이상 부터는 tf.keras를 통해 고수준의 API를 사용가능**\n",
    "\n",
    "**케라스로 작성한 모든 코드는 아무런 변경 없이 이런 백엔드 중 하나를 선택하여 실행**\n",
    "\n",
    "> - **대부분의 딥러닝 작업에서 텐서플로우 백엔트가 기본적으로 권장(확장성 때문)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2.2 케라스를 사용한 개발: 빠르게 둘러보기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전형적인 케라스 작업 흐름**\n",
    "\n",
    "> **1. 입력 텐서와 타깃 텐서로 이루어진 훈련 데이터를 정의**\n",
    "\n",
    "> **2. 입력과 타깃을 매핑하는 층으로 이루어진 네트워크(또는 모델)를 정의한다.**\n",
    "\n",
    "> **3. 손실 함수, 옵티마이저, 모니터링하기 위한 측정 지표를 선택하여 학습 과정을 설정한다.**\n",
    "\n",
    "> **4. 훈련 데이터에 대해 모델의 fit() 메서드를 반복적으로 호출**\n",
    "\n",
    "**모델을 정의하는 방법은 두 가지인데, Sequential 클래스(가장 자주 사용하는 구조인 층을 순서대로 쌓아 올리 네트워크) 또는 함수형 API(완전히 임의의 구조를 만들 수 있는 비순환 유향 그래프)를 사용한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Keras code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "# Sequential: 모델 정의 단계\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu',input_shape=(784,)))\n",
    "model.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "# complie: 학습 과정이 설정된다. 여기에서 모델이 사용할 옵티마이저와 손실 함수, 훈련하는 동안 모니터링하기 위해 필요한 측정지표\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001), # RMSprop optimizer\n",
    "              loss = 'mse', # loss function\n",
    "              metrics=['accuracy']) # accuracy\n",
    "\n",
    "# fit: 입력 데이터의 넘파이 배열을 (그리고 이에 상응하는 타깃 데이터를) 모델의 fit() 메서드에 전달함으로써 학습 과정이 이루어진다.\n",
    "# model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 딥러닝 컴퓨터 셋팅**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**딥러닝 애플리케이션 개발을 시작하기 전에 먼저 컴퓨터를 셋팅해야 한다. 아주 필수적이지는 않지만 최신 NVIDIA GPU에서 딥러닝 코드를 실행하는 것을 권장한다.**\n",
    "\n",
    "> - **특히 합성곱 신경망을 사용한 이미지 처리나 순환 신경망을 사용한 시퀀스 처리 같은 일부 애플리케이션을 GPU에서 실행하면 아주 빠른 멀티코어 CPU라도 매우 오래 걸린다.**\n",
    "\n",
    "> - **최신 GPU를 사용하면 보통 2배나 5배 또는 10배 정도 속도가 빨라진다.**\n",
    "\n",
    "**컴퓨터에 GPU 카드를 설치하고 싶지 않다면 대안으로 AWS같은 클라우드 플랫폼을 고려해 볼 수 있다. 하지만 시간에 따라 비용이 과금된다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4 영화 리뷰 분류: 이진 분류 예제**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**인터넷 영화 데이터베이스로부터 가져온 양극단의 리뷰 5만 개로 이루어진 IMDB 데이터셋을 사용**\n",
    "\n",
    "> **이 데이터셋은 훈련 데이터 25,000개와 테스트 데이터 25,000개로 나뉘어 있고 각각 50%는 부정, 50%는 긍정 리뷰로 구성되어 있다.**\n",
    "\n",
    "**훈련 데이터와 테스트 데이터를 나누는 이유**\n",
    "\n",
    "> **같은 데이터에서 머신 러닝 모델을 훈련하고 테스트해서는 절대 안 된다. 모델이 훈련 데이터에서 잘 작동한다는 것이 처음 만난 데이터에서도 잘 작동한다는 것을 보장하지 않는다. 중요한 것은 새로운 데이터에 대한 모델의 성능이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMDB 데이터셋도 케라스에 포함되어 있는데 이 데이터는 전처리되어 있어 각 리뷰(단어 시퀀스)가 숫자 시퀀스로 변환되어 있다. 여기서 각 숫자는 사전에 있는 고유한 단어를 나타낸다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# num_words = 10000 매개변수는 훈련 데이터에서 가장 자주 나타나는 단어 1만 개만 사용하겠다는 의미이다\n",
    "# 즉, 드물게 나타나는 단어는 무시하겠다는 의미\n",
    "(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words=10000)\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 자주 등장하는 단어 1만 개로 제한했기 때문에 인덱스는 9,999를 넘지 않는다.\n",
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 데이터 하나를 원래 영어 단어로 변환\n",
    "# word_index는 단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "word_index = imdb.get_word_index() \n",
    "\n",
    "# 정수 인덱스와 단어를 매핑하도록 뒤집는다.\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "# 리뷰를 디코딩한다. 0,1,2는 '패딩','문서시작','사전에 없음'을 위한 인덱스이므로 3을 뺸다.\n",
    "decoded_review = ' '.join([reverse_word_index.get(i-3,'?') for i in train_data[0]])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4.2 데이터 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**신경망에 숫자 리스트를 주입할 수는 없으므로, 리스트를 텐서로 바꿔야한다.**\n",
    "\n",
    "> - **같은 길이가 되도록 리스트에 패딩(padding)을 추가하고 (samples, sequence_length) 크기의 정수 텐서로 변환한다.(가장 긴 리뷰는 2,494개의 단어로 이루어져 있으므로 훈련 데이터를 변환한 텐서의 크기는 (25,000,2494)가 된다.) 그다음 이 정수 텐서를 다룰 수 있는 층을 신경망의 첫 번째 층으로 사용한다.(embedding 층을 말하며, 나중에 자세히 다룬다.)**\n",
    "\n",
    "> - **리스트를 원-핫 인코딩(one-hot encoding)하여 0과 1의 벡터로 변환한다. 예를 들어 시퀀스 [3,5]를 인덱스 3과 5의 위치는 1이고 그 외는 모두 0인 10,000차원의 벡터로 각각 변환한다.(리스트가 하나의 벡터로 변환되므로 훈련 데이터를 변환한 텐서의 크기는 (25,000,10,000)이 된다.) 그다음 부동 소수 벡터 데이터를 다룰 수 있는 Dense층을 신경망의 첫 번째 층으로 사용한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크기가 (len(sequences),dimension)인 0행렬을 생성\n",
    "def vectorize_sequence(sequences,dimension=10000): # 문자 sequence와 차원을 입력으로 받는다.\n",
    "    results = np.zeros((len(sequences),dimension)) # (25000,10000) 크기의 zero vector를 생성\n",
    "    for i, sequences in enumerate(sequences): # 각각의 단어를 인덱스와 단어 형태로 분리\n",
    "        results[i,sequences] = 1. # 현재 result는 0행렬로 이루어져 있음으로, result[i]에서 특정 인덱스(각 리뷰 단어의 해당 위치)의 위치를 1로 만든다. \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 첫번째 리뷰: [0. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 훈련, 테스트 데이터를 벡터로 변환한다.\n",
    "x_train = vectorize_sequence(train_data)\n",
    "x_test= vectorize_sequence(test_data)\n",
    "\n",
    "print('훈련 데이터의 첫번째 리뷰:',x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 -> 배열(float32)\n",
    "y_train = np.asarray(train_label).astype('float32') \n",
    "y_test = np.asarray(test_label).astype('float32') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4.3 신경망 모델 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**입력 데이터가 벡터고 레이블은 스칼라(1또는 0)이다. 앞으로 볼 수 있는 문제 중 가장 간단한 예**\n",
    "\n",
    "> - **이런 문제에 잘 작동하는 네트워크 종류는 relu 활성화 함수를 사용한 완전 연결 층(즉 Dense(16,activation='relu'))을 그냥 쌓은 것이다.**\n",
    "\n",
    "**Dense층에 전달한 매개변수(16)는 은닉 유닛(hidden unit)의 개수이다. 하나의 은닉 유닛은 층이 나타내는 표현 공간에서 하나의 차원이 된다.**\n",
    "\n",
    "> - **16개의 은닉 유닛이 있다는 것은 가중치 행렬 W의 크기가 (input_dimension,16)이라는 뜻이다.**\n",
    "\n",
    "> - **따라서 입력 데이터와 W를 점곱하면 입력 데이터가 16차원으로 표현된 공간으로 투영된다. 그리고 편향벡터 b를 더하고 relu 연산을 적용한다.**\n",
    "\n",
    "**표현 공간의 차원을 '신경망이 내재된 표현을 학습할 때 가질 수 있는 자유도'로 이해할 수 있다.**\n",
    "\n",
    "> - **은닉 유닛을 늘리면 (표현 공간을 더 고차원으로 만들면) 신경망이 더욱 복잡한 표현을 학습할 수 있지만 계산 비용이 커지고 원하지 않는 패턴으로 학습 할 수 있다.**\n",
    "\n",
    "> - **즉, 훈련 데이터에서는 성능이 향상되지만 테스트 데이터에서는 그렇지 않은 패턴**\n",
    "\n",
    "**Dense 층을 쌓을 때 두 가지 중요한 구조상의 결정이 필요**\n",
    "\n",
    "> - **얼마나 많은 층을 사용할 것인가? (여기선 16개의 은닉 유닛과 2개의 은닉층)**\n",
    "\n",
    "> - **각 층에 얼마나 많은 은닉 유닛을 둘 것인가? (현재 리뷰의 감정을 스칼라 값을 예측으로 출력하는 세 번째 층)**\n",
    "\n",
    "**중간에 있는 은닉 층은 활성화 함수로 relu를 사용하고 마지막 층은 확률(0과 1사이의 점수로, 어떤 샘플이 타깃 '1'일 가능성이 높다는 것은 그 리뷰가 긍정일 가능성이 높다는 것을 의미한다.**\n",
    "\n",
    "> - **출력을 위해 시그모이드 활성화 함수를 사용 (임의의 값을 0~1 사이로 압축하므로 출력 값을 확률처럼 사용)**\n",
    "\n",
    ">  ![test](./img/시그모이드드.png)\n",
    "\n",
    "> - **relu는 음수를 0으로 만드는 함수이다.**\n",
    "\n",
    ">  ![test](./img/렐루.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() # model 생성\n",
    "# 입력을 10,000을 받는 16개의 은닉노드 생성 활성화 함수를 relu (1층)\n",
    "model.add(layers.Dense(16,activation='relu',input_shape=(10000,))) \n",
    "# 입력을 16을 받는 16개의 은닉노드 생성 활성화 함수는 relu(2층)\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "# 출력층으로 1개의 값으로 출력 활성화함수는 시그모이드 함수로 0 ~ 1사이의 값(확률)\n",
    "model.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**활성화 함수를 사용하는 이유**\n",
    "\n",
    "> - **relu와 같은 활성화 함수(또는 비선형 함수라고도 부른다)가 없다면 Dense 층은 선형적인 연산인 점곱과 덧셈 2개로 구성된다. (output = dot(W,input)+b**\n",
    "\n",
    "> - **그러므로 이층은 입력에 대한 선형 변환(아핀 변환)만을 학습할 수 있다. 이 층의 가설 공간은 입력 데이터를 16차원의 공간으로 바꾸는 가능한 모든 선형 변화의 집합이다. 이런 가설 공간은 매우 제약이 많으며, 선형 층을 깊게 쌓아도 여전히 하나의 선형 연산이기 때문에 층을 여러 개로 구성하는 장점이 없다. 즉, 층을 추가해도 가서 공간이 확장되지 않는다.**\n",
    "\n",
    "> - **따라서 가설 공간을 풍부하게 만들어 층을 깊게 만드는 장점을 살리기 위해서는 비선형성 또는 활성화 함수를 추가해야 한다. relu는 딥러닝에서 가장 인기 있는 활성화 함수이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**마지막으로 손실 함수와 옵티마이저를 선택해야한다.**\n",
    "\n",
    "> - **이진 분류 문제고 신경망의 출력이 확률이기 때문에 binary_crossentropy 손실이 적합하다. (mean_squared_error도 사용가능)**\n",
    "\n",
    "> - **확률을 출력하는 모델을 사용할 때는 크로스엔트로피가 최선의 선택이다.** \n",
    "\n",
    "> - **크로스엔트로피는 정보 이론 분야에서 온 개념으로 확률 분포 간의 차이를 측정한다. 여기에서는 원본 분포와 예측 분포 사이를 측정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# rmsprop 옵티마이저와 binary_crossentropy 손실 함수로 모델을 설정하는 단계\n",
    "# 케라스에 rmsprop, binary_crossentropy, accuracy가 포함되어 있기 때문에,\n",
    "# 옵티마이저, 손실 함수, 측정 지표를 문자열로 지정하는 것이 가능하다.\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이따금 옵티마이저의 매개변수를 바꾸거나 자시만의 손실 함수, 측정 함수를 전달해야 할 경우가 있다.\n",
    "# 아래 코드와 같이 옵티마이저 파이썬 클래스를 사용해서 객체를 직접 만들어 optimizer 매개변수에 전달하면 된다.\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래의 코드는 loss와 metircs 매개변수에 함수 객체를 전달하면 된다.\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss = losses.binary_crossentropy,\n",
    "             metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4.4 훈련 검증**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**훈련하는 동안 처음 본 데이터에 대한 모델의 정확도를 측정하기 위해서는 원본 훈련 데이터에서 10,000의 샘플을 떼어 검증 세트를 만들어야 한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 세트 준비하기\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델을 512개의 샘플씩 미니 배치를 만들어 20번의 에포크 동안 훈련시킨다. (x_train과 y_train 텐서에 있는 모든 샘플에 대해 20번 반복)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**동시에 따로 떼어 놓은 1만 개의 샘플에서 손실과 정확도를 측정할 것이다. 이렇게 하려면 validation_data 매개변수에 검증 데이터를 전달해야한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.fit() 메서드는 History 객체를 반환한다. 이 객체는 훈련하는 동안 발생한 모든 정보를 담고 있는 딕셔너리인 history 속성을 가지고 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 146us/step - loss: 0.5202 - acc: 0.7801 - val_loss: 0.3968 - val_acc: 0.8620\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.3109 - acc: 0.9004 - val_loss: 0.3261 - val_acc: 0.8725\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.2294 - acc: 0.9239 - val_loss: 0.2771 - val_acc: 0.8936\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.1784 - acc: 0.9435 - val_loss: 0.2855 - val_acc: 0.8853\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.1470 - acc: 0.9526 - val_loss: 0.2774 - val_acc: 0.8897\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.1201 - acc: 0.9630 - val_loss: 0.2986 - val_acc: 0.8846\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.1016 - acc: 0.9693 - val_loss: 0.3053 - val_acc: 0.8854\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.0840 - acc: 0.9767 - val_loss: 0.3249 - val_acc: 0.8817\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0672 - acc: 0.9829 - val_loss: 0.3630 - val_acc: 0.8749\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.0572 - acc: 0.9859 - val_loss: 0.3742 - val_acc: 0.8768\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 97us/step - loss: 0.0472 - acc: 0.9883 - val_loss: 0.3964 - val_acc: 0.8763\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.0398 - acc: 0.9903 - val_loss: 0.4258 - val_acc: 0.8726\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0290 - acc: 0.9939 - val_loss: 0.4540 - val_acc: 0.8724\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0246 - acc: 0.9957 - val_loss: 0.4892 - val_acc: 0.8707\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 97us/step - loss: 0.0179 - acc: 0.9977 - val_loss: 0.5254 - val_acc: 0.8691\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0152 - acc: 0.9980 - val_loss: 0.5517 - val_acc: 0.8703\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 96us/step - loss: 0.0131 - acc: 0.9977 - val_loss: 0.5950 - val_acc: 0.8676\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 95us/step - loss: 0.0113 - acc: 0.9981 - val_loss: 0.6208 - val_acc: 0.8672\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0054 - acc: 0.9998 - val_loss: 0.6494 - val_acc: 0.8673\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0079 - acc: 0.9987 - val_loss: 0.6853 - val_acc: 0.8669\n"
     ]
    }
   ],
   "source": [
    "# 모델을 512개의 샘플씩 미니 배치를 만들어 20번의 에포크 동안 훈련시킨다. (x_train, y_train 텐서에 있는 모든 샘플에 대해 20번 반복)\n",
    "# 동시에 따로 떼어 놓은 1만 개의 샘플에서 손실과 정확도를 측정할 것이다.\n",
    "# 이렇게 하려면 validation_data 매개변수에 검증 데이터를 전달해야 한다.\n",
    "\n",
    "# max_iter = batch_size * iteration -> max_iter = 512 * iteration (1 epch을 위해 반복해야하는 iteration)\n",
    "# epoch = max_iter / x_train_size -> 20 = max_iter / 15,000\n",
    "# max_iter = 20 * 15,000  = 300,000 / 즉, 학습에 사진 300,000장을 사용하기로 했기 때문에 15,000장의 사진이 여러번 재사용\n",
    "# iteration = 약 585번\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 512,\n",
    "                   validation_data= (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit() 메서드는 History 객체를 반환한다.\n",
    "# 이 객체는 훈련하는 동안 발생한 모든 정보를 담고 있는 딕셔너리인 history 속성을 가지고 있다.\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "loss = history_dict['loss'] # history의 loss 값을 loss 변수에 할당\n",
    "val_loss = history_dict['val_loss'] # history의 val_loss 값을 val_loss변수에 할당\n",
    "\n",
    "epochs = range(1,len(loss)+1) # 1 ~ 20 정렬\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label = 'Training loss')\n",
    "plt.plot(epochs,val_loss,'b',label = 'Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf() # 그래프를 초기화\n",
    "\n",
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label = 'Training acc')\n",
    "plt.plot(epochs,val_acc,'b',label = 'Validation acc')\n",
    "plt.title('Training and Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**결과를 보면 훈련 데이터는 epochs을 반복할수록 정확도는 오르고 손실함수는 낮아지고 있는 경향을 보이지만 validation셋으로 검증한 결과는 반대로 epochs을 반복 할수록 정확도는 낮아지고 손실함수는 높아지는 경향을 보인다. 이를 과대적합(overfiting)이라 한다.**\n",
    "\n",
    "> - **즉, 두 번째 에포크 이후부터 훈련 데이터에 과도하게 최적화되어 훈련 데이터에 특화된 표현을 학습하므로 훈련 세트 이외의 데이터에는 일반화되지 못한다.**\n",
    "\n",
    "**이런 경우에 과대적합을 방지하기 위해서 세 번째 에포크 이후에 훈련을 중지할 수 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 2s 60us/step - loss: 0.4431 - acc: 0.8275\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 57us/step - loss: 0.2562 - acc: 0.9117\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 56us/step - loss: 0.1989 - acc: 0.9296\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 57us/step - loss: 0.1673 - acc: 0.9409\n",
      "------------------------------------------------------------------------------------------------------\n",
      "25000/25000 [==============================] - 2s 90us/step\n",
      "[0.2914175025320053, 0.884880006313324]\n"
     ]
    }
   ],
   "source": [
    "# 처음부터 다시 새로운 신경망을 네 번의 에포크 동안만 훈련하고 테스트 데이터에서 평가해본다.\n",
    "\n",
    "model = models.Sequential() # model 생성\n",
    "# 입력을 10,000을 받는 16개의 은닉노드 생성 활성화 함수를 relu (1층)\n",
    "model.add(layers.Dense(16,activation='relu',input_shape=(10000,))) \n",
    "# 입력을 16을 받는 16개의 은닉노드 생성 활성화 함수는 relu(2층)\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "# 출력층으로 1개의 값으로 출력 활성화함수는 시그모이드 함수로 0 ~ 1사이의 값(확률)\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "print('------------------------------------------------------------------------------------------------------')\n",
    "results = model.evaluate(x_test,y_test)\n",
    "print(results) # loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4.5 훈련된 모델로 새로운 데이터에 대해 예측하기** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델을 훈련시킨 후 이를 실전 환경에서 사용하고 싶을 것이다. predict메서드를 사용해서 어떤 리뷰가 긍정일 확률을 예측할 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**아래 결과를 보면 어떤 샘플에 대해 확신을 가지고 있지만 어떤 샘플에 대해서는 확신이 부족한 것을 볼 수 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19276223],\n",
       "       [0.9997429 ],\n",
       "       [0.89531296],\n",
       "       ...,\n",
       "       [0.12648672],\n",
       "       [0.07673863],\n",
       "       [0.70000255]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4.6 추가 실험**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 3개의 은닉 층을 사용하고 검증과 테스트 정확도에 어떤 영향을 미치는지 확인\n",
    "\n",
    "> - 층의 은닉 유닛을 추가 32개의 유닉\n",
    "\n",
    "> - mse를 손실함수로 사용\n",
    "\n",
    "> - relu 대신에 tanh 활성화 함수를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() # model 생성\n",
    "# 입력을 10,000을 받는 32개의 은닉노드 생성 활성화 함수를 tanh (1층)\n",
    "model.add(layers.Dense(32,activation='tanh',input_shape=(10000,))) \n",
    "# 입력을 32을 받는 64개의 은닉노드 생성 활성화 함수는 tanh(2층)\n",
    "model.add(layers.Dense(64,activation='tanh'))\n",
    "# 입력을 32을 받는 개의 은닉노드 생성 활성화 함수는 tanh(3층)\n",
    "model.add(layers.Dense(32,activation='tanh'))\n",
    "# 출력층으로 1개의 값으로 출력 활성화함수는 시그모이드 함수로 0 ~ 1사이의 값(확률)\n",
    "model.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.1534 - acc: 0.7784 - val_loss: 0.0893 - val_acc: 0.8827\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.0683 - acc: 0.9108 - val_loss: 0.1096 - val_acc: 0.8533\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0486 - acc: 0.9375 - val_loss: 0.0895 - val_acc: 0.8821\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0422 - acc: 0.9471 - val_loss: 0.0929 - val_acc: 0.8794\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.0321 - acc: 0.9596 - val_loss: 0.1064 - val_acc: 0.8683\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.0261 - acc: 0.9689 - val_loss: 0.1135 - val_acc: 0.8624\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0204 - acc: 0.9765 - val_loss: 0.1039 - val_acc: 0.8773\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.0233 - acc: 0.9715 - val_loss: 0.1275 - val_acc: 0.8509\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 93us/step - loss: 0.0161 - acc: 0.9819 - val_loss: 0.1121 - val_acc: 0.8714\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0166 - acc: 0.9817 - val_loss: 0.1099 - val_acc: 0.8741\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0134 - acc: 0.9850 - val_loss: 0.1115 - val_acc: 0.8743\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0145 - acc: 0.9837 - val_loss: 0.1133 - val_acc: 0.8750\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0134 - acc: 0.9850 - val_loss: 0.1158 - val_acc: 0.8693\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0116 - acc: 0.9870 - val_loss: 0.1141 - val_acc: 0.8735\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0079 - acc: 0.9919 - val_loss: 0.1172 - val_acc: 0.8705\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0140 - acc: 0.9841 - val_loss: 0.1176 - val_acc: 0.8705\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0085 - acc: 0.9909 - val_loss: 0.1251 - val_acc: 0.8625\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0094 - acc: 0.9898 - val_loss: 0.1208 - val_acc: 0.8683\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0064 - acc: 0.9935 - val_loss: 0.1455 - val_acc: 0.8423\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 94us/step - loss: 0.0112 - acc: 0.9878 - val_loss: 0.1202 - val_acc: 0.8695\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'mse',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs = 20,\n",
    "                   batch_size = 512,\n",
    "                   validation_data= (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXhU9dXA8e8hbLIjoKIsAUFLQJYQEQVFhZeiFnFBBaHuL6KiVmpLFFda3gJaoSBVsQUXIkhdqSK4QEWtAgERAUUWASOo7IigkOS8f/xuYDLMTCaZuTNZzud55pmZu55Mknvmt15RVYwxxphglZIdgDHGmNLJEoQxxpiQLEEYY4wJyRKEMcaYkCxBGGOMCckShDHGmJAsQZiEEJEUEdknIs3iuW0yiUgrEfGln3jwsUXkbREZ5EccInK/iDxZ0v0jHPcmEflPvI9rEscShAnJu0AXPPJF5EDA+5AXqkhUNU9Va6nq5nhuW1qJyHsi8kCI5ZeLyLciUqz/PVXtrapZcYirl4hsDDr2n1R1aKzHNuWPJQgTkneBrqWqtYDNQN+AZUddqESkcuKjLNWeAX4bYvlvgemqmp/YcIwpPksQpkRE5M8i8qKIzBCRH4HBInKmiHwiIrtFZKuITBSRKt72lUVERSTVez/dW/+WiPwoIh+LSIvibuutv0BEvhKRPSIySUQ+EpHrwsQdTYw3i8g6EdklIhMD9k0RkfEiskNE1gN9InxErwAniMhZAfs3AC4EnvPeXywiy72fabOI3B/h8/6w4GcqKg6vaucL77jrReQmb3ld4N9As4DS4HHe7/KZgP0vEZFV3mc0X0RODViXIyLDReRz7/OeISLVInwOgXF1F5Fsb7/FInJGwLobRWSjF/MGERngLT9FRBZ6+2wXkReiOZeJE1W1hz0iPoCNQK+gZX8GDgJ9cV80jgFOB84AKgMtga+AYd72lQEFUr3304HtQAZQBXgR9826uNseB/wI9PPWDQcOAdeF+VmiifF1oC6QCuws+NmBYcAqoAnQAFjo/oXCfm7TgCcD3t8GZAe8Px9o531+Hbyf8TfeulaBxwY+LPiZiorD+520BMQ7xwGgvbeuF7AxxO/yGe91G2Cft18V4F7vM6rirc8BPgFO8M79FXBTmJ//JuA/3uuGwB5goPc5DwZ2APWBOt661t62jYE07/W/gBHeZ1Qd6Jbs/4eK9LAShInFh6r6b1XNV9UDqrpEVRepaq6qbgCmAD0i7P+Sqmar6iEgC+hYgm1/AyxX1de9deNxF9qQoozxL6q6R1U3Av8JONeVwHhVzVHVHcCYCPECPAtcGfAN+xpvWUEs81V1pff5fQbMDBFLKBHj8H4nG9SZD7wHnB3FcQEGALO92A55x66DS6oFJqjqd9653yDy761AX2CVqs7wPvvpwAbgooKwgXYiUl1Vt6rqam/5IVyibqyqP6vqR1H+HCYOLEGYWHwT+EZEfiUib4rIdyKyFxiF++YYzncBr/cDtUqw7YmBcaiq4r7lhhRljFGdC9gUIV6A93HfjPuKyClAJ2BGQCxnish/RGSbiOzBfeOO9HkViBiHiPxGRBaJyE4R2Q30jvK4Bcc+fDx1bSU5wEkB2xTn9xbyuAFxn6Sqe3Eli9uA70TkDe/zAvg9riST7VVrXRvlz2HiwBKEiUVw18qngJVAK1WtAzyAq+bw01ZcVQsAIiIUvpgFiyXGrUDTgPcRu+F6yep5XMnht8AcVQ0s3cwEXgaaqmpd4B9RxhI2DhE5BngJ+AtwvKrWA94OOG5R3WG3AM0DjlcJ9/l+G0VcUR/X06zguKr6lqr2wlUvrcP9nvBKEzepamNcApkS2P5k/GUJwsRTbdw35p9EpA1wcwLO+QaQLiJ9xfWkuhNo5FOMs4DfichJXoPziCj2eRbXiHwDAdVLAbHsVNWfRaQrrnon1jiqAVWBbUCeiPwG6Bmw/nugoYjUjnDsi0XkXK/x/g+4Np5FUcYWzhtAWxG5yusMcDWunWWOiDT2fn81cO1aPwF5ACJypYgUJPzduASXF2MsJkqWIEw8/R64FndBeQrXmOwrVf0euAp4DNfoeTLwKfCLDzE+gavP/xxYgvumXlR864HFuAbWN4NW3wL8RVwvsHtxF+eY4lDV3cBdwKu4Bvb+uItzwfqVuFLLRq+X0nFB8a7CfT5P4JJMH+Birz2ixFR1G3AxLpnt8GL8jaruBFJwiWirt+4sXEM8uLaPJSLyE65n2G1ahsfHlDXiSsHGlA8ikoKrzuivqh8kOx5jyjIrQZgyT0T6iEhdr7fQ/UAu7lu7MSYGliBMedAd12VyO65K5BJVDVfFZIyJkq8Jwvtmt8YblZoZYv05IrJMRHJFpH/QumbiJij7QkRWizeq1phgqnqfqjZQ1dqq2lVVlyQ7JmPKA98ShFcXPBm4AEgDBopIWtBmm4HrgFDD558DHlHVNkAX4Ae/YjXGGHM0PydY6wKs80arIiIzcdMhFIyQxBupiogUmrjMSySVVfUdb7t9RZ2sYcOGmpqaGq/YjTGmQli6dOl2VQ3ZNdzPBHEShUd75lB4uH4kpwC7ReQVoAXwLpCpqoX6P4vIEGAIQLNmzcjOzo45aGOMqUhEJOyMAH62QYQaERptn9rKuLlj7sZNrtYSVxVV+GCqU1Q1Q1UzGjWKNDbKGGNMcfmZIHIoPB1AE1z/9Gj3/dSbcCwXeA1Ij3N8xhhjIvAzQSwBWotICxGpijdLZDH2rS8iBcWC8wlouzDGGOM/3xKE981/GDAP+AKYpaqrRGSUiFwMICKni0gOcAXwlIis8vbNw1UvvScin+Oqq572K1ZjjDFHKzdTbWRkZKg1UhtjTPGIyFJVzQi1rsKPpM7KgtRUqFTJPWfFfFt4Y4wpHyr0jeazsmDIENi/373ftMm9Bxg0KHlxGWNMaVChSxAjRx5JDgX273fLjTGmoqvQCWJzmFnlwy03xpiKpEIniGZhbhgZbrkxxlQkFTpBjB4NNWoUXlajhltujDEVXYVOEIMGwZQp0Lw5iLjnKVOsgdoYY6CC92IClwwsIRhjzNEqdAnCGGNMeJYgjDHGhGQJwhhjTEiWIIwxxoRkCcIYY0xIliCMMSaJhg+H229PdhShVfhursYYkyxffgkTJoAq3HortGmT7IgKsxKEMcYkyZgxUL06HHMMjBuX7GiOZgnCGGOSYONGmD7d3WLgppvc62++SXZUhVmCMMaYJBg3zt2o7O674fe/d8v++tfkxhTM1wQhIn1EZI2IrBORzBDrzxGRZSKSKyL9Q6yvIyLfisjjfsZpjDGJtHUrTJ0K110HTZq4eeCuvhqefhq2b092dEf4liBEJAWYDFwApAEDRSQtaLPNwHXAC2EO8yfgfb9iNMaYZPjrX+HQIRgx4siyP/7R3bDs8VL0ddjPEkQXYJ2qblDVg8BMoF/gBqq6UVVXAPnBO4tIZ+B44G0fYzTGmITavh2eeAIGDoSTTz6yvG1b6NcPJk6EffuSF18gPxPESUBgk0uOt6xIIlIJ+CvwhyK2GyIi2SKSvW3bthIHaowxifK3v7mSwr33Hr0uMxN27XJVTaWBnwlCQizTKPe9FZijqhHb9FV1iqpmqGpGo0aNih2gMcYk0p49MGkSXHYZpAVXuANdu0KPHq4K6uDBxMcXzM8EkQM0DXjfBNgS5b5nAsNEZCPwKHCNiIyJb3jGGJNYf/+7SxKhSg8F7rkHvv3WdXtNNj8TxBKgtYi0EJGqwABgdjQ7quogVW2mqqnA3cBzqnpULyhjjCkr9u+H8eOhTx/o3Dn8dr17Q8eOrhtsXl7i4gvFtwShqrnAMGAe8AUwS1VXicgoEbkYQEROF5Ec4ArgKRFZ5Vc8xhiTTE8/Ddu2wciRkbcTcW0Ra9bA668nJrawsahG2yxQumVkZGh2dnaywzDGmKP88gu0bAmtWsH7UXTcz8uDU0+F+vVh8WKXNPwiIktVNSPUOhtJbYwxPnv2WdiyBe67L7rtU1LcuIjsbJg/39/YIrEShDHG+Cg315UGGjSARYuiLw388gu0aOF6O737rn/xWQnCGGOSZOZM2LDBtT0Up6qoWjW46y547z1YssS/+CKxBGGMMT7Jz4e//AXatYO+fYu//803Q716MHZs/GOLhiUIY4zxyWuvwerVbtxDpRJcbevUgdtug1decTcXSjRLEMYY4wNV+POfXc+lK68s+XHuuMNVNz3ySPxii5YlCGOM8cHcufDpp25kdEpKyY9z3HFw443w/POQkxO/+KJhCcIYY+JMFUaPhqZNYfDg2I93992uPeOxx2I/VnFYgjDGmDhbuBA++siNZahaNfbjpaa66cGnTIEdO2I/XrQsQRhjTJyNHg3HH++qhuJlxAj46SeYPDl+xyyKJQhT5j39NCxbluwojHEWL4Z33oHhw+GYY+J33IKushMnukSRCJYgTJn28ccwZIibITPRDXjGhDJ6tJtD6ZZb4n/szExXxfSPf8T/2KFYgjBl2oMPuikMfv4ZrriidNxkxVRcn38Os2fDnXdC7drxP/5ZZ8HZZyfuhkKWIEyZ9dFHriifmQlTp8Inn7jeHsYUx7JlrlE5Pz/2Y/3f/0GtWnD77bEfK5x77oFvvoEXXvDvHAUsQZgy68EHXR/xW26B/v1dne+kSTBjRrIjM2XBjh2uEblzZ3ebz+bNXUPwypUlO97atTBrFtx6Kxx7bHxjDdSnD7Rv76bfiEdSi8QShCmTFi50k5hlZkLNmm7ZmDHQvTvcdBOssltPmTBU4bnn4Fe/cs8jRrgvFR06uKqb005zd3R79FE3RXe0xoxxXVqHD/cvdjhyQ6Evv3TVWb5S1XLx6Ny5s5qK49xzVU84QXX//sLLv/1W9fjjVU89VXXv3uTEZkqvNWtUzztPFVTPPFN1xYrC63/4QXXSJNUuXdw2Iqq9eqk++2zkv6dNm1QrV1YdNszf+AscOqTasqWLMz8/tmMB2RrmumolCFPmLFgA//mP+xYV3I3wxBPhxRdh3TpXfVBObndiYvTLL/Dww650sGwZPPkkfPihex+oUSMYNszdt2HNGrj/fjdV97XXunENV18Nb73l7vEQaNw49/yHPyTm56lc2Z1r8WL3v+CbcJkjHg+gD7AGWAdkhlh/DrAMyAX6ByzvCHwMrAJWAFcVdS4rQVQM+fmqZ5+teuKJR5ceAo0b574Bjh+fuNhM6TR/vuopp7i/h4EDVbduLd7++fmqH32kOnSoav367jjHHad6552qS5a441Wrpnrjjf7EH86BA6603Lt3bMchQgnCz+SQAqwHWgJVgc+AtKBtUoH2wHNBCeIUoLX3+kRgK1Av0vksQVQM777r/monTYq8XX6+6iWXuGL/Bx8kJjZTuvzwg+o117i/l5YtVefNi/2Yv/yi+tprqpdfrlq1qjt23bqqlSqprl0b+/GLa8wYF0N2dsmPESlB+FnF1AVYp6obVPUgMBPoF7iBqm5U1RVAftDyr1R1rfd6C/AD0MjHWE0ZoAoPPAAnneQaoiMRgWeecXPYXHklfP99IiI0pYEqTJvmGqFfeMHdi2HlSujdO/ZjV60K/frBSy+5v6mnn4b0dNe9ulWr2I9fXEOHuntG+HVDocr+HBaAk4BvAt7nAGcU9yAi0gVXAlkfYt0QYAhAs2bNShalKTPeeQf++1/4+9+hevWit69bF15+Gbp2hQED3P6V/fyLN0n3xRfuorlwoevR9uST0LatP+eqV899USnqy4qf6tZ1bXE//eQSY3FuaRoNP0sQoUItVpOhiDQGngeuV9Wjevyq6hRVzVDVjEaNrIBRnqm6cQ9Nm8INN0S/X/v28NRTriHvvvt8C88k2YEDrkG5Qwc3mvkf/4D33/cvOZQm99zjbkwU7+QA/pYgcoCmAe+bAFH3KhaROsCbwH2q+kmcYzNlzNy5bqT0U0+5u2sVx29/60oeY8e60sQll/gTo4kvVdf7aP9+9w058Dnw9e7dMGGC67k2eLAby3DcccmOvnzwM0EsAVqLSAvgW2AAcHU0O4pIVeBV4DlV/Zd/IZqyoKD00Lw5XHddyY4xYQJkZ7vuitnZ0Lp1XEM0xaTqupG+957rtrx589EJYP/+6EcKt2rlqhB79fI37orGtwShqrkiMgyYh+vRNFVVV4nIKFyr+WwROR2XCOoDfUXkYVVtC1yJ6wLbQESu8w55naou9yteU3rNmQNLlrgGwZLefKVaNdewmJ4Ol1/uSiM1asQ3ThPZN9+4hDB/vnsuGKXcrBm0aeOqD2vWdL+XGjVCvw63vnHj2G7raUITLScjiTIyMjQ7OzvZYZg4U4XTT4edO903zipVYjve3Llw4YWuKuLZZ/2pty3tdu1yA6wWLXKPJUugUiU49VQ45RT3KHjdsmXJk/L27a7t57333GPtWre8YUM4/3zo2dM9WrasmL+H0kJElqpqRqh11qfDlGr//jcsXepma401OYCb6OzBB+Ghh6BbN7j55tiPWZodPAiffeYSQUFS+Oort07EfXO/6CKXIL76Cl5/HbZtO7J/Sgq0aFE4aRQ8n3hi4Qv7vn3wwQdHEsJnn7kEX6uWmwzvlltcQmjXzp3PlH5WgjCllqqrEvrxRzcxWby6qObnu4vi/PluuoXTT4/PcZNNFb7++kjJYNEi+PRT19ALcMIJcMYZ0KWLez79dNeHPtiuXS5ZFDzWrDny+sCBI9vVrOkSRevW8O237ny5ua7EcdZZLhmcf747TzySu/FHpBKEJQgTk08/dd/If/1rN81xPKsKXn0VLrvMVQVdc038jgtuquf0dPd62TJ306GyZOdOV2Wzdq27cC9b5i7Q27e79ccc46axPuOMI4+mTWP7/eTnu0QQmDQKno899kiVUbdu1r5TlliCMHG3a5cbV/Dkk+6b/cGDrofRE09EN4itKPn50KmT+8a6erU/A9yWLHGDqc47D958s/Q1cv7445EEUJAMCt7v3Hlku0qV3KjhwGTQtq19azfRsTYIEzf5+W4ag8xMd5G67TZXnz9xopstc9UqeOUVaNIktvO8+iqsWAHPP+/f6OfTT3dxDx3qShMZGW52z/bt3XMixl7u2+dmCw1OAGvXHj09SNOmrjrniivcc0H1TosWxR8bYkw0rARhopad7RLC4sXum/fjj7uRqwVee80NSqtZ001x0a1byc6Tn++Oe+iQSzh+frNXhb/9Dd54wyWkwAba448/kiwKntPSildCUoXvvoP1610iWL++8Osffii8/QknuIt+waMgCZx8slXbGH9YFZOJyY4dbsKzp592F81HHoFBg0LXZ69a5UYqb9rkEsiQIcU/36xZcNVVbqK1gQNjj784vv/eTdWwYsWR59Wr4eef3fqUFHfBDkwc7dq5ZBZ44Q98DmzYrVTJlQRatnQX/YLnVq3cI1SjsTF+sgTho9xcN4XDlVeWv9G5eXkuKYwcCXv2wB13uOqkoi5iu3a5G6vMneu6kU6cGH1f+ry8Izdx+fzz0tEukJvrpnEIThxffx16+xo1jk4ABa9TU0s+rsAYP1gbhI+mTHGNtbNnw8cfl5/+3Z984u6stXSp68P++OPum3I06td3VTYjR7rkuXKlG8V8wglF7ztrlpuR88UXS0dyANcG8qtfuccVVxxZ/uOP7mdbudJVOxUkg+OPt4FfpnywEkQMduxwpYZq1Vw989SpcP31CQ0h7rZtcw3QU6e6gVB//aur7inpBe/FF91ncuyxruE50piDvLwjvW8++6z8JFtjSrNIJQj7F4zB/ffD3r3w9ttuYFBmpptZsizKy4PJk12j6HPPufvdfvmlu49CLN+Gr7rKlayqVIGzz3ZjGsKZMcP1q3/wQUsOxpQG9m9YQsuXu6mnb73V1ZlPmuS+fT/0ULIjK76PP3ZdPIcNc4OrVqxwN2GvXTs+x+/QwY056NbNjZW4807XqBsoNxdGjXKNvpddFp/zGmNiYwmiBFRdg239+q7vP7h+9EOGuLr6lSuTG19xzJsH55zjRuDOmuWmTG7TJv7nadjQnet3v3ON1r/+deEupS+84Pr+P/SQlR6MKS2sDaIEZsxwvXSeeqpwN84dO1wVTYcObrKy0t5QuWyZSw6tWrm7b9Wtm5jzPvec+9xOOMGNnWjXzjUA16njGsVL++dmTHlibRBxtG+fq59PT4cbbyy8rkEDd+u/BQtcr53S7Ouv3bTXDRq4+y0kKjmAm1fpww9du8dZZ7lG7PXrXenBkoMxpYcliGL6y1/chGUTJ4buhjlkCHTsCMOHu7tjlUbbt7tprw8edGMVTjwx8TFkZLiR2Z07w/Tp7rlv38THYYwJzxJEMaxfD48+6m42E24aiZQU12Cdk+OSSWmzfz9cfLEb6Tx7tj/tDdE6/nhXFffII25+Jys9GFO6WIIohuHDXXfNsWMjb9e9u0sijzzikkppkZfnpsj45BPIynJxJlvVqnD33UdGTxtjSg9fE4SI9BGRNSKyTkQyQ6w/R0SWiUiuiPQPWnetiKz1Htf6GWc05s5137jvvz+6Kplx49zF7667/I8tGgU9r157DSZMcPdlNsaYSHxLECKSAkwGLgDSgIEikha02WbgOuCFoH2PBR4EzgC6AA+KSH2/Yi3KwYOue2arVu45Go0bwwMPuFtmzpnjb3zRGDsW/v5318B+xx3JjsYYUxb4WYLoAqxT1Q2qehCYCfQL3EBVN6rqCiA/aN9fA++o6k5V3QW8A/TxMdaIJk1yI3wnTCjevPt33unu33vnnUdu+5gMzz8P99zjZkYdMyZ5cRhjyhY/E8RJwDcB73O8ZXHbV0SGiEi2iGRvCxx1FUdbt7rBcBdd5B7FUbWq6+20bh2MH+9LeEV65x244QZ317Rp02wQmjEmen5eLkL1SYl2VF5U+6rqFFXNUNWMRj7d/uuee9y9AEp6ge/d290f4c9/dt1jE2n5ctfW0KaNmyjP7jpmjCkOPxNEDtA04H0TYEsC9o2bTz5xk8sNHx7bvR4ee8z1IPrDH+IXW1E2bYILLoB69eCttxI7EM4YUz74mSCWAK1FpIWIVAUGALOj3Hce0FtE6nuN0729ZQmTnw+33+4am0eOjO1YLVrAH//opuh4//34xBfJzp1uINzPP7vkcFK0FXvGGBPAtwShqrnAMNyF/QtglqquEpFRInIxgIicLiI5wBXAUyKyytt3J/AnXJJZAozyliXMM8+4kb7xmtV0xAho3twlndzc2I8XzoEDbiDchg2uS2vbtv6dyxhTvtlkfSHs3u0m3WvVCj76KH4jfF95xbUJTJrkptaOt7w8d+vTV15xN+q58sr4n8MYU77YZH3FNGqUm69o0qT4Tv9w6aXQq5cbbBfvTleqblDeK6+4Ng9LDsaYWFmCCLJ6tUsMN93kJpCLJxHX7XXfPrj33vge+9FHXdzDh5ee0dvGmLLNEkQAVTeorVYtGD3an3O0aePO8c9/ujaOWKm6gXB//KO7vecjj8R+TGOMAUsQhbz2Grz7rqti8mlYBeCm4Dj+eNcOkR88hjwK+/e76Ttuuw1atnT3V+jRw3XJtYFwxph4scuJ58ABVz3Tti3ccou/56pTx82NtGiRu7taNDZuhMmT3WjuBg3c8zPPuHs4P/UUvPmmDYQzxsRX5WQHUFo8+qi7CL/3HlROwKcyeDA8+aTr/nrppUcPZDt0yPWgevNNV1pYvdotP/lkd1Oiiy5ytwutXt3/WI0J5dChQ+Tk5PDzzz8nOxQTherVq9OkSROqVKkS9T6WIIDNm93Nffr3h/PPT8w5K1WCxx93d1Z76CE3lcd337lpxd98E95+G/budfefOOcc12h+0UWu+60xpUFOTg61a9cmNTUVsbs9lWqqyo4dO8jJyaFFixZR72cJAjcFhqorRSRSerorDUyaBB98AEuXuuWNG8MVV7iE0KtXfAbqGRNvP//8syWHMkJEaNCgAcWd1LTCJ4ivvoJ//QsefNCNdE600aNhwQI38+uf/uSSQseOdvtNUzZYcig7SvK7qvAJ4pRTYPHi5E1J0aCBu9eEMaZ4duzYQc+ePQH47rvvSElJoWBW58WLF1O1atUij3H99deTmZnJqaeeGnabyZMnU69ePQYNGhRzzN27d+fxxx+nY8eOMR8rESp8ggDXDmCM8VdWlpv4cvNmaNbMlZ5jueY2aNCA5cuXA/DQQw9Rq1Yt7r777kLbqCqqSqUw/b+nTZtW5Hluu+22kgdZxlk3V2OM77KyXHvbpk2uvW/TJvc+Kyv+51q3bh3t2rVj6NChpKens3XrVoYMGUJGRgZt27Zl1KhRh7ft3r07y5cvJzc3l3r16pGZmUmHDh0488wz+eGHHwC47777mDBhwuHtMzMz6dKlC6eeeir//e9/Afjpp5+4/PLL6dChAwMHDiQjI+Nw8gpn+vTpnHbaabRr1457vakVcnNz+e1vf3t4+cSJEwEYP348aWlpdOjQgcGDB8f9MwvHShDGGN+NHOkGeAbav98tj0PNzVFWr17NtGnTePLJJwEYM2YMxx57LLm5uZx33nn079+ftLS0Qvvs2bOHHj16MGbMGIYPH87UqVPJzMw86tiqyuLFi5k9ezajRo1i7ty5TJo0iRNOOIGXX36Zzz77jPT09Ijx5eTkcN9995GdnU3dunXp1asXb7zxBo0aNWL79u18/vnnAOzevRuAcePGsWnTJqpWrXp4WSJEVYIQkZNFpJr3+lwRuUNE6vkbmjGmvNi8uXjLY3XyySdz+umnH34/Y8YM0tPTSU9P54svvmB1wcCiAMcccwwXXHABAJ07d2bjxo0hj33ZZZcdtc2HH37IgAEDAOjQoQNti2jUXLRoEeeffz4NGzakSpUqXH311SxcuJBWrVqxZs0a7rzzTubNm0ddb4BU27ZtGTx4MFlZWcUaxxCraKuYXgbyRKQV8E+gBfCCb1EZY8qVZs2KtzxWNWvWPPx67dq1/O1vf2P+/PmsWLGCPn36hBzcF9ionZKSQm6YG7dU86YsCNymuLdNCLd9gwYNWLFiBd27d2fixIncfPPNAMybN4+hQ4eyePFiMjIyyMvLK9b5SiraBJHv3QDoUmCCqt4FNPYvLGNMeTJ6NNSoUXhZjRr+TWVYIwMAABkSSURBVIoZaO/evdSuXZs6deqwdetW5s2L/80pu3fvzqxZswD4/PPPQ5ZQAnXt2pUFCxawY8cOcnNzmTlzJj169GDbtm2oKldccQUPP/wwy5YtIy8vj5ycHM4//3weeeQRtm3bxv7g+jqfRNsGcUhEBgLXAn29ZYkr5xhjyrSCdoZ49mKKVnp6OmlpabRr146WLVvSrVu3uJ/j9ttv55prrqF9+/akp6fTrl27w9VDoTRp0oRRo0Zx7rnnoqr07duXiy66iGXLlnHjjTeiqogIY8eOJTc3l6uvvpoff/yR/Px8RowYQe0EjZ6N6o5yIpIGDAU+VtUZItICuEpVx/gdYLTieUc5Y0zRvvjiC9q0aZPsMEqF3NxccnNzqV69OmvXrqV3796sXbuWyomY2K0YQv3OIt1RLqroVXU1cId3sPpA7WiSg4j0Af4GpAD/CN7Ha/h+DugM7MAlnY0iUgX4B5Duxficqv4lmliNMSbR9u3bR8+ePcnNzUVVeeqpp0pdciiJqH4CEfkPcLG3/XJgm4i8r6rDI+yTAkwG/gfIAZaIyGwv2RS4Edilqq1EZAAwFrgKuAKopqqniUgNYLWIzFDVjcX+CY0xxmf16tVjacFkauVItI3UdVV1L3AZME1VOwO9itinC7BOVTeo6kFgJtAvaJt+wLPe65eAnuImDFGgpohUBo4BDgJ7o4zVGGNMHESbICqLSGPgSuCNKPc5Cfgm4H2OtyzkNl4vqT1AA1yy+AnYCmwGHlXVncEnEJEhIpItItnFnaXQGGNMZNEmiFHAPGC9qi4RkZbA2iL2CTV1YHCLeLhtugB5wIm4MRe/985ZeEPVKaqaoaoZjfy8R6gxxlRA0TZS/wv4V8D7DcDlReyWAzQNeN8E2BJmmxyvOqkusBO4GpirqoeAH0TkIyAD2BBNvMYYY2IX7VQbTUTkVRH5QUS+F5GXRaRJEbstAVqLSAsRqQoMAGYHbTMbN7YCoD8wX12/283A+eLUBLoCX0b7Qxljyr9zzz33qEFvEyZM4NZbb424X61atQDYsmUL/fv3D3vsorrNT5gwodCAtQsvvDAu8yQ99NBDPJrou5eFEW0V0zTcxfxEXLvBv71lYXltCsNwVVNfALNUdZWIjBKRi73N/gk0EJF1wHCgYGasyUAtYCUu0UxT1RVR/1TGmHJv4MCBzJw5s9CymTNnMnDgwKj2P/HEE3nppZdKfP7gBDFnzhzq1StfU9RFmyAaqeo0Vc31Hs8ARVb6q+ocVT1FVU9W1dHesgdUdbb3+mdVvUJVW6lqF6/qClXd5y1vq6ppqvpICX8+Y0w51b9/f9544w1++eUXADZu3MiWLVvo3r374XEJ6enpnHbaabz++utH7b9x40batWsHwIEDBxgwYADt27fnqquu4sCBA4e3u+WWWw5PFf7ggw8CMHHiRLZs2cJ5553HeeedB0Bqairbt28H4LHHHqNdu3a0a9fu8FThGzdupE2bNvzv//4vbdu2pXfv3oXOE8ry5cvp2rUr7du359JLL2XXrl2Hz5+Wlkb79u0PTxL4/vvv07FjRzp27EinTp348ccfS/zZFoh2JMd2ERkMzPDeD8QNbDPGGH73Oyji9gfF1rEjeNfWkBo0aECXLl2YO3cu/fr1Y+bMmVx11VWICNWrV+fVV1+lTp06bN++na5du3LxxReHve3mE088QY0aNVixYgUrVqwoNF336NGjOfbYY8nLy6Nnz56sWLGCO+64g8cee4wFCxbQsGHDQsdaunQp06ZNY9GiRagqZ5xxBj169KB+/fqsXbuWGTNm8PTTT3PllVfy8ssvR7y/wzXXXMOkSZPo0aMHDzzwAA8//DATJkxgzJgxfP3111SrVu1wtdajjz7K5MmT6datG/v27aN69erF+LRDi7YEcQOui+t3uK6n/YHrYz67McbEILCaKbB6SVW59957ad++Pb169eLbb7/l+++/D3uchQsXHr5Qt2/fnvbt2x9eN2vWLNLT0+nUqROrVq0qciK+Dz/8kEsvvZSaNWtSq1YtLrvsMj744AMAWrRocfh2o5GmFAd3f4rdu3fTo0cPAK699loWLlx4OMZBgwYxffr0wyO2u3XrxvDhw5k4cSK7d++Oy0juaHsxbcaNpD5MRH4HRMjvxpiKItI3fT9dcsklDB8+nGXLlnHgwIHD3/yzsrLYtm0bS5cupUqVKqSmpoac4jtQqNLF119/zaOPPsqSJUuoX78+1113XZHHiTS/XcFU4eCmCy+qiimcN998k4ULFzJ79mz+9Kc/sWrVKjIzM7nooouYM2cOXbt25d133+VXv/pViY5fIJZbjoadZsMYYxKhVq1anHvuudxwww2FGqf37NnDcccdR5UqVViwYAGbNm2KeJxzzjmHLO/+pytXrmTFCtcnZu/evdSsWZO6devy/fff89Zbbx3ep3bt2iHr+c855xxee+019u/fz08//cSrr77K2WefXeyfrW7dutSvX/9w6eP555+nR48e5Ofn880333Deeecxbtw4du/ezb59+1i/fj2nnXYaI0aMICMjgy+/jL3jZyxlkNCVecYYk0ADBw7ksssuK9SjadCgQfTt25eMjAw6duxY5DfpW265heuvv5727dvTsWNHunTpAri7w3Xq1Im2bdseNVX4kCFDuOCCC2jcuDELFiw4vDw9PZ3rrrvu8DFuuukmOnXqFLE6KZxnn32WoUOHsn//flq2bMm0adPIy8tj8ODB7NmzB1Xlrrvuol69etx///0sWLCAlJQU0tLSDt8dLxZRTfcdckeRzarq0/2gis+m+zYmsWy677InrtN9i8iPHD09BrjSwzElDdIYY0zpF7ENQlVrq2qdEI/aqlr2JzuPg6wsSE2FSpXcs1eNaYwxZZ5d5GOQlQVDhkDBYMpNm9x7SMytFI0xxk+x9GKq8EaOPJIcCuzf75YbUxGUtA3TJF5JfleWIGKweXPxlhtTnlSvXp0dO3ZYkigDVJUdO3YUe3S1VTHFoFkzV60Uarkx5V2TJk3IycnBbtZVNlSvXp0mTYqahLswSxAxGD26cBsEQI0abrkx5V2VKlVo0aJFssMwPrIqphgMGgRTpkDz5iDinqdMsQZqY0z5YCWIGA0aZAnBGFM+WQnCGGNMSJYgjDHGhGQJwhhjTEi+JggR6SMia0RknYhkhlhfTURe9NYvEpHUgHXtReRjEVklIp+LSOy3RzLGGBM13xKEiKQAk4ELgDRgoIikBW12I7BLVVsB44Gx3r6VgenAUFVtC5wLHPIrVmOMMUfzswTRBVinqhtU9SAwE+gXtE0/4Fnv9UtAT3G3deoNrFDVzwBUdYeq5vkYqzHGmCB+JoiTgG8C3ud4y0Juo6q5wB6gAXAKoCIyT0SWicgfQ51ARIaISLaIZNtoTmOMiS8/E0SoO84FT9oSbpvKQHdgkPd8qYj0PGpD1SmqmqGqGY0aNYo1XmOMMQH8TBA5QNOA902ALeG28dod6gI7veXvq+p2Vd0PzAHSfYzVGGNMED8TxBKgtYi0EJGqwABgdtA2s4Frvdf9gfnqpoacB7QXkRpe4ugBrPYxVmOMMUF8m2pDVXNFZBjuYp8CTFXVVSIyCshW1dnAP4HnRWQdruQwwNt3l4g8hksyCsxR1Tf9itUYY8zRpLzM5Z6RkaHZ2dnJDsMYY8oUEVmqqhmh1tlIamOMMSFZgjDGGBOSJQhjjDEhWYIwxhgTkiUIY4wxIVmCMMYYE5IliCTLyoLUVKhUyT1nZSU7ImOMceye1EmUlQVDhsD+/e79pk3uPdh9ro0xyWcliCQaOfJIciiwf79bbowxyWYJIok2by7ecmOMSSRLEEnUrFnxlhtjTCJZgkii0aOhRo3Cy2rUcMuNMSbZLEEk0aBBMGUKNG8OIu55yhRroDbGlA7WiynJBg2yhGCMKZ2sBGGMMSYkSxBlnA20M8b4xaqYyjAbaGeM8ZOVIMowG2hnjPGTrwlCRPqIyBoRWScimSHWVxORF731i0QkNWh9MxHZJyJ3+xlnWWUD7YwxfvItQYhICjAZuABIAwaKSFrQZjcCu1S1FTAeGBu0fjzwll8xlnU20M4Y4yc/SxBdgHWqukFVDwIzgX5B2/QDnvVevwT0FBEBEJFLgA3AKh9jLNNsoJ0xxk9+JoiTgG8C3ud4y0Juo6q5wB6ggYjUBEYAD0c6gYgMEZFsEcnetm1b3AIvK2ygnTHGT372YpIQyzTKbR4GxqvqPq9AEZKqTgGmAGRkZAQfu0KwgXbGGL/4mSBygKYB75sAW8JskyMilYG6wE7gDKC/iIwD6gH5IvKzqj7uY7zGGGMC+JkglgCtRaQF8C0wALg6aJvZwLXAx0B/YL6qKnB2wQYi8hCwz5KDMcYklm8JQlVzRWQYMA9IAaaq6ioRGQVkq+ps4J/A8yKyDldyGOBXPMYYY4pH3Bf2si8jI0Ozs7OTHYYxxpQpIrJUVTNCrbOR1MYYY0KyBFHB2WR/xphwbLK+Cswm+zPGRGIliArMJvszxkRiCaICs8n+jDGRWIKowGyyP2NMJJYgKjCb7M8YE4kliArMJvszxkRivZgqOJvszxgTjpUgTExsHIUx5ZeVIEyJ2TgKY8o3K0GYErNxFMaUb5YgTInZOApjyjdLEKbESsM4CmsDMcY/liBMiSV7HEVBG8imTaB6pA3EkoQx8WEJwpRYssdRWBuIMf6yGwaZMqtSJVdyCCYC+fmJj8eYsihpNwwSkT4iskZE1olIZoj11UTkRW/9IhFJ9Zb/j4gsFZHPvefz/YzTJE8sbQiloQ3EmPLMtwQhIinAZOACIA0YKCJpQZvdCOxS1VbAeGCst3w70FdVTwOuBZ73K06TPLG2ISS7DcSY8s7PEkQXYJ2qblDVg8BMoF/QNv2AZ73XLwE9RURU9VNV3eItXwVUF5FqPsZqkiDWNoRkt4EYU975OZL6JOCbgPc5wBnhtlHVXBHZAzTAlSAKXA58qqq/+BirSYJ4jKOwuaSM8Y+fJQgJsSy4STHiNiLSFlftdHPIE4gMEZFsEcnetm1biQM1yWFtCMaUbn4miBygacD7JsCWcNuISGWgLrDTe98EeBW4RlXXhzqBqk5R1QxVzWjUqFGcwzd+Kw9tCDZQz5RnfiaIJUBrEWkhIlWBAcDsoG1m4xqhAfoD81VVRaQe8CZwj6p+5GOMJolKQxtCLBd4G6hnyjtfx0GIyIXABCAFmKqqo0VkFJCtqrNFpDquh1InXMlhgKpuEJH7gHuAtQGH662qP4Q7l42DMMUVPBstuBJMtEkqNdUlhWDNm8PGjfGK0hh/RRoHYQPlTIUV6wW+NAzUy8pyvb42b3ZtN6NHW6O9KZ6kDZQzpjSLtRdVPBrZk13FVdbbUMp6/KWeqpaLR+fOndWY4mjeXNVdWgs/mjePbv/p01Vr1Ci8b40abnki9k92/MlW1uMvLXBV/iGvq0m/sMfrYQnCFFc8LjDTp7sLsoh7Ls6+sV7gRULvL5KY8ydbWY+/tIiUIKyKyVRY8ehFNWiQa6/Iz3fPxdk32VVc8RiomMwqHrthlf+fvyUIU6HFcoGPVawX+FjHkcR6/mR3863oAy0T8flbgjAmSWK9wMdaAor1/Mm+H0d5GGgZi4R8/uHqnsraw9ogTFkUSxtGss8faxtIPJTlzy9W8fr8idAGYeMgjDElUh4GCsYyjiTWgZaxitfnb+MgjDFxF48qnmQ2csdah18hqtjCFS3K2sOqmIxJvFiqWJI9jiHZ3YzjIR5VXFgVkzGmtEl2FVWsU6XEI/7SMFWKVTEZY0qdZI9jSHY342R3E46GJQhjTFIkexxDsrsZJ7sNIxqWIIwxSZHscQxlfSR9IliCMMYkRWm4YVRZHkmfCJYgjDFJk8wLdLIluwQVDUsQxhiTBKWhBFWUyskOwBhjKqpBg0pXQgjmawlCRPqIyBoRWScimSHWVxORF731i0QkNWDdPd7yNSLyaz/jNMYYczTfEoSIpACTgQuANGCgiKQFbXYjsEtVWwHjgbHevmnAAKAt0Af4u3c8Y4wxCeJnCaILsE5VN6jqQWAm0C9om37As97rl4CeIiLe8pmq+ouqfg2s845njDEmQfxMECcB3wS8z/GWhdxGVXOBPUCDKPc1xhjjIz8ThIRYFjzzSbhtotkXERkiItkikr1t27YShGiMMSYcP3sx5QBNA943AbaE2SZHRCoDdYGdUe6Lqk4BpgCIyDYRCTF1VqnRENie7CAisPhiY/HFxuKLTSzxNQ+3ws8EsQRoLSItgG9xjc5XB20zG7gW+BjoD8xXVRWR2cALIvIYcCLQGlgc6WSq2ijO8ceViGSHmzGxNLD4YmPxxcbii41f8fmWIFQ1V0SGAfOAFGCqqq4SkVG4+cdnA/8EnheRdbiSwwBv31UiMgtYDeQCt6lqnl+xGmOMOZqvA+VUdQ4wJ2jZAwGvfwauCLPvaKAUDTo3xpiKxabaSJwpyQ6gCBZfbCy+2Fh8sfElvnJzRzljjDHxZSUIY4wxIVmCMMYYE5IliDgRkaYiskBEvhCRVSJyZ4htzhWRPSKy3Hs8EOpYPse5UUQ+986fHWK9iMhEb6LEFSKSnsDYTg34bJaLyF4R+V3QNgn9DEVkqoj8ICIrA5YdKyLviMha77l+mH2v9bZZKyLXJjC+R0TkS+/396qI1Auzb8S/BR/je0hEvg34HV4YZt+Ik336GN+LAbFtFJHlYfZNxOcX8rqSsL9BVbVHHB5AYyDde10b+ApIC9rmXOCNJMe5EWgYYf2FwFu40exdgUVJijMF+A5onszPEDgHSAdWBiwbB2R6rzOBsSH2OxbY4D3X917XT1B8vYHK3uuxoeKL5m/Bx/geAu6O4ve/HmgJVAU+C/5/8iu+oPV/BR5I4ucX8rqSqL9BK0HEiapuVdVl3usfgS8om/NH9QOeU+cToJ6INE5CHD2B9aqa1NHxqroQN0YnUOAkk88Cl4TY9dfAO6q6U1V3Ae/gZib2PT5VfVvd3GYAn+BmIkiKMJ9fNKKZ7DNmkeLzJg69EpgR7/NGK8J1JSF/g5YgfCDuvhadgEUhVp8pIp+JyFsi0jahgTkKvC0iS0VkSIj1pWWixAGE/8dM9md4vKpuBfcPDBwXYpvS8jnegCsRhlLU34KfhnlVYFPDVI+Uhs/vbOB7VV0bZn1CP7+g60pC/gYtQcSZiNQCXgZ+p6p7g1Yvw1WZdAAmAa8lOj6gm6qm4+7TcZuInBO0PqqJEv0kIlWBi4F/hVhdGj7DaJSGz3EkbiaCrDCbFPW34JcngJOBjsBWXDVOsKR/fsBAIpceEvb5FXFdCbtbiGXF+gwtQcSRiFTB/RKzVPWV4PWquldV93mv5wBVRKRhImNU1S3e8w/Aqxx9n42oJkr02QXAMlX9PnhFafgMge8Lqt285x9CbJPUz9FrkPwNMEi9CulgUfwt+EJVv1fVPFXNB54Oc95kf36VgcuAF8Ntk6jPL8x1JSF/g5Yg4sSrr/wn8IWqPhZmmxO87RCRLrjPf0cCY6wpIrULXuMaM1cGbTYbuMbrzdQV2FNQlE2gsN/ckv0ZegommcR7fj3ENvOA3iJS36tC6e0t852I9AFGABer6v4w20Tzt+BXfIFtWpeGOe/hyT69EuUA3OeeKL2AL1U1J9TKRH1+Ea4rifkb9LMFviI9gO644tsKYLn3uBAYCgz1thkGrML1yPgEOCvBMbb0zv2ZF8dIb3lgjIK7Vex64HMgI8Ex1sBd8OsGLEvaZ4hLVFuBQ7hvZDfibmr1HrDWez7W2zYD+EfAvjfg7oa4Drg+gfGtw9U9F/wdPulteyIwJ9LfQoLie97721qBu9A1Do7Pe38hrtfO+kTG5y1/puBvLmDbZHx+4a4rCfkbtKk2jDHGhGRVTMYYY0KyBGGMMSYkSxDGGGNCsgRhjDEmJEsQxhhjQrIEYUwRRCRPCs8yG7eZRUUkNXAmUWNKE1/vSW1MOXFAVTsmOwhjEs1KEMaUkHc/gLEisth7tPKWNxeR97zJ6N4TkWbe8uPF3Z/hM+9xlneoFBF52pvv/20ROcbb/g4RWe0dZ2aSfkxTgVmCMKZoxwRVMV0VsG6vqnYBHgcmeMsex02Z3h43Ud5Eb/lE4H11Ew2m40bgArQGJqtqW2A3cLm3PBPo5B1nqF8/nDHh2EhqY4ogIvtUtVaI5RuB81V1gzeh2neq2kBEtuOmjzjkLd+qqg1FZBvQRFV/CThGKm7O/tbe+xFAFVX9s4jMBfbhZqx9Tb1JCo1JFCtBGBMbDfM63Dah/BLwOo8jbYMX4ebF6gws9WYYNSZhLEEYE5urAp4/9l7/Fzf7KMAg4EPv9XvALQAikiIidcIdVEQqAU1VdQHwR6AecFQpxhg/2TcSY4p2jBS+cf1cVS3o6lpNRBbhvmwN9JbdAUwVkT8A24DrveV3AlNE5EZcSeEW3EyioaQA00WkLm6G3fGqujtuP5ExUbA2CGNKyGuDyFDV7cmOxRg/WBWTMcaYkKwEYYwxJiQrQRhjjAnJEoQxxpiQLEEYY4wJyRKEMcaYkCxBGGOMCen/AY+1oCJ4UDTGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label = 'Training loss')\n",
    "plt.plot(epochs,val_loss,'b',label = 'Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label = 'Training acc')\n",
    "plt.plot(epochs,val_acc,'b',label = 'Validation acc')\n",
    "plt.title('Training and Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "15000/15000 [==============================] - 1s 63us/step - loss: 0.0142 - acc: 0.9845\n",
      "Epoch 2/3\n",
      "15000/15000 [==============================] - 1s 57us/step - loss: 0.0080 - acc: 0.9915\n",
      "Epoch 3/3\n",
      "15000/15000 [==============================] - 1s 58us/step - loss: 0.0115 - acc: 0.9873\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'mse',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs = 3,\n",
    "                   batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 87us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13587923765533372, 0.8522400259971619]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.3023310e-04],\n",
       "       [9.9992001e-01],\n",
       "       [9.9987996e-01],\n",
       "       ...,\n",
       "       [5.1584840e-04],\n",
       "       [7.7226758e-04],\n",
       "       [9.9364007e-01]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4.7 정리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **원본 데이터를 신경망에 텐서로 주입하기 위해서는 꽤 많은 전처리가 필요하다. 단어 시퀀스는 이진 벡터로 인코딩될 수 있고 다른 인코딩 방식도 있다.**\n",
    "\n",
    "> - **relu 활성화 함수와 함께 Dense층을 쌓은 네트워크는 (감성 분류를 포함하여) 여러 종류의 문제에 적용할 수 있어 앞으로도 자주 사용**\n",
    "\n",
    "> - **(출력 클래스가 2개인) 이진 분류 문제에서 네트워크는 하나의 유닛과 sigmoid 활성화 함수를가진 Dense층으로 끝나야 한다. 이 신경망의 출력은 확률을 나타내는 0과 1사이의 스칼라 값이다.**\n",
    "\n",
    "> - **이진 분류 문제에서 이런 스칼라 시그모이드 출력에 대해 사용할 손실 함수는 binary_crossentropy이다.**\n",
    "\n",
    "> - **rmsprop 옵티마이저는 문제에 상관없이 일반적으로 충분히 좋은 선택이다. 걱정할 거리가 하나 줄은 셈**\n",
    "\n",
    "> - **훈련 데이터에 대해 성능이 향상됨에 따라 신경망은 과대적합되기 시작하고 이전에 본적 없는 데이터에서는 결과가 점점 나빠지게 된다. 항상 훈련 세트 이외의 데이터에서 성능을 모니터링 해야한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5 뉴스 기사 분류: 다중 분류 문제**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2개 이상의 클래스를 분류해야하는 경우는 어떻게 해야 할까?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**로이터 뉴스를 46개의 상호 배타적인 토픽으로 분류하는 신경망을 만들어 본다.**\n",
    "\n",
    "> - **클래스가 많기 때문에 이 문제는 다중 분류(multiclass classification)의 예이다.**\n",
    "\n",
    "> - **각 데이터 포인트가 정확히 하나의 범주로 분류되기 때문에 좀 더 정확히 말하면 단일 레이블 다중 분류(single-label, multiclass classification)이다.**\n",
    "\n",
    "> - **각 데이터 포인트가 여러 개의 범주에 속할 수 있다면 이것은 다중 레이블 다중 분류(multi-label, multiclass classification) 문제가 된다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5.1 로이터 데이터셋**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1986년에 로이터에서 공개한 짧은 뉴스 기사와 토픽의 집합인 로이터 데이터셋을 사용한다.**\n",
    "\n",
    "> - **이 데이터셋은 텍스트 분류를 위해 널리 사용되는 간단한 데이터셋이다.**\n",
    "\n",
    "> - **46개의 토픽이 있으며 어떤 토픽은 다른 것에 비해 데이터가 많다. 각 토픽은 훈련 세트에 최소한 10개의 샘플을 가지고 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data,test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data)) # 8982개의 훈련샘플\n",
    "print(len(test_data)) # 2246개의 테스트 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data[10] #각 샘플은 정수 리스트이다.(단어 인덱스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 단어 decoding\n",
    "word_index = reuters.get_word_index() \n",
    "# word_index에 있는 key와 value를 reverse하여 딕셔너리 형태로 변수에 할당\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "# train_data의 첫 번째 리뷰를 key -> value로 변환된 결과를 decoded_newswire에 할당\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i-3,'?') for i in train_data[0]])\n",
    "print(decoded_newswire)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5.2 데이터 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이전의 예제와 동일한 코드를 사용해서 데이터를 벡터로 변환한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences,dimension = 10000): # data_size, 자주 출현하는 단어 10000\n",
    "    results = np.zeros((len(sequences),dimension)) # 동일한 크기의 0행렬\n",
    "    for i, sequences in enumerate(sequences): # 각 단어 별 인덱스 부여\n",
    "        results[i,sequences] = 1. # 각 데이터 마다 해당 단어의 인덱스 위치에 1을 추가\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data) # 훈련 데이터 벡터 변환\n",
    "x_test = vectorize_sequences(test_data) # 테스트 데이터 벡터 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**레이블을 벡터로 바꾸는 방법은 두 가지이다.**\n",
    "\n",
    "> - **레이블의 리스트를 정수 텐서로 변환하는 것**\n",
    "\n",
    "> - **원-핫 인코딩을 사용하는 것 (원-핫 인코딩이 범주형 데이터에 널리 사용되기 때문에 범주형 인코딩이라고도 부른다.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension = 46):\n",
    "    results = np.zeros((len(labels), dimension)) # data_size(label), 46개의 토픽\n",
    "    for i, label in enumerate(labels): # 각 레이블에 인덱스를 할당하고, labels의 값들을 label에 할당\n",
    "        results[i,label]= 1. # 즉, 첫 번째 labels이라면 result[0,3] -> 첫 번재 리뷰는 3번째 토픽을 의미\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train_label = to_one_hot(train_labels) # 훈련 레이블 벡터 변환\n",
    "one_hot_test_label = to_one_hot(test_labels) # 테스트 레이블 벡터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST 예제에서 이미 보았듯이 케라스에는 이를 위한 내장 함수가 있다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5.3 모델 구성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 토픽 분류 문제는 이전의 영화 리뷰 분류 문제와 비슷해 보인다. 두 경우 모두 짧은 텍스트를 분류하는 것이다. 하지만 이 예제에서는 출력 클래스의 개수가 2에서 46개로 늘어난 점이다.**\n",
    "\n",
    "> - **즉, 출력 공간의 차원이 훨씬 커졌다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이전에 사용했던 것처럼 Dense 층을 쌓으면 각 층은 이전 층의 출력에서 제공한 정보만 사용할 수 있다. 한 층이 분류 문제에 필요한 일부 정보를 누락하면 그다음 층에서 이를 복원할 방법이 없다.** \n",
    "\n",
    "> - **각 층은 잠재적으로 정보의 병목이 될 수 있다.**\n",
    "\n",
    "> - **이전 예제에서 16차원을 가진 중간층을 사용했지만 16차원 공간은 46개의 클래스를 구분하기에 너무 제약이 많다.**\n",
    "\n",
    "> - **따라서 16차원의 공간은 46개의 클래스를 구분하기에 너무 제약이 많으므로 은닉 유닛을 64개로 늘린다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 구조에서 주목해야 할 점**\n",
    "\n",
    "> - **마지막 Dense 층의 크기가 46이다. 각 입력 샘플에 대해서 46차원의 벡터를 출력한다는 뜻이다. 이 벡터의 각 원소(각 차원)는 각기 다른 출력 클래스가 인코딩된 것이다.**\n",
    "\n",
    "> - **마지막 층에 softmax 활성화 함수가 사용되었다. 각 입력 샘플마다 46개의 출력 클래스에 대한 확률 분포를 출력한다. 즉, 46차원의 출력 벡터를 만들며 output[i]는 어떤 샘플이 클래스 i에 속할 확률이다. 46개의 값을 모두 더하면 1이 된다.**\n",
    "\n",
    "**이런 문제에 사용할 최선의 손실 함수는 categorical_crossentropy이다. 이 함수는 두 확률 분포 사이의 거리를 측정한다. 여기에서는 네트워크가 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리이다. 두 분포사이의 거리를 최소화하면 진짜 레이블에 가능한 가까운 출력을 내도록 모델을 훈련하게 된다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5.4 훈련 검증**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**훈련 데이터에서 1,000개의 샘플을 따로 떼어서 검증 세트로 사용한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 89us/step - loss: 2.6946 - accuracy: 0.5083 - val_loss: 1.7783 - val_accuracy: 0.6430\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 1.4445 - accuracy: 0.7112 - val_loss: 1.3306 - val_accuracy: 0.7200\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 1.0758 - accuracy: 0.7740 - val_loss: 1.1458 - val_accuracy: 0.7620\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.8486 - accuracy: 0.8230 - val_loss: 1.0435 - val_accuracy: 0.7970\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.6773 - accuracy: 0.8592 - val_loss: 0.9723 - val_accuracy: 0.8050\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 0.5455 - accuracy: 0.8855 - val_loss: 0.9782 - val_accuracy: 0.7840\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.4397 - accuracy: 0.9107 - val_loss: 0.9058 - val_accuracy: 0.8150\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.3569 - accuracy: 0.9258 - val_loss: 0.9180 - val_accuracy: 0.8200\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.2964 - accuracy: 0.9362 - val_loss: 0.9118 - val_accuracy: 0.8210\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.2449 - accuracy: 0.9434 - val_loss: 0.9228 - val_accuracy: 0.8190\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.2143 - accuracy: 0.9473 - val_loss: 0.9327 - val_accuracy: 0.8150\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.1859 - accuracy: 0.9503 - val_loss: 0.9524 - val_accuracy: 0.8190\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.1676 - accuracy: 0.9549 - val_loss: 0.9724 - val_accuracy: 0.8110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.1537 - accuracy: 0.9554 - val_loss: 0.9964 - val_accuracy: 0.8150\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.1395 - accuracy: 0.9544 - val_loss: 0.9859 - val_accuracy: 0.8140\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1327 - accuracy: 0.9562 - val_loss: 1.0472 - val_accuracy: 0.8120\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.1253 - accuracy: 0.9548 - val_loss: 1.0233 - val_accuracy: 0.8120\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1192 - accuracy: 0.9574 - val_loss: 1.0485 - val_accuracy: 0.8110\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.1188 - accuracy: 0.9590 - val_loss: 1.0559 - val_accuracy: 0.8110\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.1133 - accuracy: 0.9582 - val_loss: 1.0814 - val_accuracy: 0.8180\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs = 20,\n",
    "                   batch_size=512,\n",
    "                   validation_data = (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss)+1) #범위 1 ~ 21\n",
    "\n",
    "plt.plot(epochs,loss,'bo',label = 'Training loss')\n",
    "plt.plot(epochs,val_loss,'b',label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de5xVZb3H8c8P5CKC3AZFQS6amUKA4zRo4j0JvECJF4iTF1LSE6Aeu5B4ylTM1NQs8oimx3KSSNPgpJgiaWYogzKAkEIKOEKIhFwEgcHf+ePZw+wZ9szsuay998z6vl+v9dp7XfePNZv12+t5nvU85u6IiEh8tch2ACIikl1KBCIiMadEICISc0oEIiIxp0QgIhJz+2U7gLrKy8vzPn36ZDsMEZEmZeHChR+6e7dU65pcIujTpw/FxcXZDkNEpEkxs9XVrVPRkIhIzCkRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIRKyoCPr0gRYtwmtRUbYjqkyJQESkFg25kBcVwfjxsHo1uIfX8ePrfowoE4kSgUgMNPRCEuf9G3ohnzIFtm+vvGz79rA8E5+fFndvUtNxxx3nIpK+Rx91b9fOPVxGwtSuXViu/WvXu3flfcun3r3T298s9f5mmfn8ckCxV3NdzfqFva6TEoHE0aOPhv/4ZuE13YuYe8MvJHHfP9sX8oZ+frmaEoGKhkQi1hjFGg0pGlizpm7LtX9lvXrVbXlVU6dCu3aVl7VrF5Zn4vPToUQgEqHGKN9taBlzQy8kcd+/oRfysWNh+nTo3RvMwuv06WF5Jj4/LdXdKuTqpKIhybRsFsu4N7xoINtl7E19//Jj1Pc70Bga4/NRHYFI/TT0ItIY5buNkUwaeiGJ+/7NQU2JwML6pqOgoMDVDbVkSp8+oTinqt69YdWq6PeHiuKl5OKhdu3qVrwgYmYL3b0g1TrVEUiz15DK2oZWNDZG+W5Dy5hFatPkBqYRqYuqv6bLK2shvQtpr16pf9GnW9FY/hlTpoTk0atXSAJ1vYiPHasLv0RHRUPSrDW0aEbFMtJcqGhIYquhRTsqlpE4UNGQNGsNLdoBFctI86c7Asl5DanszcjDOCJNnBKB5LSGPpmroh2R2qmyWHJaY7TDFxFVFksT1tDKXhGpnRKB5LRM9LwoEndKBJLTVNkrEj0lAolcQ1r9qLJXJHp6jkAi1dAuHsq304VfJDq6I5BINXRQFRGJnhKBREqtfkRynxKBREqtfkRyX6SJwMyGmdlbZrbSzCanWN/bzOaa2WIz+4uZ9YwyHsk8tfoRyX2RJQIzawlMA4YDxwBjzOyYKpvdCfza3QcANwE/jioeyQ61+hHJfVG2GioEVrr7OwBmNgMYCSxL2uYY4NrE+3nAUxHGI1miVj8iuS3KoqEewHtJ86WJZclKgFGJ918FOphZ16oHMrPxZlZsZsUbNmyIJFgRkbiKMhFYimVVe7j7NnCKmb0BnAK8D5Tts5P7dHcvcPeCbt26NX6kIiIxFmXRUClwWNJ8T2Bt8gbuvhY4D8DM2gOj3H1zhDGJiEgVUd4RLACONLO+ZtYaGA3MSt7AzPLMrDyG7wMPRRiP1FNDuogQkdwXWSJw9zJgAvAssByY6e5vmtlNZjYisdmpwFtm9jZwMKBGhTmmoQPDiEju08A0UiMNDCPSPGhgGqk3dREh0vwpEUiN1EWESPOnRCA1UhcRIs2fEoHUSF1EiDR/GphGaqUuIkSaN90RiIjEnBKBiEjMKRGIiMScEoGISMwpEcSA+goSkZqo1VAzV95X0PbtYb68ryBQSyARCXRH0MxNmVKRBMpt3x6Wi4iAEkGzp76CRKQ2SgTNnPoKEpHaKBE0c+orSERqo0TQzKmvIBGpjVoNxYD6ChKRmuiOQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiTomgCdB4AiISJT1ZnOM0noCIRE13BDlO4wmISNR0R5Dj4jyewK5dsGoV/POfYXrvPdixA3burN9UVgbt28OBB9Zv6tgRDjoIWrbM/LnYswc2b4bOnUPngSKNSYkgx/XqFYqDUi1vDrZurbjQr1xZ8b78wv/ppxXbtm4dutBu06b6qX371Mtbt4b99oOPP4YtWyqmdevgrbcq5j/5pOZ4W7aEQw+Fnj2rnw45BFq1Sv8c7N4d4igtrZjee6/y/Lp1IRkccggcf3zFdNxxcMAB9Tv3kvs2boQ33qiYrrgCTjut8T9HiSDHTZ1auY4AmuZ4Ap98An/4Q7joJl/sN2yovF1eHhxxBJx4YnhNnrp3j/7X8K5dITklJ4vyadMmWLu24kJdUgJ/+tO+RXctWoRYqyaIvLzw702+wJeWwr/+Be6Vj9GuHRx2WNjvjDPCa8eOsHgxzJ8PTz4ZtmvZEgYMqJwcjjyy8c7Trl3w/vsVcbZsmTrJVpeYW7XSHUw63MM5fuMNeP31igv/e+9VbNOzJ5xzTjSfb171G5jjCgoKvLi4ONthZFRRUagTWLMm3AlMndq0KorXr4evfCVcwMzCBa784v6Zz1S8P/zwcLFrStzho4/2vbhXnbZsqdinY8ea7yjKL/o1XUA//BBefTWc0/nzw/utW8O6Ll0qJ4bCwtTndceOiot8ddP69Q07P2YViaJtW+jQofpit9qK5jp0aHixXKtWIVFn0549sGJF5V/6b7wRfv1DOGef/Swce2zlKS+vYZ9rZgvdvSDlOiUCidLSpeFXzAcfwMMPh4TQpk22o8q8LVvCxbtbt3BBa2x79sA//gF//3tFcli2LCQqMzj6aBg0KNQzlF/kyy88yTp3rj45de8etvnkk/rV0XzyCWzblvpua/PmUESWCfvtV3PxYk1TQ5LI7t3hb1JSUnEX2bo19O9f+YI/YEAo4mxsSgSSFXPmwIUXhi/1rFlQkPIrKFHZvBkWLKhIDEuWQNeu1V/oe/TIbn3Dzp2VE0PVZLF1a+U6o7pyDxfj+jY22Llz3yK8umjRItwB5+dXXPSPPjokg0yoKRGojkAiMW0aTJoEn/88zJ4dioMkszp2hC99KUxNQZs24Y6pW7dsRxI/kZaWmdkwM3vLzFaa2eQU63uZ2Twze8PMFpvZWVHGEyfu8M478NvfhgtyYWF4KvnnP4/2FrysLHzehAlw1lnw8stKAiK5LrI7AjNrCUwDzgRKgQVmNsvdlyVtdgMw093vM7NjgKeBPlHF1Jxt3QrFxRXFAPPnh3J5CC1QCgtDRfOkSXDffXD33fDlLzd+DKNHw9NPw7XXwh13ZKfNvYjUTZRFQ4XASnd/B8DMZgAjgeRE4MCBifcdgbURxtNsfPppaIaZfNFfurSi/PSoo2D48NBi5IQToF+/UEHmHsrqr7sOhg2Ds8+Gu+4KLRQaas2aUCm8bFlINFde2fBjikhmRJkIegBJrWApBQZX2eZG4M9mNhE4AEhZmmlm44HxAL2ay5NUdbRhQyh3//vfQ1PBzZvD8k6dYPBg+OpXK5oKdumS+hhmMHJkSAL33gs33xySxKRJ8N//HY5VH6+9BiNGhOaIzzwDZ55Zv+OISHZE1mrIzC4Avuzulyfmvw4UuvvEpG3+KxHDT83sBOBXQH93r7ZtQBxbDb3/fnioaMWK0NSs/Jf+8ceHX/P1bdK2fn14PuGhh0Jrkltugcsvr1txzu9/DxdfHJ54/b//g2OOqV8sIhKtmloNRVlZXAokVxP2ZN+in28AMwHc/e9AW6CBj000L+++CyedFJ5o/ctfQhvk+++HSy+Fz32uYe2aDz4YHnwQFi4MzdiuvDI0bZs3r/Z93eHWW0Pz0Pz8cJeiJCDSNEWZCBYAR5pZXzNrDYwGZlXZZg1wBoCZHU1IBFU6HYivt9+Gk08OXRs8/3xICFE49lh48UWYOTMUOZ1+OowaFVodpbJzJ1x2Wbib+NrXYO5cNfkTacoiSwTuXgZMAJ4FlhNaB71pZjeZ2YjEZtcBV5hZCfAYcKk3tSfcIrJ0aUgCO3eGO4HCwmg/zwwuuACWLw91B3PmhLuE73+/ousCCE/HnnkmPPII3HgjPPpo6DpARJouPVmcgxYuhKFDwwX2+efDBTnT3n8/JIHf/CZ0LXDrraFO4txzQ/cEDz8MY8ZkPi4RqZ9s1RFIPbzySiia6dABXnopO0kAQncDv/51aJrapw+MGxdaGG3ZAi+8oCQg0pwoEeSQF14IdwIHHwx//WvokTPbBg+Gv/0tFAFdcEGoFP7iF7MdlYg0JiWCDEhn8Pmnnw5dMvTtG+4EcqlbhhYtQrfXv/tdiE9EmpdaE4GZTTCzzpkIpjkqH3x+9erQ5LJ88PnkZPDEE6F75n79QsVweXe/IiKZkM4dQXdCP0EzE53IabyhOqht8PmiIrjootBF89y54cEuEZFMqjURuPsNwJGEp34vBVaY2a1mlgMl2LmvpsHnH3gAvv710Ez0z3+ufxcPIiINkVYdQaJt/78SUxnQGXjczG6PMLZmobqukTp1CkVEw4eHcW+jGJFIRCQd6dQRTDKzhcDtwN+Az7v7VcBxwKiI42vypk4N3UAna9UqPC183nlhEPL9989ObCIikF7vo3nAee6+Onmhu39qZudEE1bzUT7I/JQpoaL4wANDW/yxY+F//zd0Dy0ikk3pFA09Dfy7fMbMOpjZYAB3Xx5VYM3J2LGh87hrrglJ4PLLQxcNSgIikgvSSQT3AduS5j9OLJM6mDwZ7rkHrr4apk/XyF0ikjvSSQSW3BFcYqwA/Zatg//5H7j9drjqqjBEpBrgikguSScRvJOoMG6VmK4GqumgWKr605/gW98Kwzjee6+SgIjknnQSwZXAF4H3qRhucnyUQTUXr78eHhYbNAgee0x1AiKSm2q9NLn7B4RBZaQO1qwJg8N37RqGcNRzAiKSq2pNBGbWljCkZD/CCGIAuPu4CONq0j76KHQgt2NHGE/gkEOyHZGISPXSKRr6DaG/oS8DLxLGHt5a4x4xtmtXGObx7bfhD38IHcmJiOSydBLBZ9z9v4GP3f0R4Gzg89GG1TS5h24jXnghDAp/+unZjkhEpHbpJILdidePzKw/0BHoE1lETdhNN1WM5XvxxdmORkQkPem0Y5meGI/gBmAW0B7470ijaoLKE8Cll8IPfpDtaERE0ldjIjCzFsAWd98EvAQcnpGompgXXgjdRpxxBtx/v54VEJGmpcaiocRTxBMyFEuT9OaboRfRo46Cxx+H1q2zHZGISN2kU0fwnJl928wOM7Mu5VPkkTUB69aFZqLt2oUxhzWwjIg0RenUEZQ/L/CtpGVOzIuJtm0L3UZs3BgGm69uABoRkVyXzpPFfTMRSFNSVgajR8OiRTB7NuTnZzsiEZH6S+fJ4pQNId39140fTu5zh0mTQmdy990XioZERJqydIqGvpD0vi1wBvA6EMtE8NOfhgTw3e/ClVdmOxoRkYZLp2hoYvK8mXUkdDsRO7//PXznO3DhhfDjH2c7GhGRxpFOq6GqtgNHNnYgua64GL7+dTjxxPDwWIv6nDkRkRyUTh3BbEIrIQiJ4xhgZpRB5aLrrw/NQ596Ctq2rX17EZGmIp06gjuT3pcBq929NKJ4ctLNN8Nzz4X3BQUwdWoYkF5EpDlIJxGsAda5+ycAZra/mfVx91WRRpYjiorgRz+qmF+9OvQwCkoGItI8pFPS/Xvg06T5PYllsfDd78KePZWXbd8OU6ZkJx4RkcaWTiLYz913lc8k3semR521a1MvX7Mms3GIiEQlnUSwwcxGlM+Y2Ujgw+hCyh2rV1e/Tl1KiEhzkU4iuBK43szWmNka4HvAN9M5uJkNM7O3zGylmU1Osf5uM1uUmN42s4/qFn607rwTWrbct5VQu3ahwlhEpDlI54GyfwLHm1l7wNw9rfGKzawlMA04EygFFpjZLHdflnTsa5O2nwgcW8f4I7N+fRhu8pJLwpCTU6aE4qBevdRqSESal1rvCMzsVjPr5O7b3H2rmXU2s1vSOHYhsNLd30nUK8wARtaw/RjgsfTCjt7PfgY7d4bK4rFjYdUq+PTT8KokICLNSTpFQ8PdfW+RTWK0snS6WusBvJc0X5pYtg8z6w30BV6oZv14Mys2s+INGzak8dENs3kzTJsG558fBpwREWnO0kkELc2sTfmMme0PtKlh+72bpljmKZYBjAYed/c9qVa6+3R3L3D3gm7duqXx0Q3zy1/Cli3w/e9H/lEiIlmXzgNljwJzzezhxPxlwCNp7FcKHJY03xOopjEmo6k88E3WbN8Od98Nw4bBsTlTYyEiEp10KotvN7PFwJcIv/LnAL3TOPYC4Egz6wu8T7jYf63qRmZ2FNAZ+Hsd4o7MQw/Bhg26GxCR+Ei3D81/EZ4uHkUYj2B5bTu4exlh4PtnE9vPdPc3zeym5OcSCJXEM9y9umKjjNm9G+64A774RTjppGxHIyKSGdXeEZjZZwm/4scAG4HfEZqPnpbuwd39aeDpKst+UGX+xjrEG6nf/jY0Ef3lL8FS1XCIiDRDNRUN/QP4K3Cuu68EMLNra9i+Sfv0U7jtNhgwQMNPiki81JQIRhHuCOaZ2RzCcwDN9nfyU0/BP/4Bjz2muwERiZdq6wjc/Ul3vwj4HPAX4FrgYDO7z8yGZii+jHAPQ08ecUR4dkBEJE5qrSx294/dvcjdzyE0AV0E7NNvUFM2d24YivJ734P90mlQKyLSjNRp5F13/7e73+/up0cVUDbceisceihcfHG2IxERybzYD8E+fz7MmwfXXQdt0nleWkSkmYl9Ivjxj6FLl4rhJ0VE4ibWiWDpUpg1CyZOhPbtsx2NiEh2xDoR3HYbHHBASAQiInEV20TwzjswYwZ885vQtWu2oxERyZ7YJoI77gjDUP7Xf2U7EhGR7IplIli3Dh5+OAxD2SPlUDkiIvERy0Rw992hp9HvfjfbkYiIZF/sEsGmTXDffXDhhfCZz2Q7GhGR7ItdIpg2DbZt08AzIiLlYpUIPv4Y7rkHzj47dDctIiIxSwQPPggbN8L112c7EhGR3BGbRLBrF9x5J5x8chiKUkREgth0uvzoo1BaCg88kO1IRERyS2zuCI44Aq64Ar785WxHIiKSW2JzR3DKKWESEZHKYnNHICIiqSkRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzCkRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzCkRiIjEXKSJwMyGmdlbZrbSzCZXs82FZrbMzN40s99GGY+IiOwrsvEIzKwlMA04EygFFpjZLHdflrTNkcD3gRPdfZOZHRRVPCIiklqUdwSFwEp3f8fddwEzgJFVtrkCmObumwDc/YMI4xERkRSiTAQ9gPeS5ksTy5J9Fvismf3NzOab2bBUBzKz8WZWbGbFGzZsiChcEZF4ijIRWIplXmV+P+BI4FRgDPCgmXXaZyf36e5e4O4F3bp1a/RARUTiLMpEUAocljTfE1ibYps/uvtud38XeIuQGEREJEOiTAQLgCPNrK+ZtQZGA7OqbPMUcBqAmeURioreiTAmERGpIrJE4O5lwATgWWA5MNPd3zSzm8xsRGKzZ4GNZrYMmAd8x903RhWTiIjsy9yrFtvntoKCAi8uLs52GCIiTYqZLXT3glTr9GSxiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzCkRiIjEXGS9j4pI87J7925KS0v55JNPsh2K1KBt27b07NmTVq1apb2PEoGIpKW0tJQOHTrQp08fzFJ1JSbZ5u5s3LiR0tJS+vbtm/Z+KhoSkbR88skndO3aVUkgh5kZXbt2rfNdmxKBiKRNSSD31edvpEQgIhJzSgQiEomiIujTB1q0CK9FRQ073saNGxk0aBCDBg2ie/fu9OjRY+/8rl270jrGZZddxltvvVXjNtOmTaOoocE2MaosFpFGV1QE48fD9u1hfvXqMA8wdmz9jtm1a1cWLVoEwI033kj79u359re/XWkbd8fdadEi9W/chx9+uNbP+da3vlW/AJsw3RGISKObMqUiCZTbvj0sb2wrV66kf//+XHnlleTn57Nu3TrGjx9PQUEB/fr146abbtq77ZAhQ1i0aBFlZWV06tSJyZMnM3DgQE444QQ++CAMmX7DDTdwzz337N1+8uTJFBYWctRRR/HKK68A8PHHHzNq1CgGDhzImDFjKCgo2Jukkv3whz/kC1/4wt74ynt7fvvttzn99NMZOHAg+fn5rFq1CoBbb72Vz3/+8wwcOJApUZysaigRiEijW7OmbssbatmyZXzjG9/gjTfeoEePHtx2220UFxdTUlLCc889x7Jly/bZZ/PmzZxyyimUlJRwwgkn8NBDD6U8trvz2muvcccdd+xNKj//+c/p3r07JSUlTJ48mTfeeCPlvldffTULFixgyZIlbN68mTlz5gAwZswYrr32WkpKSnjllVc46KCDmD17Ns888wyvvfYaJSUlXHfddY10dmqnRCAija5Xr7otb6gjjjiCL3zhC3vnH3vsMfLz88nPz2f58uUpE8H+++/P8OHDATjuuOP2/iqv6rzzzttnm5dffpnRo0cDMHDgQPr165dy37lz51JYWMjAgQN58cUXefPNN9m0aRMffvgh5557LhAeAGvXrh3PP/8848aNY//99wegS5cudT8R9aREICKNbupUaNeu8rJ27cLyKBxwwAF7369YsYKf/exnvPDCCyxevJhhw4albFffunXrve9btmxJWVlZymO3adNmn23SGdBr+/btTJgwgSeffJLFixczbty4vXGkauLp7llrnqtEICKNbuxYmD4devcGs/A6fXr9K4rrYsuWLXTo0IEDDzyQdevW8eyzzzb6ZwwZMoSZM2cCsGTJkpR3HDt27KBFixbk5eWxdetWnnjiCQA6d+5MXl4es2fPBsKDetu3b2fo0KH86le/YseOHQD8+9//bvS4q6NWQyISibFjM3Phryo/P59jjjmG/v37c/jhh3PiiSc2+mdMnDiRiy++mAEDBpCfn0///v3p2LFjpW26du3KJZdcQv/+/enduzeDBw/eu66oqIhvfvObTJkyhdatW/PEE09wzjnnUFJSQkFBAa1ateLcc8/l5ptvbvTYU9GYxSKSluXLl3P00UdnO4ycUFZWRllZGW3btmXFihUMHTqUFStWsN9+ufHbOtXfqqYxi3MjahGRJmTbtm2cccYZlJWV4e7cf//9OZME6qPpRi4ikiWdOnVi4cKF2Q6j0aiyWEQk5pQIRERiTolARCTmlAhERGJOiUBEmoRTTz11n4fD7rnnHv7zP/+zxv3at28PwNq1azn//POrPXZtzdLvuecetif1pHfWWWfx0UcfpRN6zlMiEJEmYcyYMcyYMaPSshkzZjBmzJi09j/00EN5/PHH6/35VRPB008/TadOnep9vFyi5qMiUmfXXAMpel1ukEGDINH7c0rnn38+N9xwAzt37qRNmzasWrWKtWvXMmTIELZt28bIkSPZtGkTu3fv5pZbbmHkyJGV9l+1ahXnnHMOS5cuZceOHVx22WUsW7aMo48+em+3DgBXXXUVCxYsYMeOHZx//vn86Ec/4t5772Xt2rWcdtpp5OXlMW/ePPr06UNxcTF5eXncdddde3svvfzyy7nmmmtYtWoVw4cPZ8iQIbzyyiv06NGDP/7xj3s7lSs3e/ZsbrnlFnbt2kXXrl0pKiri4IMPZtu2bUycOJHi4mLMjB/+8IeMGjWKOXPmcP3117Nnzx7y8vKYO3dug8+9EoGINAldu3alsLCQOXPmMHLkSGbMmMFFF12EmdG2bVuefPJJDjzwQD788EOOP/54RowYUW0nbvfddx/t2rVj8eLFLF68mPz8/L3rpk6dSpcuXdizZw9nnHEGixcvZtKkSdx1113MmzePvLy8SsdauHAhDz/8MK+++iruzuDBgznllFPo3LkzK1as4LHHHuOBBx7gwgsv5IknnuA//uM/Ku0/ZMgQ5s+fj5nx4IMPcvvtt/PTn/6Um2++mY4dO7JkyRIANm3axIYNG7jiiit46aWX6Nu3b6P1R6REICJ1VtMv9yiVFw+VJ4LyX+HuzvXXX89LL71EixYteP/991m/fj3du3dPeZyXXnqJSZMmATBgwAAGDBiwd93MmTOZPn06ZWVlrFu3jmXLllVaX9XLL7/MV7/61b09oJ533nn89a9/ZcSIEfTt25dBgwYB1Xd1XVpaykUXXcS6devYtWsXffv2BeD555+vVBTWuXNnZs+ezcknn7x3m8bqqjoWdQSNPXaqiGTHV77yFebOncvrr7/Ojh079v6SLyoqYsOGDSxcuJBFixZx8MEHp+x6Olmqu4V3332XO++8k7lz57J48WLOPvvsWo9TU39t5V1YQ/VdXU+cOJEJEyawZMkS7r///r2fl6pb6qi6qm72iaB87NTVq8G9YuxUJQORpqd9+/aceuqpjBs3rlIl8ebNmznooINo1aoV8+bNY/Xq1TUe5+STT947QP3SpUtZvHgxELqwPuCAA+jYsSPr16/nmWee2btPhw4d2Lp1a8pjPfXUU2zfvp2PP/6YJ598kpNOOintf9PmzZvp0aMHAI888sje5UOHDuUXv/jF3vlNmzZxwgkn8OKLL/Luu+8CjddVdbNPBJkcO1VEojdmzBhKSkr2jhAGMHbsWIqLiykoKKCoqIjPfe5zNR7jqquuYtu2bQwYMIDbb7+dwsJCIIw2duyxx9KvXz/GjRtXqQvr8ePHM3z4cE477bRKx8rPz+fSSy+lsLCQwYMHc/nll3Psscem/e+58cYbueCCCzjppJMq1T/ccMMNbNq0if79+zNw4EDmzZtHt27dmD59Oueddx4DBw7koosuSvtzahJpN9RmNgz4GdASeNDdb6uy/lLgDuD9xKJfuPuDNR2zrt1Qt2gR7gT2jQ0+/TTtw4jEnrqhbjpyphtqM2sJTAPOBEqBBWY2y92rDuXzO3efEFUcvXqF4qBUy0VEJNqioUJgpbu/4+67gBnAyFr2aXSZHjtVRKSpiTIR9ADeS5ovTSyrapSZLTazx83ssMYOIptjp4o0N01tRMM4qs/fKMpEkKqNU9UIZwN93H0A8DzwyL67gOKwS+oAAAdUSURBVJmNN7NiMyvesGFDnQMZOxZWrQp1AqtWKQmI1Efbtm3ZuHGjkkEOc3c2btxI27Zt67RflA+UlQLJv/B7AmuTN3D3jUmzDwA/SXUgd58OTIdQWdy4YYpIOnr27ElpaSn1+TEmmdO2bVt69uxZp32iTAQLgCPNrC+hVdBo4GvJG5jZIe6+LjE7AlgeYTwi0gCtWrXa+0SrNC+RJQJ3LzOzCcCzhOajD7n7m2Z2E1Ds7rOASWY2AigD/g1cGlU8IiKSWqTPEUShrs8RiIhIzc8RNPsni0VEpGZN7o7AzDYANXckkj15wIfZDqIGiq9hcj0+yP0YFV/DNCS+3u7eLdWKJpcIcpmZFVd365ULFF/D5Hp8kPsxKr6GiSo+FQ2JiMScEoGISMwpETSu6dkOoBaKr2FyPT7I/RgVX8NEEp/qCEREYk53BCIiMadEICISc0oEdWRmh5nZPDNbbmZvmtnVKbY51cw2m9mixPSDDMe4ysyWJD57n8ewLbjXzFYmugDPz2BsRyWdl0VmtsXMrqmyTcbPn5k9ZGYfmNnSpGVdzOw5M1uReO1czb6XJLZZYWaXZCi2O8zsH4m/35Nm1qmafWv8LkQc441m9n7S3/GsavYdZmZvJb6PkzMY3++SYltlZouq2TfSc1jdNSWj3z9311SHCTgEyE+87wC8DRxTZZtTgf/LYoyrgLwa1p8FPEPoKvx44NUsxdkS+BfhQZesnj/gZCAfWJq07HZgcuL9ZOAnKfbrAryTeO2ceN85A7ENBfZLvP9JqtjS+S5EHOONwLfT+A78EzgcaA2UVP3/FFV8Vdb/FPhBNs5hddeUTH7/dEdQR+6+zt1fT7zfSugxNdWAO7lsJPBrD+YDnczskCzEcQbwT3fP+pPi7v4SoePDZCOpGCPjEeArKXb9MvCcu//b3TcBzwHDoo7N3f/s7mWJ2fmEbt6zpprzl46MjGRYU3xmZsCFwGON/bnpqOGakrHvnxJBA5hZH+BY4NUUq08wsxIze8bM+mU0sDAA0J/NbKGZjU+xPt3R46I2mur/82Xz/JU72BPdpCdeD0qxTS6cy3GEO7xUavsuRG1CovjqoWqKNnLh/J0ErHf3FdWsz9g5rHJNydj3T4mgnsysPfAEcI27b6my+nVCccdA4OfAUxkO70R3zweGA98ys5OrrE9n9LhImVlrwhgUv0+xOtvnry6yei7NbAqhG/eiajap7bsQpfuAI4BBwDpC8UtVWf8uAmOo+W4gI+ewlmtKtbulWFbn86dEUA9m1orwByty9z9UXe/uW9x9W+L900ArM8vLVHzuvjbx+gHwJOH2O1mto8dlwHDgdXdfX3VFts9fkvXlRWaJ1w9SbJO1c5moGDwHGOuJAuOq0vguRMbd17v7Hnf/lDACYarPzup30cz2A84DflfdNpk4h9VcUzL2/VMiqKNEeeKvgOXuflc123RPbIeZFRLO88ZU20YQ3wFm1qH8PaFScWmVzWYBFydaDx0PbPaKkeIypdpfYdk8f1XMAspbYVwC/DHFNs8CQ82sc6LoY2hiWaTMbBjwPWCEu2+vZpt0vgtRxphc7/TVaj5770iGibvE0YTznilfAv7h7qWpVmbiHNZwTcnc9y+qmvDmOgFDCLdei4FFieks4ErgysQ2E4A3CS0g5gNfzGB8hyc+tyQRw5TE8uT4DJhGaK2xBCjI8DlsR7iwd0xaltXzR0hK64DdhF9Z3wC6AnOBFYnXLoltC4AHk/YdB6xMTJdlKLaVhLLh8u/g/yS2PRR4uqbvQgbP328S36/FhIvaIVVjTMyfRWgp88+oYkwVX2L5/5Z/75K2zeg5rOGakrHvn7qYEBGJORUNiYjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgUiCme2xyj2jNlpPmGbWJ7nnS5Fcsl+2AxDJITvcfVC2gxDJNN0RiNQi0R/9T8zstcT0mcTy3mY2N9Gp2lwz65VYfrCFMQJKEtMXE4dqaWYPJPqc/7OZ7Z/YfpKZLUscZ0aW/pkSY0oEIhX2r1I0dFHSui3uXgj8ArgnsewXhO68BxA6fbs3sfxe4EUPneblE55IBTgSmObu/YCPgFGJ5ZOBYxPHuTKqf5xIdfRksUiCmW1z9/Yplq8CTnf3dxKdg/3L3bua2YeEbhN2J5avc/c8M9sA9HT3nUnH6EPoN/7IxPz3gFbufouZzQG2EXpZfcoTHe6JZIruCETS49W8r26bVHYmvd9DRR3d2YS+n44DFiZ6xBTJGCUCkfRclPT698T7Vwi9ZQKMBV5OvJ8LXAVgZi3N7MDqDmpmLYDD3H0e8F2gE7DPXYlIlPTLQ6TC/lZ5APM57l7ehLSNmb1K+PE0JrFsEvCQmX0H2ABcllh+NTDdzL5B+OV/FaHny1RaAo+aWUdCr7B3u/tHjfYvEkmD6ghEapGoIyhw9w+zHYtIFFQ0JCISc7ojEBGJOd0RiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxNz/A9LsyJeSley9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf() # 그래프를 초기화\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label = 'Training acc')\n",
    "plt.plot(epochs,val_acc,'b',label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 모델은 아홉 번째 에포크 이후에 과대적합이 시작된다. 아홉 번 에포크로 새로운 모델을 훈련하고 테스트 세트에서 평가한다.\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu',input_shape = (10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 2.5762 - accuracy: 0.5440 - val_loss: 1.6839 - val_accuracy: 0.6590\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 1.3662 - accuracy: 0.7159 - val_loss: 1.2635 - val_accuracy: 0.7390\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 1s 65us/step - loss: 1.0118 - accuracy: 0.7840 - val_loss: 1.1317 - val_accuracy: 0.7490\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.7920 - accuracy: 0.8317 - val_loss: 1.0299 - val_accuracy: 0.7850\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.6285 - accuracy: 0.8675 - val_loss: 0.9576 - val_accuracy: 0.8100\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.5051 - accuracy: 0.8949 - val_loss: 0.9326 - val_accuracy: 0.8170\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.4043 - accuracy: 0.9171 - val_loss: 0.9393 - val_accuracy: 0.8010\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s 66us/step - loss: 0.3333 - accuracy: 0.9296 - val_loss: 0.9220 - val_accuracy: 0.8140\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 67us/step - loss: 0.2761 - accuracy: 0.9380 - val_loss: 0.9436 - val_accuracy: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15888085a20>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(partial_x_train,\n",
    "         partial_y_train,\n",
    "         epochs = 9,\n",
    "         batch_size = 512,\n",
    "         validation_data = (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 0s 107us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0020089924388556, 0.7862867116928101]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.evaluate(x_test,one_hot_test_labels)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5.5 새로운 데이터에 대해 예측하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 객체의 predict 메서드는 46개의 토픽에 대한 확률 분포를 반환한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.43038560e-05, 1.29849550e-05, 1.14431236e-06, ...,\n",
       "        2.77593244e-06, 2.26174461e-06, 1.20275272e-05],\n",
       "       [1.47458690e-04, 6.37981202e-03, 2.93934045e-05, ...,\n",
       "        3.83477280e-04, 7.22020177e-09, 5.23806136e-07],\n",
       "       [5.63030830e-04, 7.55641401e-01, 3.94312898e-03, ...,\n",
       "        6.60336297e-03, 9.24273118e-05, 8.95887497e-05],\n",
       "       ...,\n",
       "       [2.01863095e-05, 9.93212743e-05, 2.44912153e-05, ...,\n",
       "        1.24512235e-05, 7.13964982e-06, 3.59208098e-05],\n",
       "       [2.83270888e-03, 4.98439632e-02, 2.53909896e-03, ...,\n",
       "        2.14425358e-03, 6.80453319e-04, 1.92333129e-03],\n",
       "       [6.18123391e-04, 2.60398775e-01, 1.45302033e-02, ...,\n",
       "        7.46783242e-03, 1.91061219e-04, 1.28193918e-04]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,)\n",
      "1.0000001\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0].shape) # 각 항목은 길이가 46인 벡터\n",
    "print(np.sum(predictions[0])) # 이 벡터의 원소의 합은 1이다.\n",
    "print(np.argmax(predictions[0])) # 가장 큰 값이 예측 클래스가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5.6 레이블과 손실을 다루는 다른 방법**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**앞서 언급한 것처럼 레이블을 인코딩하는 다른 방법은 다음과 같이 정수 텐서로 변환하는 것이다.**\n",
    "\n",
    "> **이 방식을 사용하려면 categorical_crossentropy -> sparse_categorical_crossentropy로 손실 함수 하나만 바꾸면 된다.**\n",
    "\n",
    "**이 손실 함수는 인터페이스만 다를 뿐이고 수학적으로는 categorical_crossentropy와 동일하다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  3 ... 25  3 25]\n",
      "[ 3  4  3 ... 25  3 25]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5.7 충분히 큰 중간층을 두어야 하는 이유**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**마지막 출력이 46차원이기 때문에 충간층의 히든 유닛이 46개보다 많이 적어서는 안 된다. 46차원보다 훨씬 작은 중간층을 두면 정보의 병목이 어떻게 나타나는지 확인해 본다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(64,activation='relu',input_shape=(10000,)))\n",
    "model.add(layers.Dense(4,activation='relu'))\n",
    "model.add(layers.Dense(46,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 96us/step - loss: 2.6589 - accuracy: 0.3262 - val_loss: 1.9298 - val_accuracy: 0.5800\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 82us/step - loss: 1.6437 - accuracy: 0.6232 - val_loss: 1.5258 - val_accuracy: 0.6490\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 1.3412 - accuracy: 0.6768 - val_loss: 1.4059 - val_accuracy: 0.6630\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 81us/step - loss: 1.1829 - accuracy: 0.6978 - val_loss: 1.3380 - val_accuracy: 0.6750\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 81us/step - loss: 1.0629 - accuracy: 0.7296 - val_loss: 1.3157 - val_accuracy: 0.6910\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.9659 - accuracy: 0.7549 - val_loss: 1.2723 - val_accuracy: 0.7060\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.8890 - accuracy: 0.7706 - val_loss: 1.2816 - val_accuracy: 0.7070\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 81us/step - loss: 0.8245 - accuracy: 0.7808 - val_loss: 1.2877 - val_accuracy: 0.7090\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 79us/step - loss: 0.7705 - accuracy: 0.7914 - val_loss: 1.4042 - val_accuracy: 0.6990\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.7247 - accuracy: 0.7970 - val_loss: 1.3331 - val_accuracy: 0.7090\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 81us/step - loss: 0.6839 - accuracy: 0.8033 - val_loss: 1.3365 - val_accuracy: 0.7130\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.6471 - accuracy: 0.8094 - val_loss: 1.3570 - val_accuracy: 0.7190\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.6156 - accuracy: 0.8147 - val_loss: 1.4210 - val_accuracy: 0.7090\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 81us/step - loss: 0.5844 - accuracy: 0.8197 - val_loss: 1.4374 - val_accuracy: 0.7120\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 79us/step - loss: 0.5605 - accuracy: 0.8252 - val_loss: 1.5000 - val_accuracy: 0.7130\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.5362 - accuracy: 0.8358 - val_loss: 1.5132 - val_accuracy: 0.7180\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.5146 - accuracy: 0.8509 - val_loss: 1.5229 - val_accuracy: 0.7210\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 79us/step - loss: 0.4926 - accuracy: 0.8587 - val_loss: 1.6273 - val_accuracy: 0.7150\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.4768 - accuracy: 0.8601 - val_loss: 1.6880 - val_accuracy: 0.7130\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 80us/step - loss: 0.4607 - accuracy: 0.8648 - val_loss: 1.7037 - val_accuracy: 0.7130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x158b95eeeb8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(partial_x_train,\n",
    "         partial_y_train,\n",
    "         epochs = 20,\n",
    "         batch_size = 128,\n",
    "         validation_data = (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 0s 95us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0230983711201915, 0.6963490843772888]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.evaluate(x_test,one_hot_test_labels)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**검증 정확도의 최고 값은 약 72%로 7% 정도 감소되었다. 이러한 손실의 원인 대부분은 많은 정보(클래스 46개의 분할 초평면을 복원하기에 충분한 정보)를 중간층의 저차원 표현 공간으로 압축하려고 했기 때문이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 네트워크는 필요한 정보 대부분을 4차원 표현 안에 구겨 넣었지만 전부는 넣지 못했다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5.8 추가 실험**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **더 크거나 작은 층을 사용해 보자. 32개의 유닛, 128개의 유닛 등**\n",
    "\n",
    "> - **여기에서 2개의 은닉 층을 사용했다. 1개나 3개의 은닉 층을 사용해보자**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(64, activation= 'relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation= 'relu'))\n",
    "model.add(layers.Dense(128, activation= 'relu'))\n",
    "model.add(layers.Dense(128, activation= 'relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 1s 102us/step - loss: 1.7088 - accuracy: 0.6131 - val_loss: 1.2612 - val_accuracy: 0.7020\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.9871 - accuracy: 0.7658 - val_loss: 1.0717 - val_accuracy: 0.7630\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 83us/step - loss: 0.6703 - accuracy: 0.8380 - val_loss: 1.0098 - val_accuracy: 0.7870\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.4617 - accuracy: 0.8906 - val_loss: 1.0419 - val_accuracy: 0.7980\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.3220 - accuracy: 0.9251 - val_loss: 1.3102 - val_accuracy: 0.7270\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 83us/step - loss: 0.2441 - accuracy: 0.9412 - val_loss: 1.4067 - val_accuracy: 0.7610\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.2058 - accuracy: 0.9453 - val_loss: 1.4987 - val_accuracy: 0.7650\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 83us/step - loss: 0.1742 - accuracy: 0.9528 - val_loss: 1.1972 - val_accuracy: 0.7930\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 83us/step - loss: 0.1555 - accuracy: 0.9529 - val_loss: 1.3447 - val_accuracy: 0.7920\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.1424 - accuracy: 0.9555 - val_loss: 1.4301 - val_accuracy: 0.7910\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.1327 - accuracy: 0.9548 - val_loss: 1.5936 - val_accuracy: 0.7670\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 83us/step - loss: 0.1231 - accuracy: 0.9580 - val_loss: 1.4693 - val_accuracy: 0.7820\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 83us/step - loss: 0.1157 - accuracy: 0.9541 - val_loss: 1.5892 - val_accuracy: 0.7830\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.1080 - accuracy: 0.9570 - val_loss: 1.5809 - val_accuracy: 0.7920\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 83us/step - loss: 0.1023 - accuracy: 0.9557 - val_loss: 1.7667 - val_accuracy: 0.7930\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.0976 - accuracy: 0.9564 - val_loss: 1.7445 - val_accuracy: 0.7900\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.0952 - accuracy: 0.9585 - val_loss: 2.0063 - val_accuracy: 0.7800\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 83us/step - loss: 0.0901 - accuracy: 0.9583 - val_loss: 1.8657 - val_accuracy: 0.7810\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.0844 - accuracy: 0.9607 - val_loss: 2.3203 - val_accuracy: 0.7860\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 84us/step - loss: 0.0849 - accuracy: 0.9599 - val_loss: 2.1925 - val_accuracy: 0.7760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x158b9885eb8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(partial_x_train,\n",
    "         partial_y_train,\n",
    "         epochs = 20,\n",
    "         batch_size = 128,\n",
    "         validation_data = (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 0s 97us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4443609143407454, 0.7729296684265137]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.evaluate(x_test,one_hot_test_labels)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5.9 정리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **N개의 클래스로 데이터 포인트를 분류하려면 네트워크의 마지막 Dense층의 크기는 N이어야 한다.**\n",
    "\n",
    "> - **단일 레이블, 다중 분류 문제에서는 N개의 클래스에 대한 확률 분포를 출력하기 위해 softmax 활성화 함수를 사용해야 한다.**\n",
    "\n",
    "> - **이런 문제에는 항상 범주형 크로스엔트로피를 사용. 이 함수는 모델이 출력한 확률 분포와 타깃 분포 사이의 거리를 최소화한다.**\n",
    "\n",
    "> - **다중 분류에서 레이블을 다루는 두 가지 방법이 존재한다. (원-핫인코딩, 정수인코딩)**\n",
    "\n",
    "> - **많은 수의 범주를 분류할 때 중간층의 크기가 너무 작아 네트워크에 정보의 병목이 생기지 않도록 주의**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.6 주택 가격 예측: 회귀 문제**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**또 다른 종류의 머신 러닝 문제는 개별적인 레이블 대신 연속적인 값을 예측하는 회귀(regression)이다.**\n",
    "\n",
    "> **예를 들어 기상 데이터가 주어졌을 때 내일 기온을 예측하거나, 소프트웨어 명세가 주어졌을 때 소프트웨어 프로젝트가 완료될 시간을 예측**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.6.1 보스턴 주택 가격 데이터셋**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1970년 중반 보스턴 외곽 지역의 범죄율, 지방세율 등의 데이터가 주어졌을 때 주택 가격의 중간 값을 예측한다.**\n",
    "\n",
    "> **데이터 포인트가 506개로 비교적 개수가 적고 404개는 훈련 샘플로, 102개의 테스트 샘플로 나뉘어져 있다. 입력 데이터에 있는 각 특성(feature)은 스케일이 서로 다르다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_target) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 shape: (404, 13)\n",
      "테스트 데이터의 shape: (102, 13)\n",
      "훈련 데이터의 상위 10개 label: [15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4]\n"
     ]
    }
   ],
   "source": [
    "# 여기서 볼 수 있듯이 404개의 훈련 샘플과 102개의 테스트 샘플이 있고 모두 13개의 수치 특성이 있다.\n",
    "# 이 특성들은 1인당 범죄율, 주택당 평균 방의 개수, 고속도로 접근성 등이다.\n",
    "print('훈련 데이터의 shape:',train_data.shape) # 훈련 데이터 shape\n",
    "print('테스트 데이터의 shape:',test_data.shape) # 테스트 데이터 shape\n",
    "# 이 가격은 일반적으로 1만 달러에서 5만 달러 사이다.\n",
    "print('훈련 데이터의 상위 10개 label:',train_targets[:10]) # label (상위 10개만 추출)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.6.2 데이터 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상이한 스케일을 가진 값을 신경망에 주입하면 문제가 된다. 네트워크가 이런 다양한 데이터에 자동으로 맞추려고 할 수 있지만 이는 확실히 학습을 더 어렵게 많든다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이런 데이터를 다룰 때 대표적인 방법은 특성별로 정규화를 하는 것이다. 입력 데이터에 있는 각 특성에 대해서 특성의 평균을 빼고 표준 편차로 나눈다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0) # 열을 기준으로 평균 \n",
    "train_data -= mean # 모든 훈련 데이터에 평균을 뺀다.\n",
    "std = train_data.std(axis=0) # 열을 기준으로 표준편차\n",
    "train_data /= std # 평균을 뺀 값에서 표준편차로 나눠 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 정규화할 때 사용한 값이 훈련 데이터에서 계산한 값임을 주목해야한다. 머신러닝 작업 과정에서 절대로 테스트 데이터에서 계산한 어떤 값도 사용해서는 안 된다.\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.6.3 모델 구성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**샘플 개수가 적기 때문에 64개의 유닛을 가진 2개의 은닉 층으로 작은 네트워크를 구성하여 사용한다.**\n",
    "\n",
    "> **일반적으로 훈련 데이터의 개수가 적을수록 과대적합이 더 쉽게 일어나므로 작은 모델을 사용하는 것이 과대적합을 피하는 한 방법이다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 네트워크의 마지막 층은 하나의 유닛을 가지고 있고 활성화 함수가 없다. 이것이 전형적인 스칼라 회귀(하나의 연속적인 값을 예측하는 회귀)를 위한 구성이다.**\n",
    "\n",
    "> - **마지막 층이 순수한 선형이므로 네트워크가 어떤 범위의 값이라도 예측하도록 자유롭게 학습된다.**\n",
    "\n",
    "> - **mse는 평균 제곱 오차의 약어로 예측과 타깃 사이 거이의 제곱이다. 회귀 문제에서 널리 사용되는 손실함수 이다.**\n",
    "\n",
    "> - **훈련하는 동안 모니터링을 위해 새로운 지표인 평균 절대 오차(MAE)를 측정한다. 이는 예측과 타깃 사이 거리의 절댓값이다. 예를들어 이 예제에서 MAE가 0.5이면 예측이 평균적으로 500달러 정도 차이가 난다는 뜻이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.6.4 K-겹 검증을 사용한 훈련 검증**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**매개변수들을 조정하면서 모델을 평가하기 위해 이전 예제에서 했던 것처럼 데이터를 훈련 세트와 검증 세트로 나눈다.**\n",
    "\n",
    "> - **이 경우 데이터 포인트가 많지 않기 때문에 검증 세트도 매우 작다.**\n",
    "\n",
    "> - **결국 검증 세트와 훈련 세트로 어떤 데이터 포인트가 선택되었는지에 따라 검증 점수가 크게 달라진다. 즉, 검증 세트의 분할에 대한 검증 점수의 분산이 높다. 이렇게 되면 신뢰 있는 모델을 평가하기 어렵다.**\n",
    "\n",
    "**이런 상황에서 가장 좋은 방법은 K-겹 교차 검증을 사용하는 것이다. 데이터를 K개의 분할(즉, Fold)로 나누고, K개의 모델을 각각 만들어 K-1개의 분할에서 훈련하고 나머지 분할에서 평가하는 방법이다.**\n",
    "\n",
    "> - **모델의 검증 점수는 K개의 검증 점수 평균이 된다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4 # 4-fold\n",
    "num_val_samples = len(train_data) // k # data셋을 4개로 나눔\n",
    "num_epochs = 100 # 반복 수\n",
    "all_scores = [] # MAE를 담을 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리중인 폴드 # 0\n",
      "처리중인 폴드 # 1\n",
      "처리중인 폴드 # 2\n",
      "처리중인 폴드 # 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(k): # K-Fold 반복수 \n",
    "    print('처리중인 폴드 #',i) \n",
    "    # validation_set(train) 0: num_val_sample -> num_val_sample: 2*num_val_sample ....\n",
    "    val_data = train_data[i*num_val_samples: (i+1) * num_val_samples] \n",
    "    # validation_set(target) 0: num_val_sample -> num_val_sample: 2*num_val_sample\n",
    "    val_targets = train_targets[i*num_val_samples: (i+1) * num_val_samples]\n",
    "    \n",
    "    # 1num_val_sample: -> :num_val_sample, 2num_val_sample: -> ... \n",
    "    partial_train_data = np.concatenate(\n",
    "    [train_data[:i * num_val_samples],\n",
    "    train_data[(i+1) * num_val_samples:]], axis = 0\n",
    "    )\n",
    "    # 1num_val_sample: -> :num_val_sample, 2num_val_sample: -> ...\n",
    "    partial_train_target = np.concatenate(\n",
    "    [train_targets[:i * num_val_samples],\n",
    "    train_targets[(i+1) * num_val_samples:]], axis = 0\n",
    "    )\n",
    "    \n",
    "    model = build_model() # Include keras compile\n",
    "    model.fit(partial_train_data,partial_train_target,\n",
    "             epochs = num_epochs, batch_size = 1, verbose = 0 ) \n",
    "    # verbos=0 train process print(x)\n",
    "    \n",
    "    val_mse, val_mae = model.evaluate(val_data,val_targets,verbose=0) # mse, mae\n",
    "    all_scores.append(val_mae) # val_mae(검증셋 점수 리스트 추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_score: [1.9140071868896484, 2.786120653152466, 2.4797942638397217, 2.4682586193084717]\n",
      "K-폴드 교차검증의 평균: 2.412045180797577\n"
     ]
    }
   ],
   "source": [
    "print('all_score:',all_scores)\n",
    "print('K-폴드 교차검증의 평균:',np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**검증 세트가 다르므로 확실히 검증 검수가 2.1에서 2.9까지 변화가 크다. 평균값(2.4)이 각각의 점수보다 훨씬 신뢰할 만하다. 이것이 K-겹 교차 검증의 핵심이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위 예제에서는 평균적으로 2400달러 정도 차이가 난다. 주택 가격 범위가 1만 달러에서 5만달러 사이인 것을 감안하면 비교적 큰 값이다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리중인 폴드: 0\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 303 samples, validate on 101 samples\n",
      "Epoch 1/500\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 225.0525 - mae: 11.7886 - val_loss: 35.7250 - val_mae: 3.8943\n",
      "Epoch 2/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 27.4738 - mae: 3.6226 - val_loss: 21.8675 - val_mae: 2.9225\n",
      "Epoch 3/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 19.7625 - mae: 3.0075 - val_loss: 17.8286 - val_mae: 2.9215\n",
      "Epoch 4/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 16.6199 - mae: 2.7729 - val_loss: 14.9752 - val_mae: 2.4392\n",
      "Epoch 5/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 15.5848 - mae: 2.6877 - val_loss: 13.6868 - val_mae: 2.6250\n",
      "Epoch 6/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 13.9183 - mae: 2.4956 - val_loss: 13.3808 - val_mae: 2.2586\n",
      "Epoch 7/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 13.2175 - mae: 2.4347 - val_loss: 13.0985 - val_mae: 2.4989\n",
      "Epoch 8/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 13.1137 - mae: 2.4020 - val_loss: 11.8043 - val_mae: 2.1747\n",
      "Epoch 9/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 12.9024 - mae: 2.3536 - val_loss: 11.0123 - val_mae: 2.1073\n",
      "Epoch 10/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 12.4823 - mae: 2.2887 - val_loss: 11.3172 - val_mae: 2.3089\n",
      "Epoch 11/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 12.0523 - mae: 2.2568 - val_loss: 9.8430 - val_mae: 2.0362\n",
      "Epoch 12/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 11.9525 - mae: 2.2260 - val_loss: 9.6680 - val_mae: 2.0125\n",
      "Epoch 13/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 11.3899 - mae: 2.1914 - val_loss: 9.8326 - val_mae: 2.0009\n",
      "Epoch 14/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.9233 - mae: 2.1519 - val_loss: 10.0170 - val_mae: 2.1123\n",
      "Epoch 15/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.0278 - mae: 2.1115 - val_loss: 14.6975 - val_mae: 2.4512\n",
      "Epoch 16/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 11.2440 - mae: 2.1314 - val_loss: 9.0275 - val_mae: 2.0941\n",
      "Epoch 17/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.2273 - mae: 2.0778 - val_loss: 10.7638 - val_mae: 2.3763\n",
      "Epoch 18/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.0291 - mae: 2.0933 - val_loss: 8.5958 - val_mae: 1.9394\n",
      "Epoch 19/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.7162 - mae: 2.0964 - val_loss: 9.0249 - val_mae: 1.9273\n",
      "Epoch 20/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.3383 - mae: 2.0365 - val_loss: 9.2284 - val_mae: 1.9482\n",
      "Epoch 21/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.5868 - mae: 2.0178 - val_loss: 9.6705 - val_mae: 2.0203\n",
      "Epoch 22/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.8970 - mae: 2.0155 - val_loss: 8.4762 - val_mae: 2.0773\n",
      "Epoch 23/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.5367 - mae: 2.0097 - val_loss: 8.3547 - val_mae: 2.0443\n",
      "Epoch 24/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.6747 - mae: 1.9837 - val_loss: 9.4543 - val_mae: 1.9711\n",
      "Epoch 25/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.0301 - mae: 1.9386 - val_loss: 8.0886 - val_mae: 1.8169\n",
      "Epoch 26/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.8552 - mae: 1.9426 - val_loss: 8.3459 - val_mae: 2.1775\n",
      "Epoch 27/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.1560 - mae: 1.9031 - val_loss: 8.5599 - val_mae: 1.9450\n",
      "Epoch 28/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.4425 - mae: 1.8772 - val_loss: 8.5777 - val_mae: 2.0141\n",
      "Epoch 29/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.6899 - mae: 1.9335 - val_loss: 9.4420 - val_mae: 2.2446\n",
      "Epoch 30/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.1464 - mae: 1.9448 - val_loss: 9.0216 - val_mae: 2.0834\n",
      "Epoch 31/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.5021 - mae: 1.8936 - val_loss: 8.9467 - val_mae: 1.8186\n",
      "Epoch 32/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.9882 - mae: 1.8473 - val_loss: 9.5253 - val_mae: 2.1085\n",
      "Epoch 33/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.9626 - mae: 1.8064 - val_loss: 8.5280 - val_mae: 2.0704\n",
      "Epoch 34/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.1837 - mae: 1.8417 - val_loss: 8.4610 - val_mae: 1.8952\n",
      "Epoch 35/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.9299 - mae: 1.8541 - val_loss: 9.1524 - val_mae: 2.1407\n",
      "Epoch 36/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.8062 - mae: 1.8430 - val_loss: 8.0129 - val_mae: 2.0608\n",
      "Epoch 37/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.2808 - mae: 1.8567 - val_loss: 9.7133 - val_mae: 2.3821\n",
      "Epoch 38/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.5493 - mae: 1.7667 - val_loss: 8.1364 - val_mae: 2.1216\n",
      "Epoch 39/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.3778 - mae: 1.7642 - val_loss: 7.4556 - val_mae: 1.7368\n",
      "Epoch 40/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.2862 - mae: 1.7433 - val_loss: 8.4581 - val_mae: 1.8752\n",
      "Epoch 41/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.5326 - mae: 1.6781 - val_loss: 7.1474 - val_mae: 1.8429\n",
      "Epoch 42/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.5758 - mae: 1.7761 - val_loss: 7.7710 - val_mae: 1.8825\n",
      "Epoch 43/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.9435 - mae: 1.7029 - val_loss: 9.3096 - val_mae: 2.3680\n",
      "Epoch 44/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.5288 - mae: 1.6982 - val_loss: 7.7936 - val_mae: 2.0005\n",
      "Epoch 45/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.0206 - mae: 1.7607 - val_loss: 7.2782 - val_mae: 1.7246\n",
      "Epoch 46/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.0870 - mae: 1.5894 - val_loss: 7.2718 - val_mae: 1.7784\n",
      "Epoch 47/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.3301 - mae: 1.6544 - val_loss: 10.5291 - val_mae: 2.5997\n",
      "Epoch 48/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.5379 - mae: 1.6933 - val_loss: 8.1556 - val_mae: 2.0455\n",
      "Epoch 49/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.4757 - mae: 1.6131 - val_loss: 8.4875 - val_mae: 1.9574\n",
      "Epoch 50/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.4792 - mae: 1.6654 - val_loss: 9.2638 - val_mae: 2.0125\n",
      "Epoch 51/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.8860 - mae: 1.6397 - val_loss: 7.7943 - val_mae: 1.9242\n",
      "Epoch 52/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.2329 - mae: 1.6364 - val_loss: 7.1004 - val_mae: 1.8337\n",
      "Epoch 53/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.0267 - mae: 1.5993 - val_loss: 7.2152 - val_mae: 1.7905\n",
      "Epoch 54/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.2549 - mae: 1.6648 - val_loss: 7.8841 - val_mae: 1.9827\n",
      "Epoch 55/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.1304 - mae: 1.6445 - val_loss: 8.9123 - val_mae: 2.3567\n",
      "Epoch 56/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.7765 - mae: 1.6138 - val_loss: 8.1768 - val_mae: 2.1139\n",
      "Epoch 57/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.9022 - mae: 1.5869 - val_loss: 10.6222 - val_mae: 2.5351\n",
      "Epoch 58/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.7802 - mae: 1.5803 - val_loss: 7.8269 - val_mae: 1.9177\n",
      "Epoch 59/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.8766 - mae: 1.5905 - val_loss: 8.7082 - val_mae: 2.2635\n",
      "Epoch 60/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.4832 - mae: 1.5440 - val_loss: 7.9715 - val_mae: 2.0352\n",
      "Epoch 61/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.4825 - mae: 1.6039 - val_loss: 8.1908 - val_mae: 1.9812\n",
      "Epoch 62/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.9255 - mae: 1.5497 - val_loss: 7.4282 - val_mae: 1.8600\n",
      "Epoch 63/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.6553 - mae: 1.5831 - val_loss: 7.9288 - val_mae: 1.9853\n",
      "Epoch 64/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.5469 - mae: 1.5649 - val_loss: 7.8520 - val_mae: 2.0440\n",
      "Epoch 65/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.3568 - mae: 1.5529 - val_loss: 7.7273 - val_mae: 2.0559\n",
      "Epoch 66/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.4551 - mae: 1.5304 - val_loss: 8.2001 - val_mae: 2.1770\n",
      "Epoch 67/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.6414 - mae: 1.5572 - val_loss: 7.3805 - val_mae: 1.8300\n",
      "Epoch 68/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0550 - mae: 1.4659 - val_loss: 8.1245 - val_mae: 2.1222\n",
      "Epoch 69/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0942 - mae: 1.4839 - val_loss: 8.2933 - val_mae: 2.0701\n",
      "Epoch 70/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9647 - mae: 1.4851 - val_loss: 8.2793 - val_mae: 2.0411\n",
      "Epoch 71/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.1009 - mae: 1.4960 - val_loss: 10.7590 - val_mae: 2.5578\n",
      "Epoch 72/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.2595 - mae: 1.4784 - val_loss: 9.1423 - val_mae: 2.3384\n",
      "Epoch 73/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0029 - mae: 1.3891 - val_loss: 9.2817 - val_mae: 2.1807\n",
      "Epoch 74/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0110 - mae: 1.4606 - val_loss: 8.4975 - val_mae: 2.2276\n",
      "Epoch 75/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9110 - mae: 1.5087 - val_loss: 9.1694 - val_mae: 2.3303\n",
      "Epoch 76/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.2464 - mae: 1.4892 - val_loss: 7.5992 - val_mae: 2.0281\n",
      "Epoch 77/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0014 - mae: 1.4361 - val_loss: 8.0645 - val_mae: 2.0965\n",
      "Epoch 78/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.7026 - mae: 1.4079 - val_loss: 7.6588 - val_mae: 1.8838\n",
      "Epoch 79/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.7816 - mae: 1.4039 - val_loss: 8.2214 - val_mae: 2.0426\n",
      "Epoch 80/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8352 - mae: 1.4990 - val_loss: 8.5682 - val_mae: 2.3201\n",
      "Epoch 81/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4722 - mae: 1.4484 - val_loss: 9.1173 - val_mae: 2.1882\n",
      "Epoch 82/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.5636 - mae: 1.4415 - val_loss: 8.8265 - val_mae: 2.3051\n",
      "Epoch 83/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4494 - mae: 1.4147 - val_loss: 8.3904 - val_mae: 2.0539\n",
      "Epoch 84/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4467 - mae: 1.3757 - val_loss: 7.7908 - val_mae: 1.9714\n",
      "Epoch 85/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4326 - mae: 1.3809 - val_loss: 9.2339 - val_mae: 2.2524\n",
      "Epoch 86/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4656 - mae: 1.3842 - val_loss: 8.1157 - val_mae: 1.9988\n",
      "Epoch 87/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2803 - mae: 1.3577 - val_loss: 9.1336 - val_mae: 2.1727\n",
      "Epoch 88/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.5647 - mae: 1.4421 - val_loss: 7.8244 - val_mae: 2.0370\n",
      "Epoch 89/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2270 - mae: 1.3837 - val_loss: 9.0487 - val_mae: 2.1593\n",
      "Epoch 90/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2720 - mae: 1.4135 - val_loss: 9.8920 - val_mae: 2.3288\n",
      "Epoch 91/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0242 - mae: 1.3502 - val_loss: 8.5001 - val_mae: 2.1069\n",
      "Epoch 92/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4085 - mae: 1.3625 - val_loss: 8.7708 - val_mae: 2.2201\n",
      "Epoch 93/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3190 - mae: 1.3952 - val_loss: 8.0112 - val_mae: 1.9909\n",
      "Epoch 94/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2194 - mae: 1.3361 - val_loss: 8.0880 - val_mae: 2.0999\n",
      "Epoch 95/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8259 - mae: 1.2782 - val_loss: 9.1424 - val_mae: 2.1675\n",
      "Epoch 96/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2168 - mae: 1.3165 - val_loss: 9.2715 - val_mae: 2.1500\n",
      "Epoch 97/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8813 - mae: 1.3052 - val_loss: 7.9661 - val_mae: 2.1831\n",
      "Epoch 98/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.9198 - mae: 1.3156 - val_loss: 8.5644 - val_mae: 2.2611\n",
      "Epoch 99/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8094 - mae: 1.3264 - val_loss: 8.3938 - val_mae: 2.0101\n",
      "Epoch 100/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7176 - mae: 1.3088 - val_loss: 8.0391 - val_mae: 2.1047\n",
      "Epoch 101/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7020 - mae: 1.3047 - val_loss: 9.0512 - val_mae: 2.2705\n",
      "Epoch 102/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8688 - mae: 1.3342 - val_loss: 8.4271 - val_mae: 2.0813\n",
      "Epoch 103/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8096 - mae: 1.2687 - val_loss: 9.2687 - val_mae: 2.2083\n",
      "Epoch 104/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6292 - mae: 1.3492 - val_loss: 8.2391 - val_mae: 2.0711\n",
      "Epoch 105/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7606 - mae: 1.3381 - val_loss: 9.5175 - val_mae: 2.1537\n",
      "Epoch 106/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4779 - mae: 1.3042 - val_loss: 8.7508 - val_mae: 2.2854\n",
      "Epoch 107/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7422 - mae: 1.3018 - val_loss: 8.0586 - val_mae: 2.0781\n",
      "Epoch 108/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7173 - mae: 1.3028 - val_loss: 7.7802 - val_mae: 2.0648\n",
      "Epoch 109/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5011 - mae: 1.2746 - val_loss: 8.7505 - val_mae: 2.1624\n",
      "Epoch 110/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3852 - mae: 1.2795 - val_loss: 9.2211 - val_mae: 2.3302\n",
      "Epoch 111/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.9947 - mae: 1.3627 - val_loss: 9.1002 - val_mae: 2.2384\n",
      "Epoch 112/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7360 - mae: 1.2584 - val_loss: 9.5452 - val_mae: 2.3463\n",
      "Epoch 113/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5397 - mae: 1.3094 - val_loss: 11.7108 - val_mae: 2.6978\n",
      "Epoch 114/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3709 - mae: 1.2906 - val_loss: 8.2336 - val_mae: 2.0984\n",
      "Epoch 115/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3469 - mae: 1.3096 - val_loss: 9.1788 - val_mae: 2.1103\n",
      "Epoch 116/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3266 - mae: 1.2633 - val_loss: 9.8893 - val_mae: 2.4093\n",
      "Epoch 117/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3657 - mae: 1.2449 - val_loss: 9.5602 - val_mae: 2.2436\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5039 - mae: 1.3085 - val_loss: 9.5537 - val_mae: 2.1973\n",
      "Epoch 119/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4367 - mae: 1.2332 - val_loss: 10.2108 - val_mae: 2.2553\n",
      "Epoch 120/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5907 - mae: 1.2740 - val_loss: 9.0203 - val_mae: 2.2081\n",
      "Epoch 121/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3376 - mae: 1.2359 - val_loss: 8.9944 - val_mae: 2.1299\n",
      "Epoch 122/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3993 - mae: 1.2412 - val_loss: 8.8602 - val_mae: 2.0948\n",
      "Epoch 123/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1073 - mae: 1.2156 - val_loss: 9.1914 - val_mae: 2.1229\n",
      "Epoch 124/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0771 - mae: 1.1634 - val_loss: 8.7153 - val_mae: 2.1743\n",
      "Epoch 125/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2444 - mae: 1.2579 - val_loss: 9.1872 - val_mae: 2.1165\n",
      "Epoch 126/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1777 - mae: 1.2390 - val_loss: 9.5755 - val_mae: 2.3215\n",
      "Epoch 127/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9810 - mae: 1.1886 - val_loss: 10.0451 - val_mae: 2.3360\n",
      "Epoch 128/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2826 - mae: 1.2305 - val_loss: 9.0996 - val_mae: 2.1299\n",
      "Epoch 129/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0404 - mae: 1.2393 - val_loss: 10.1798 - val_mae: 2.2517\n",
      "Epoch 130/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0986 - mae: 1.2079 - val_loss: 10.4843 - val_mae: 2.2947\n",
      "Epoch 131/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9327 - mae: 1.2269 - val_loss: 8.1791 - val_mae: 2.1253\n",
      "Epoch 132/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2853 - mae: 1.2053 - val_loss: 8.8996 - val_mae: 2.2152\n",
      "Epoch 133/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1402 - mae: 1.2116 - val_loss: 8.8911 - val_mae: 2.1360\n",
      "Epoch 134/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8263 - mae: 1.2133 - val_loss: 9.6755 - val_mae: 2.2650\n",
      "Epoch 135/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9173 - mae: 1.1905 - val_loss: 11.7493 - val_mae: 2.3283\n",
      "Epoch 136/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9423 - mae: 1.2312 - val_loss: 11.0301 - val_mae: 2.4747\n",
      "Epoch 137/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9193 - mae: 1.1697 - val_loss: 9.2023 - val_mae: 2.2083\n",
      "Epoch 138/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7559 - mae: 1.1331 - val_loss: 9.9407 - val_mae: 2.1695\n",
      "Epoch 139/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8988 - mae: 1.1899 - val_loss: 11.7500 - val_mae: 2.3675\n",
      "Epoch 140/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0752 - mae: 1.2281 - val_loss: 9.6624 - val_mae: 2.2046\n",
      "Epoch 141/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8312 - mae: 1.2132 - val_loss: 9.4725 - val_mae: 2.1374\n",
      "Epoch 142/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9075 - mae: 1.1873 - val_loss: 9.8808 - val_mae: 2.2553\n",
      "Epoch 143/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9639 - mae: 1.1278 - val_loss: 10.4345 - val_mae: 2.4015\n",
      "Epoch 144/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6126 - mae: 1.1426 - val_loss: 9.5501 - val_mae: 2.1287\n",
      "Epoch 145/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9971 - mae: 1.1927 - val_loss: 8.8963 - val_mae: 2.2313\n",
      "Epoch 146/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6920 - mae: 1.1587 - val_loss: 9.3209 - val_mae: 2.1548\n",
      "Epoch 147/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7603 - mae: 1.1555 - val_loss: 10.0858 - val_mae: 2.3118\n",
      "Epoch 148/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9097 - mae: 1.1696 - val_loss: 9.6745 - val_mae: 2.0666\n",
      "Epoch 149/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7147 - mae: 1.1556 - val_loss: 10.6953 - val_mae: 2.4039\n",
      "Epoch 150/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6461 - mae: 1.1617 - val_loss: 10.3881 - val_mae: 2.2998\n",
      "Epoch 151/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6503 - mae: 1.1792 - val_loss: 11.1622 - val_mae: 2.2896\n",
      "Epoch 152/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9097 - mae: 1.2048 - val_loss: 9.4294 - val_mae: 2.2689\n",
      "Epoch 153/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5386 - mae: 1.1927 - val_loss: 12.3013 - val_mae: 2.5510\n",
      "Epoch 154/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6237 - mae: 1.1190 - val_loss: 11.8959 - val_mae: 2.6624\n",
      "Epoch 155/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7314 - mae: 1.1534 - val_loss: 10.2461 - val_mae: 2.2871\n",
      "Epoch 156/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5054 - mae: 1.0880 - val_loss: 10.4053 - val_mae: 2.3914\n",
      "Epoch 157/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3608 - mae: 1.0828 - val_loss: 9.8001 - val_mae: 2.1408\n",
      "Epoch 158/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8182 - mae: 1.1417 - val_loss: 12.0365 - val_mae: 2.3844\n",
      "Epoch 159/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5650 - mae: 1.1494 - val_loss: 11.3001 - val_mae: 2.4480\n",
      "Epoch 160/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7080 - mae: 1.1907 - val_loss: 10.8554 - val_mae: 2.2593\n",
      "Epoch 161/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3798 - mae: 1.1119 - val_loss: 10.2746 - val_mae: 2.2226\n",
      "Epoch 162/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2854 - mae: 1.1266 - val_loss: 9.5285 - val_mae: 2.2281\n",
      "Epoch 163/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5327 - mae: 1.0914 - val_loss: 9.4199 - val_mae: 2.2124\n",
      "Epoch 164/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5200 - mae: 1.1168 - val_loss: 11.3972 - val_mae: 2.3062\n",
      "Epoch 165/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3532 - mae: 1.1074 - val_loss: 9.9302 - val_mae: 2.2458\n",
      "Epoch 166/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6082 - mae: 1.1392 - val_loss: 9.6439 - val_mae: 2.2122\n",
      "Epoch 167/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3579 - mae: 1.0690 - val_loss: 10.1502 - val_mae: 2.2267\n",
      "Epoch 168/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4257 - mae: 1.1070 - val_loss: 11.2061 - val_mae: 2.4022\n",
      "Epoch 169/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4013 - mae: 1.1362 - val_loss: 9.5155 - val_mae: 2.2130\n",
      "Epoch 170/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4601 - mae: 1.1192 - val_loss: 9.9988 - val_mae: 2.2025\n",
      "Epoch 171/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3249 - mae: 1.0745 - val_loss: 9.8357 - val_mae: 2.1659\n",
      "Epoch 172/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4541 - mae: 1.1228 - val_loss: 10.5317 - val_mae: 2.3979\n",
      "Epoch 173/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1784 - mae: 1.0910 - val_loss: 10.0468 - val_mae: 2.1781\n",
      "Epoch 174/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2098 - mae: 1.0908 - val_loss: 10.1805 - val_mae: 2.2949\n",
      "Epoch 175/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2220 - mae: 1.0640 - val_loss: 10.6843 - val_mae: 2.1963\n",
      "Epoch 176/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2105 - mae: 1.1084 - val_loss: 9.8429 - val_mae: 2.2146\n",
      "Epoch 177/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2178 - mae: 1.0557 - val_loss: 9.0526 - val_mae: 2.2109\n",
      "Epoch 178/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1894 - mae: 1.0836 - val_loss: 10.0488 - val_mae: 2.1792\n",
      "Epoch 179/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2249 - mae: 1.0734 - val_loss: 11.6899 - val_mae: 2.4490\n",
      "Epoch 180/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9259 - mae: 1.0333 - val_loss: 10.3404 - val_mae: 2.2556\n",
      "Epoch 181/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2846 - mae: 1.0748 - val_loss: 10.3946 - val_mae: 2.2872\n",
      "Epoch 182/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8582 - mae: 0.9623 - val_loss: 10.4886 - val_mae: 2.3389\n",
      "Epoch 183/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3339 - mae: 1.0906 - val_loss: 9.8893 - val_mae: 2.1977\n",
      "Epoch 184/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0204 - mae: 1.0311 - val_loss: 10.2366 - val_mae: 2.2964\n",
      "Epoch 185/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0366 - mae: 1.0173 - val_loss: 9.8019 - val_mae: 2.2603\n",
      "Epoch 186/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9793 - mae: 1.0174 - val_loss: 11.2936 - val_mae: 2.4560\n",
      "Epoch 187/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0208 - mae: 1.0279 - val_loss: 10.6708 - val_mae: 2.3051\n",
      "Epoch 188/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0308 - mae: 1.0700 - val_loss: 10.1873 - val_mae: 2.2405\n",
      "Epoch 189/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7808 - mae: 0.9827 - val_loss: 10.7823 - val_mae: 2.4257\n",
      "Epoch 190/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0769 - mae: 1.0145 - val_loss: 10.6266 - val_mae: 2.3501\n",
      "Epoch 191/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8263 - mae: 1.0029 - val_loss: 11.2550 - val_mae: 2.4126\n",
      "Epoch 192/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9326 - mae: 1.0258 - val_loss: 11.3788 - val_mae: 2.2872\n",
      "Epoch 193/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8668 - mae: 1.0167 - val_loss: 11.6817 - val_mae: 2.2183\n",
      "Epoch 194/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8964 - mae: 0.9852 - val_loss: 11.3451 - val_mae: 2.3698\n",
      "Epoch 195/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6765 - mae: 0.9457 - val_loss: 11.5859 - val_mae: 2.3928\n",
      "Epoch 196/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8936 - mae: 1.0297 - val_loss: 13.2387 - val_mae: 2.4490\n",
      "Epoch 197/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7896 - mae: 0.9840 - val_loss: 11.5340 - val_mae: 2.3289\n",
      "Epoch 198/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0133 - mae: 1.0517 - val_loss: 13.3282 - val_mae: 2.5486\n",
      "Epoch 199/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8504 - mae: 0.9709 - val_loss: 11.8155 - val_mae: 2.3224\n",
      "Epoch 200/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9425 - mae: 1.0091 - val_loss: 10.6537 - val_mae: 2.2763\n",
      "Epoch 201/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9425 - mae: 1.0070 - val_loss: 13.7016 - val_mae: 2.5344\n",
      "Epoch 202/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8088 - mae: 0.9957 - val_loss: 13.8021 - val_mae: 2.6775\n",
      "Epoch 203/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8487 - mae: 0.9652 - val_loss: 11.2154 - val_mae: 2.3727\n",
      "Epoch 204/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8682 - mae: 1.0262 - val_loss: 12.7557 - val_mae: 2.4062\n",
      "Epoch 205/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9038 - mae: 1.0106 - val_loss: 13.1008 - val_mae: 2.3874\n",
      "Epoch 206/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8434 - mae: 0.9914 - val_loss: 11.3477 - val_mae: 2.3286\n",
      "Epoch 207/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7965 - mae: 0.9751 - val_loss: 12.8836 - val_mae: 2.4024\n",
      "Epoch 208/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6629 - mae: 0.9173 - val_loss: 11.2261 - val_mae: 2.3124\n",
      "Epoch 209/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7146 - mae: 0.9913 - val_loss: 12.8833 - val_mae: 2.5508\n",
      "Epoch 210/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8882 - mae: 0.9820 - val_loss: 12.0900 - val_mae: 2.3306\n",
      "Epoch 211/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7021 - mae: 0.9680 - val_loss: 12.2814 - val_mae: 2.3601\n",
      "Epoch 212/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7225 - mae: 0.9404 - val_loss: 11.5277 - val_mae: 2.3671\n",
      "Epoch 213/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5960 - mae: 0.9224 - val_loss: 11.2338 - val_mae: 2.4210\n",
      "Epoch 214/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7395 - mae: 0.9326 - val_loss: 11.5598 - val_mae: 2.2888\n",
      "Epoch 215/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8905 - mae: 1.0406 - val_loss: 10.8278 - val_mae: 2.1884\n",
      "Epoch 216/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4612 - mae: 0.8655 - val_loss: 14.7966 - val_mae: 2.7505\n",
      "Epoch 217/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6035 - mae: 0.9350 - val_loss: 12.1947 - val_mae: 2.4930\n",
      "Epoch 218/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7986 - mae: 0.9769 - val_loss: 11.0373 - val_mae: 2.2390\n",
      "Epoch 219/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6975 - mae: 0.9460 - val_loss: 10.4210 - val_mae: 2.2066\n",
      "Epoch 220/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6236 - mae: 0.9645 - val_loss: 11.1344 - val_mae: 2.2467\n",
      "Epoch 221/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5215 - mae: 0.9087 - val_loss: 11.0881 - val_mae: 2.2340\n",
      "Epoch 222/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5837 - mae: 0.9205 - val_loss: 12.1601 - val_mae: 2.2966\n",
      "Epoch 223/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7521 - mae: 0.9619 - val_loss: 12.2213 - val_mae: 2.5270\n",
      "Epoch 224/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6815 - mae: 0.9524 - val_loss: 11.7870 - val_mae: 2.3545\n",
      "Epoch 225/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6092 - mae: 0.9204 - val_loss: 11.5740 - val_mae: 2.3492\n",
      "Epoch 226/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4734 - mae: 0.8922 - val_loss: 13.1878 - val_mae: 2.4900\n",
      "Epoch 227/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5854 - mae: 0.9307 - val_loss: 12.0914 - val_mae: 2.4555\n",
      "Epoch 228/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5949 - mae: 0.9415 - val_loss: 11.4018 - val_mae: 2.4312\n",
      "Epoch 229/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8979 - mae: 0.9833 - val_loss: 12.2347 - val_mae: 2.3920\n",
      "Epoch 230/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3771 - mae: 0.8579 - val_loss: 10.9712 - val_mae: 2.2732\n",
      "Epoch 231/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5416 - mae: 0.9264 - val_loss: 11.6392 - val_mae: 2.3542\n",
      "Epoch 232/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4894 - mae: 0.8848 - val_loss: 12.1786 - val_mae: 2.3998\n",
      "Epoch 233/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6320 - mae: 0.9493 - val_loss: 12.0113 - val_mae: 2.3789\n",
      "Epoch 234/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3662 - mae: 0.8595 - val_loss: 13.9302 - val_mae: 2.5509\n",
      "Epoch 235/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5551 - mae: 0.9184 - val_loss: 14.3716 - val_mae: 2.6951\n",
      "Epoch 236/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5369 - mae: 0.9241 - val_loss: 11.1955 - val_mae: 2.3898\n",
      "Epoch 237/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4656 - mae: 0.9003 - val_loss: 12.9118 - val_mae: 2.5083\n",
      "Epoch 238/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3973 - mae: 0.8967 - val_loss: 10.7752 - val_mae: 2.3269\n",
      "Epoch 239/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5350 - mae: 0.9095 - val_loss: 13.4415 - val_mae: 2.4326\n",
      "Epoch 240/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5068 - mae: 0.8848 - val_loss: 12.2305 - val_mae: 2.4518\n",
      "Epoch 241/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4786 - mae: 0.8889 - val_loss: 12.1278 - val_mae: 2.3556\n",
      "Epoch 242/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4892 - mae: 0.9489 - val_loss: 10.9903 - val_mae: 2.3462\n",
      "Epoch 243/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5896 - mae: 0.8902 - val_loss: 12.7054 - val_mae: 2.6060\n",
      "Epoch 244/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3715 - mae: 0.8847 - val_loss: 12.8471 - val_mae: 2.4364\n",
      "Epoch 245/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3262 - mae: 0.8754 - val_loss: 12.1351 - val_mae: 2.5028\n",
      "Epoch 246/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4256 - mae: 0.9018 - val_loss: 13.8189 - val_mae: 2.4619\n",
      "Epoch 247/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3469 - mae: 0.8579 - val_loss: 12.8147 - val_mae: 2.5158\n",
      "Epoch 248/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5286 - mae: 0.9136 - val_loss: 11.4137 - val_mae: 2.2863\n",
      "Epoch 249/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3766 - mae: 0.8788 - val_loss: 12.4921 - val_mae: 2.4221\n",
      "Epoch 250/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5294 - mae: 0.8739 - val_loss: 12.1520 - val_mae: 2.4758\n",
      "Epoch 251/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2970 - mae: 0.8744 - val_loss: 13.8246 - val_mae: 2.4456\n",
      "Epoch 252/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2962 - mae: 0.8405 - val_loss: 12.4875 - val_mae: 2.3545\n",
      "Epoch 253/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4001 - mae: 0.8883 - val_loss: 13.3277 - val_mae: 2.6125\n",
      "Epoch 254/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3646 - mae: 0.8592 - val_loss: 15.4530 - val_mae: 2.5967\n",
      "Epoch 255/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3055 - mae: 0.8503 - val_loss: 13.2406 - val_mae: 2.4542\n",
      "Epoch 256/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3644 - mae: 0.8829 - val_loss: 12.8625 - val_mae: 2.4056\n",
      "Epoch 257/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2260 - mae: 0.8055 - val_loss: 13.1989 - val_mae: 2.6153\n",
      "Epoch 258/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3540 - mae: 0.8828 - val_loss: 14.4775 - val_mae: 2.5073\n",
      "Epoch 259/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2287 - mae: 0.8460 - val_loss: 13.9007 - val_mae: 2.5072\n",
      "Epoch 260/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3057 - mae: 0.8531 - val_loss: 13.5963 - val_mae: 2.5577\n",
      "Epoch 261/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3835 - mae: 0.8234 - val_loss: 13.3497 - val_mae: 2.4241\n",
      "Epoch 262/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2044 - mae: 0.8587 - val_loss: 13.8750 - val_mae: 2.5005\n",
      "Epoch 263/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4675 - mae: 0.9359 - val_loss: 12.7638 - val_mae: 2.3403\n",
      "Epoch 264/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2164 - mae: 0.8000 - val_loss: 11.7876 - val_mae: 2.3754\n",
      "Epoch 265/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3125 - mae: 0.8159 - val_loss: 11.8894 - val_mae: 2.3870\n",
      "Epoch 266/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4220 - mae: 0.8502 - val_loss: 12.7122 - val_mae: 2.5229\n",
      "Epoch 267/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3969 - mae: 0.8531 - val_loss: 11.8671 - val_mae: 2.2698\n",
      "Epoch 268/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3684 - mae: 0.8808 - val_loss: 13.1517 - val_mae: 2.4350\n",
      "Epoch 269/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3837 - mae: 0.8296 - val_loss: 13.3031 - val_mae: 2.4724\n",
      "Epoch 270/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4385 - mae: 0.8735 - val_loss: 12.0548 - val_mae: 2.3681\n",
      "Epoch 271/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4316 - mae: 0.8788 - val_loss: 11.8407 - val_mae: 2.4090\n",
      "Epoch 272/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2344 - mae: 0.8140 - val_loss: 12.7176 - val_mae: 2.5156\n",
      "Epoch 273/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3506 - mae: 0.8786 - val_loss: 12.8102 - val_mae: 2.4357\n",
      "Epoch 274/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3429 - mae: 0.8579 - val_loss: 12.2927 - val_mae: 2.3921\n",
      "Epoch 275/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0411 - mae: 0.7715 - val_loss: 11.6301 - val_mae: 2.3164\n",
      "Epoch 276/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3608 - mae: 0.8585 - val_loss: 12.5298 - val_mae: 2.3749\n",
      "Epoch 277/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2090 - mae: 0.8340 - val_loss: 11.8659 - val_mae: 2.3672\n",
      "Epoch 278/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2919 - mae: 0.8397 - val_loss: 13.9955 - val_mae: 2.6755\n",
      "Epoch 279/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2278 - mae: 0.8150 - val_loss: 11.7840 - val_mae: 2.3017\n",
      "Epoch 280/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2787 - mae: 0.8343 - val_loss: 11.9991 - val_mae: 2.3312\n",
      "Epoch 281/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4484 - mae: 0.8582 - val_loss: 12.5387 - val_mae: 2.4803\n",
      "Epoch 282/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2291 - mae: 0.8437 - val_loss: 14.4313 - val_mae: 2.4711\n",
      "Epoch 283/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3376 - mae: 0.8408 - val_loss: 12.2301 - val_mae: 2.4741\n",
      "Epoch 284/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2318 - mae: 0.7994 - val_loss: 14.4633 - val_mae: 2.4242\n",
      "Epoch 285/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1467 - mae: 0.7998 - val_loss: 12.7059 - val_mae: 2.4155\n",
      "Epoch 286/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0890 - mae: 0.7603 - val_loss: 11.9231 - val_mae: 2.4314\n",
      "Epoch 287/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2176 - mae: 0.8489 - val_loss: 13.6927 - val_mae: 2.5058\n",
      "Epoch 288/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2902 - mae: 0.8342 - val_loss: 13.9086 - val_mae: 2.5243\n",
      "Epoch 289/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1559 - mae: 0.7727 - val_loss: 12.4705 - val_mae: 2.3114\n",
      "Epoch 290/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0874 - mae: 0.8095 - val_loss: 13.9607 - val_mae: 2.4662\n",
      "Epoch 291/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2208 - mae: 0.8287 - val_loss: 12.9171 - val_mae: 2.4048\n",
      "Epoch 292/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0645 - mae: 0.7624 - val_loss: 14.4734 - val_mae: 2.5714\n",
      "Epoch 293/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2538 - mae: 0.8117 - val_loss: 14.4029 - val_mae: 2.5663\n",
      "Epoch 294/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1121 - mae: 0.7932 - val_loss: 11.9020 - val_mae: 2.3314\n",
      "Epoch 295/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2053 - mae: 0.8150 - val_loss: 12.7781 - val_mae: 2.3610\n",
      "Epoch 296/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2190 - mae: 0.8051 - val_loss: 11.9599 - val_mae: 2.3502\n",
      "Epoch 297/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2841 - mae: 0.8344 - val_loss: 12.5979 - val_mae: 2.4530\n",
      "Epoch 298/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1405 - mae: 0.8071 - val_loss: 11.9599 - val_mae: 2.3261\n",
      "Epoch 299/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2406 - mae: 0.8021 - val_loss: 11.4885 - val_mae: 2.2485\n",
      "Epoch 300/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1367 - mae: 0.8014 - val_loss: 13.1727 - val_mae: 2.5124\n",
      "Epoch 301/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2048 - mae: 0.8232 - val_loss: 14.0924 - val_mae: 2.5350\n",
      "Epoch 302/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3176 - mae: 0.8544 - val_loss: 12.1913 - val_mae: 2.3244\n",
      "Epoch 303/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0227 - mae: 0.7566 - val_loss: 12.4983 - val_mae: 2.4758\n",
      "Epoch 304/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1181 - mae: 0.7904 - val_loss: 13.2128 - val_mae: 2.4183\n",
      "Epoch 305/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0933 - mae: 0.7971 - val_loss: 11.7940 - val_mae: 2.3563\n",
      "Epoch 306/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2514 - mae: 0.8471 - val_loss: 12.6515 - val_mae: 2.4145\n",
      "Epoch 307/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0140 - mae: 0.7542 - val_loss: 14.4022 - val_mae: 2.6091\n",
      "Epoch 308/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1813 - mae: 0.7892 - val_loss: 10.9392 - val_mae: 2.3320\n",
      "Epoch 309/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1778 - mae: 0.7579 - val_loss: 11.7100 - val_mae: 2.3184\n",
      "Epoch 310/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1693 - mae: 0.7895 - val_loss: 12.2000 - val_mae: 2.3194\n",
      "Epoch 311/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0297 - mae: 0.7572 - val_loss: 13.4845 - val_mae: 2.5683\n",
      "Epoch 312/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1474 - mae: 0.7908 - val_loss: 12.6521 - val_mae: 2.3757\n",
      "Epoch 313/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0646 - mae: 0.7519 - val_loss: 10.6233 - val_mae: 2.2275\n",
      "Epoch 314/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2742 - mae: 0.8284 - val_loss: 12.0474 - val_mae: 2.2506\n",
      "Epoch 315/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0958 - mae: 0.7823 - val_loss: 12.4199 - val_mae: 2.4113\n",
      "Epoch 316/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0315 - mae: 0.7677 - val_loss: 11.7640 - val_mae: 2.3139\n",
      "Epoch 317/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1208 - mae: 0.7691 - val_loss: 12.5628 - val_mae: 2.3394\n",
      "Epoch 318/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1979 - mae: 0.7849 - val_loss: 13.0144 - val_mae: 2.4235\n",
      "Epoch 319/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0328 - mae: 0.7924 - val_loss: 12.6730 - val_mae: 2.4697\n",
      "Epoch 320/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1858 - mae: 0.7872 - val_loss: 13.1836 - val_mae: 2.4366\n",
      "Epoch 321/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0870 - mae: 0.7606 - val_loss: 12.6377 - val_mae: 2.3933\n",
      "Epoch 322/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0408 - mae: 0.7682 - val_loss: 15.1213 - val_mae: 2.5950\n",
      "Epoch 323/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9972 - mae: 0.7444 - val_loss: 12.6777 - val_mae: 2.4311\n",
      "Epoch 324/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1550 - mae: 0.7962 - val_loss: 12.3321 - val_mae: 2.4379\n",
      "Epoch 325/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0880 - mae: 0.7565 - val_loss: 12.5049 - val_mae: 2.3897\n",
      "Epoch 326/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0135 - mae: 0.7654 - val_loss: 12.3649 - val_mae: 2.4198\n",
      "Epoch 327/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0885 - mae: 0.7883 - val_loss: 11.6651 - val_mae: 2.3537\n",
      "Epoch 328/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0217 - mae: 0.7564 - val_loss: 12.3834 - val_mae: 2.3792\n",
      "Epoch 329/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9610 - mae: 0.7346 - val_loss: 14.5337 - val_mae: 2.5742\n",
      "Epoch 330/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1129 - mae: 0.7743 - val_loss: 14.1925 - val_mae: 2.5889\n",
      "Epoch 331/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1071 - mae: 0.7898 - val_loss: 12.6103 - val_mae: 2.3404\n",
      "Epoch 332/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9966 - mae: 0.7331 - val_loss: 12.8327 - val_mae: 2.4395\n",
      "Epoch 333/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9121 - mae: 0.7371 - val_loss: 12.3672 - val_mae: 2.4330\n",
      "Epoch 334/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2403 - mae: 0.7866 - val_loss: 13.5562 - val_mae: 2.4811\n",
      "Epoch 335/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0992 - mae: 0.7604 - val_loss: 12.2653 - val_mae: 2.4638\n",
      "Epoch 336/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0996 - mae: 0.7345 - val_loss: 13.6639 - val_mae: 2.6675\n",
      "Epoch 337/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1278 - mae: 0.7763 - val_loss: 12.9510 - val_mae: 2.4762\n",
      "Epoch 338/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9741 - mae: 0.7457 - val_loss: 13.9120 - val_mae: 2.4844\n",
      "Epoch 339/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0231 - mae: 0.7424 - val_loss: 13.4234 - val_mae: 2.4704\n",
      "Epoch 340/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0271 - mae: 0.7621 - val_loss: 13.6040 - val_mae: 2.5003\n",
      "Epoch 341/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0178 - mae: 0.7274 - val_loss: 13.6075 - val_mae: 2.4184\n",
      "Epoch 342/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8947 - mae: 0.7026 - val_loss: 14.6585 - val_mae: 2.7465\n",
      "Epoch 343/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0755 - mae: 0.7649 - val_loss: 13.8315 - val_mae: 2.4909\n",
      "Epoch 344/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0417 - mae: 0.7360 - val_loss: 12.7854 - val_mae: 2.3943\n",
      "Epoch 345/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0858 - mae: 0.7390 - val_loss: 12.2769 - val_mae: 2.4537\n",
      "Epoch 346/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9349 - mae: 0.7298 - val_loss: 13.4177 - val_mae: 2.3821\n",
      "Epoch 347/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0192 - mae: 0.7445 - val_loss: 13.0318 - val_mae: 2.4390\n",
      "Epoch 348/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3363 - mae: 0.7687 - val_loss: 13.8728 - val_mae: 2.5532\n",
      "Epoch 349/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0266 - mae: 0.7589 - val_loss: 12.8542 - val_mae: 2.3953\n",
      "Epoch 350/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0775 - mae: 0.7747 - val_loss: 13.3196 - val_mae: 2.4114\n",
      "Epoch 351/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9694 - mae: 0.6943 - val_loss: 13.9604 - val_mae: 2.5019\n",
      "Epoch 352/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9003 - mae: 0.7149 - val_loss: 14.4716 - val_mae: 2.6587\n",
      "Epoch 353/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0562 - mae: 0.7763 - val_loss: 11.9654 - val_mae: 2.3938\n",
      "Epoch 354/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8938 - mae: 0.7204 - val_loss: 11.7064 - val_mae: 2.2849\n",
      "Epoch 355/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9055 - mae: 0.6911 - val_loss: 14.2199 - val_mae: 2.6350\n",
      "Epoch 356/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0545 - mae: 0.7518 - val_loss: 12.5157 - val_mae: 2.3773\n",
      "Epoch 357/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8593 - mae: 0.7068 - val_loss: 14.2414 - val_mae: 2.4992\n",
      "Epoch 358/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1019 - mae: 0.7531 - val_loss: 13.2222 - val_mae: 2.5345\n",
      "Epoch 359/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8742 - mae: 0.6904 - val_loss: 13.9966 - val_mae: 2.5353\n",
      "Epoch 360/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8463 - mae: 0.6956 - val_loss: 13.4847 - val_mae: 2.5893\n",
      "Epoch 361/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0342 - mae: 0.7139 - val_loss: 12.3444 - val_mae: 2.3592\n",
      "Epoch 362/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9838 - mae: 0.7297 - val_loss: 13.6160 - val_mae: 2.5674\n",
      "Epoch 363/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8953 - mae: 0.7024 - val_loss: 13.9057 - val_mae: 2.3638\n",
      "Epoch 364/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0540 - mae: 0.7218 - val_loss: 14.5924 - val_mae: 2.5701\n",
      "Epoch 365/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8957 - mae: 0.7329 - val_loss: 15.9064 - val_mae: 2.5674\n",
      "Epoch 366/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0170 - mae: 0.7514 - val_loss: 14.4439 - val_mae: 2.4311\n",
      "Epoch 367/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8166 - mae: 0.6756 - val_loss: 14.9135 - val_mae: 2.4500\n",
      "Epoch 368/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0652 - mae: 0.6950 - val_loss: 13.3560 - val_mae: 2.3642\n",
      "Epoch 369/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9691 - mae: 0.7423 - val_loss: 14.6690 - val_mae: 2.5267\n",
      "Epoch 370/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8591 - mae: 0.7042 - val_loss: 13.0397 - val_mae: 2.4947\n",
      "Epoch 371/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9623 - mae: 0.7287 - val_loss: 14.0532 - val_mae: 2.4474\n",
      "Epoch 372/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0392 - mae: 0.7123 - val_loss: 13.2433 - val_mae: 2.4654\n",
      "Epoch 373/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0043 - mae: 0.7229 - val_loss: 13.4687 - val_mae: 2.4235\n",
      "Epoch 374/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9114 - mae: 0.7052 - val_loss: 13.6320 - val_mae: 2.5297\n",
      "Epoch 375/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9390 - mae: 0.7095 - val_loss: 16.4109 - val_mae: 2.6382\n",
      "Epoch 376/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9371 - mae: 0.7192 - val_loss: 12.9528 - val_mae: 2.4576\n",
      "Epoch 377/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9374 - mae: 0.7330 - val_loss: 13.1395 - val_mae: 2.3974\n",
      "Epoch 378/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0033 - mae: 0.7357 - val_loss: 14.2908 - val_mae: 2.5250\n",
      "Epoch 379/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9797 - mae: 0.7053 - val_loss: 13.9599 - val_mae: 2.4956\n",
      "Epoch 380/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9668 - mae: 0.7098 - val_loss: 12.7744 - val_mae: 2.3986\n",
      "Epoch 381/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0131 - mae: 0.7107 - val_loss: 15.2700 - val_mae: 2.6385\n",
      "Epoch 382/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7341 - mae: 0.6586 - val_loss: 13.3237 - val_mae: 2.4632\n",
      "Epoch 383/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9281 - mae: 0.7160 - val_loss: 13.3186 - val_mae: 2.5608\n",
      "Epoch 384/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8258 - mae: 0.6631 - val_loss: 14.9246 - val_mae: 2.5284\n",
      "Epoch 385/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8176 - mae: 0.6721 - val_loss: 13.1433 - val_mae: 2.4421\n",
      "Epoch 386/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8180 - mae: 0.6803 - val_loss: 13.5818 - val_mae: 2.5010\n",
      "Epoch 387/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8312 - mae: 0.6753 - val_loss: 14.2022 - val_mae: 2.3936\n",
      "Epoch 388/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9453 - mae: 0.7306 - val_loss: 13.5787 - val_mae: 2.5640\n",
      "Epoch 389/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8728 - mae: 0.7007 - val_loss: 14.2873 - val_mae: 2.4524\n",
      "Epoch 390/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7817 - mae: 0.6511 - val_loss: 14.2931 - val_mae: 2.5049\n",
      "Epoch 391/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9031 - mae: 0.6913 - val_loss: 14.6372 - val_mae: 2.4656\n",
      "Epoch 392/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9607 - mae: 0.7447 - val_loss: 12.6593 - val_mae: 2.3251\n",
      "Epoch 393/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8430 - mae: 0.6376 - val_loss: 14.7921 - val_mae: 2.4903\n",
      "Epoch 394/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9037 - mae: 0.7199 - val_loss: 14.6971 - val_mae: 2.5757\n",
      "Epoch 395/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9359 - mae: 0.6929 - val_loss: 14.5927 - val_mae: 2.5989\n",
      "Epoch 396/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8661 - mae: 0.6929 - val_loss: 12.1167 - val_mae: 2.3621\n",
      "Epoch 397/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9175 - mae: 0.6690 - val_loss: 12.4213 - val_mae: 2.3195\n",
      "Epoch 398/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7211 - mae: 0.6383 - val_loss: 13.6297 - val_mae: 2.4971\n",
      "Epoch 399/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8169 - mae: 0.7047 - val_loss: 14.2737 - val_mae: 2.4942\n",
      "Epoch 400/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8439 - mae: 0.6756 - val_loss: 13.8220 - val_mae: 2.4084\n",
      "Epoch 401/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8354 - mae: 0.6748 - val_loss: 12.6982 - val_mae: 2.3816\n",
      "Epoch 402/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0730 - mae: 0.7041 - val_loss: 14.2466 - val_mae: 2.5599\n",
      "Epoch 403/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8370 - mae: 0.6862 - val_loss: 13.2203 - val_mae: 2.5100\n",
      "Epoch 404/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8095 - mae: 0.6774 - val_loss: 12.8457 - val_mae: 2.3604\n",
      "Epoch 405/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8093 - mae: 0.6770 - val_loss: 12.9837 - val_mae: 2.4688\n",
      "Epoch 406/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8486 - mae: 0.7116 - val_loss: 13.6348 - val_mae: 2.4641\n",
      "Epoch 407/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8435 - mae: 0.6812 - val_loss: 13.2280 - val_mae: 2.4870\n",
      "Epoch 408/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7711 - mae: 0.6176 - val_loss: 14.0097 - val_mae: 2.4392\n",
      "Epoch 409/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9102 - mae: 0.6954 - val_loss: 13.7328 - val_mae: 2.5247\n",
      "Epoch 410/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7898 - mae: 0.6417 - val_loss: 13.4823 - val_mae: 2.5051\n",
      "Epoch 411/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8257 - mae: 0.6673 - val_loss: 14.0708 - val_mae: 2.5364\n",
      "Epoch 412/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9181 - mae: 0.7081 - val_loss: 14.3968 - val_mae: 2.5309\n",
      "Epoch 413/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8623 - mae: 0.6821 - val_loss: 13.4617 - val_mae: 2.5367\n",
      "Epoch 414/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8115 - mae: 0.6730 - val_loss: 14.9205 - val_mae: 2.6717\n",
      "Epoch 415/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8097 - mae: 0.6697 - val_loss: 13.3273 - val_mae: 2.4621\n",
      "Epoch 416/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8545 - mae: 0.6998 - val_loss: 12.9264 - val_mae: 2.3030\n",
      "Epoch 417/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8360 - mae: 0.6746 - val_loss: 12.7801 - val_mae: 2.4529\n",
      "Epoch 418/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8018 - mae: 0.6382 - val_loss: 13.6482 - val_mae: 2.4833\n",
      "Epoch 419/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9247 - mae: 0.6529 - val_loss: 14.0199 - val_mae: 2.4554\n",
      "Epoch 420/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8401 - mae: 0.6383 - val_loss: 13.8612 - val_mae: 2.4682\n",
      "Epoch 421/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8484 - mae: 0.6909 - val_loss: 13.4922 - val_mae: 2.4845\n",
      "Epoch 422/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8183 - mae: 0.6781 - val_loss: 12.7748 - val_mae: 2.4483\n",
      "Epoch 423/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7610 - mae: 0.6655 - val_loss: 12.6995 - val_mae: 2.3148\n",
      "Epoch 424/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8937 - mae: 0.6927 - val_loss: 14.2728 - val_mae: 2.5021\n",
      "Epoch 425/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6909 - mae: 0.6222 - val_loss: 12.6304 - val_mae: 2.4077\n",
      "Epoch 426/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8659 - mae: 0.6729 - val_loss: 15.0086 - val_mae: 2.6136\n",
      "Epoch 427/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8189 - mae: 0.6853 - val_loss: 14.2188 - val_mae: 2.5387\n",
      "Epoch 428/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8538 - mae: 0.6925 - val_loss: 13.2785 - val_mae: 2.4505\n",
      "Epoch 429/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0435 - mae: 0.6630 - val_loss: 13.8426 - val_mae: 2.5013\n",
      "Epoch 430/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8096 - mae: 0.6599 - val_loss: 13.0666 - val_mae: 2.4533\n",
      "Epoch 431/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8472 - mae: 0.6519 - val_loss: 14.3870 - val_mae: 2.4814\n",
      "Epoch 432/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7643 - mae: 0.6111 - val_loss: 14.8888 - val_mae: 2.6747\n",
      "Epoch 433/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6738 - mae: 0.5903 - val_loss: 12.8178 - val_mae: 2.4531\n",
      "Epoch 434/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8671 - mae: 0.6856 - val_loss: 13.4263 - val_mae: 2.4650\n",
      "Epoch 435/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7352 - mae: 0.6300 - val_loss: 15.3226 - val_mae: 2.5182\n",
      "Epoch 436/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8573 - mae: 0.6430 - val_loss: 13.3865 - val_mae: 2.4585\n",
      "Epoch 437/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8061 - mae: 0.6561 - val_loss: 13.3877 - val_mae: 2.4908\n",
      "Epoch 438/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7369 - mae: 0.6337 - val_loss: 14.1550 - val_mae: 2.4904\n",
      "Epoch 439/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6800 - mae: 0.6151 - val_loss: 13.4021 - val_mae: 2.4940\n",
      "Epoch 440/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7739 - mae: 0.6450 - val_loss: 14.9101 - val_mae: 2.7026\n",
      "Epoch 441/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8094 - mae: 0.6809 - val_loss: 15.3943 - val_mae: 2.5902\n",
      "Epoch 442/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7843 - mae: 0.6284 - val_loss: 13.0303 - val_mae: 2.4984\n",
      "Epoch 443/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7549 - mae: 0.6372 - val_loss: 14.6365 - val_mae: 2.5884\n",
      "Epoch 444/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8232 - mae: 0.6506 - val_loss: 15.1581 - val_mae: 2.5852\n",
      "Epoch 445/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7797 - mae: 0.6364 - val_loss: 14.5253 - val_mae: 2.5138\n",
      "Epoch 446/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6667 - mae: 0.6383 - val_loss: 12.9800 - val_mae: 2.5269\n",
      "Epoch 447/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8091 - mae: 0.6359 - val_loss: 13.0028 - val_mae: 2.4126\n",
      "Epoch 448/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7773 - mae: 0.6390 - val_loss: 14.2969 - val_mae: 2.6136\n",
      "Epoch 449/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6907 - mae: 0.6169 - val_loss: 12.8776 - val_mae: 2.4442\n",
      "Epoch 450/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7821 - mae: 0.6372 - val_loss: 13.7975 - val_mae: 2.5551\n",
      "Epoch 451/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7583 - mae: 0.6260 - val_loss: 14.5845 - val_mae: 2.5985\n",
      "Epoch 452/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7444 - mae: 0.6498 - val_loss: 12.6858 - val_mae: 2.4727\n",
      "Epoch 453/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7967 - mae: 0.6611 - val_loss: 12.8838 - val_mae: 2.4992\n",
      "Epoch 454/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9324 - mae: 0.6394 - val_loss: 13.5880 - val_mae: 2.5095\n",
      "Epoch 455/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7946 - mae: 0.6484 - val_loss: 15.3965 - val_mae: 2.6228\n",
      "Epoch 456/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6570 - mae: 0.5819 - val_loss: 15.2041 - val_mae: 2.5661\n",
      "Epoch 457/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7636 - mae: 0.6201 - val_loss: 19.0072 - val_mae: 2.8532\n",
      "Epoch 458/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7199 - mae: 0.6016 - val_loss: 12.2845 - val_mae: 2.4693\n",
      "Epoch 459/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8803 - mae: 0.6720 - val_loss: 12.8758 - val_mae: 2.5079\n",
      "Epoch 460/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7914 - mae: 0.6575 - val_loss: 13.9692 - val_mae: 2.5846\n",
      "Epoch 461/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8207 - mae: 0.6353 - val_loss: 16.2868 - val_mae: 2.6278\n",
      "Epoch 462/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7068 - mae: 0.6161 - val_loss: 13.8968 - val_mae: 2.5203\n",
      "Epoch 463/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7360 - mae: 0.5988 - val_loss: 13.4655 - val_mae: 2.4909\n",
      "Epoch 464/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7318 - mae: 0.6154 - val_loss: 16.5154 - val_mae: 2.6739\n",
      "Epoch 465/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7076 - mae: 0.5857 - val_loss: 14.3194 - val_mae: 2.4879\n",
      "Epoch 466/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8540 - mae: 0.6839 - val_loss: 13.5449 - val_mae: 2.4519\n",
      "Epoch 467/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7556 - mae: 0.6252 - val_loss: 14.5406 - val_mae: 2.4879\n",
      "Epoch 468/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6364 - mae: 0.5864 - val_loss: 14.6463 - val_mae: 2.6530\n",
      "Epoch 469/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7508 - mae: 0.6511 - val_loss: 14.4568 - val_mae: 2.5753\n",
      "Epoch 470/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7424 - mae: 0.6151 - val_loss: 14.7095 - val_mae: 2.5305\n",
      "Epoch 471/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7869 - mae: 0.6537 - val_loss: 14.3945 - val_mae: 2.4761\n",
      "Epoch 472/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7276 - mae: 0.6352 - val_loss: 14.1509 - val_mae: 2.5523\n",
      "Epoch 473/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6290 - mae: 0.5888 - val_loss: 14.3801 - val_mae: 2.5895\n",
      "Epoch 474/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6467 - mae: 0.6249 - val_loss: 15.2121 - val_mae: 2.6346\n",
      "Epoch 475/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7423 - mae: 0.6197 - val_loss: 14.3253 - val_mae: 2.5929\n",
      "Epoch 476/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7296 - mae: 0.6326 - val_loss: 12.6703 - val_mae: 2.5320\n",
      "Epoch 477/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8246 - mae: 0.6369 - val_loss: 15.4830 - val_mae: 2.6958\n",
      "Epoch 478/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6558 - mae: 0.6061 - val_loss: 14.1674 - val_mae: 2.6922\n",
      "Epoch 479/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7765 - mae: 0.6463 - val_loss: 16.6699 - val_mae: 2.7530\n",
      "Epoch 480/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6621 - mae: 0.6146 - val_loss: 15.8632 - val_mae: 2.7321\n",
      "Epoch 481/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7314 - mae: 0.6268 - val_loss: 14.2753 - val_mae: 2.6603\n",
      "Epoch 482/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7955 - mae: 0.6455 - val_loss: 14.1612 - val_mae: 2.5401\n",
      "Epoch 483/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7608 - mae: 0.6304 - val_loss: 13.9142 - val_mae: 2.6102\n",
      "Epoch 484/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7698 - mae: 0.6192 - val_loss: 12.7091 - val_mae: 2.5215\n",
      "Epoch 485/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7383 - mae: 0.6326 - val_loss: 15.6373 - val_mae: 2.5577\n",
      "Epoch 486/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8726 - mae: 0.6535 - val_loss: 15.4260 - val_mae: 2.6174\n",
      "Epoch 487/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6669 - mae: 0.6067 - val_loss: 12.5688 - val_mae: 2.4634\n",
      "Epoch 488/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7012 - mae: 0.6371 - val_loss: 15.4958 - val_mae: 2.6202\n",
      "Epoch 489/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8534 - mae: 0.6194 - val_loss: 15.4573 - val_mae: 2.6306\n",
      "Epoch 490/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7131 - mae: 0.6120 - val_loss: 13.5743 - val_mae: 2.5641\n",
      "Epoch 491/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6466 - mae: 0.6057 - val_loss: 13.8396 - val_mae: 2.5369\n",
      "Epoch 492/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6832 - mae: 0.6193 - val_loss: 14.2536 - val_mae: 2.6011\n",
      "Epoch 493/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6794 - mae: 0.6056 - val_loss: 14.1567 - val_mae: 2.6357\n",
      "Epoch 494/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6674 - mae: 0.5765 - val_loss: 14.2362 - val_mae: 2.6249\n",
      "Epoch 495/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6994 - mae: 0.6129 - val_loss: 15.2640 - val_mae: 2.6766\n",
      "Epoch 496/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7519 - mae: 0.6475 - val_loss: 13.4437 - val_mae: 2.6021\n",
      "Epoch 497/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7350 - mae: 0.6140 - val_loss: 16.4729 - val_mae: 2.6249\n",
      "Epoch 498/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7394 - mae: 0.6168 - val_loss: 14.7407 - val_mae: 2.5452\n",
      "Epoch 499/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6653 - mae: 0.6139 - val_loss: 14.8225 - val_mae: 2.6140\n",
      "Epoch 500/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7959 - mae: 0.6353 - val_loss: 15.5807 - val_mae: 2.6629\n",
      "처리중인 폴드: 1\n",
      "Train on 303 samples, validate on 101 samples\n",
      "Epoch 1/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 212.6634 - mae: 10.9526 - val_loss: 38.7614 - val_mae: 4.5256\n",
      "Epoch 2/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 28.9803 - mae: 3.6856 - val_loss: 20.5532 - val_mae: 3.3265\n",
      "Epoch 3/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 21.2034 - mae: 2.9807 - val_loss: 17.0147 - val_mae: 3.0722\n",
      "Epoch 4/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 16.5382 - mae: 2.6904 - val_loss: 14.4729 - val_mae: 2.8689\n",
      "Epoch 5/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 14.8313 - mae: 2.5270 - val_loss: 15.8886 - val_mae: 3.0920\n",
      "Epoch 6/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 13.1006 - mae: 2.4358 - val_loss: 14.0894 - val_mae: 2.8687\n",
      "Epoch 7/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 12.5647 - mae: 2.3461 - val_loss: 12.2849 - val_mae: 2.6232\n",
      "Epoch 8/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 11.6527 - mae: 2.2626 - val_loss: 12.2702 - val_mae: 2.6732\n",
      "Epoch 9/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.9589 - mae: 2.2852 - val_loss: 13.8790 - val_mae: 2.9864\n",
      "Epoch 10/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.5937 - mae: 2.1738 - val_loss: 12.3465 - val_mae: 2.7436\n",
      "Epoch 11/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 11.0007 - mae: 2.1443 - val_loss: 12.1099 - val_mae: 2.6849\n",
      "Epoch 12/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.3679 - mae: 2.1514 - val_loss: 11.7289 - val_mae: 2.6719\n",
      "Epoch 13/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.5702 - mae: 2.1119 - val_loss: 11.3055 - val_mae: 2.5507\n",
      "Epoch 14/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.2439 - mae: 2.1067 - val_loss: 11.4351 - val_mae: 2.5821\n",
      "Epoch 15/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.0217 - mae: 2.0426 - val_loss: 9.9756 - val_mae: 2.3736\n",
      "Epoch 16/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.7523 - mae: 2.0431 - val_loss: 10.7301 - val_mae: 2.5178\n",
      "Epoch 17/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.2155 - mae: 2.0312 - val_loss: 9.5962 - val_mae: 2.3516\n",
      "Epoch 18/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.8868 - mae: 2.0002 - val_loss: 15.3999 - val_mae: 3.0831\n",
      "Epoch 19/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.1183 - mae: 1.9986 - val_loss: 11.0707 - val_mae: 2.5816\n",
      "Epoch 20/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.2059 - mae: 1.9238 - val_loss: 10.4804 - val_mae: 2.4347\n",
      "Epoch 21/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.8328 - mae: 1.9843 - val_loss: 11.1076 - val_mae: 2.6207\n",
      "Epoch 22/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.6833 - mae: 1.9826 - val_loss: 10.6053 - val_mae: 2.5139\n",
      "Epoch 23/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.4798 - mae: 1.9409 - val_loss: 11.6293 - val_mae: 2.5871\n",
      "Epoch 24/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.5021 - mae: 1.9514 - val_loss: 9.7064 - val_mae: 2.3553\n",
      "Epoch 25/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.5159 - mae: 1.8756 - val_loss: 9.6717 - val_mae: 2.3563\n",
      "Epoch 26/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.6438 - mae: 1.8947 - val_loss: 10.5569 - val_mae: 2.4792\n",
      "Epoch 27/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.2303 - mae: 1.8228 - val_loss: 10.7642 - val_mae: 2.4446\n",
      "Epoch 28/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.4385 - mae: 1.8986 - val_loss: 9.9051 - val_mae: 2.3427\n",
      "Epoch 29/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.9775 - mae: 1.8897 - val_loss: 9.5887 - val_mae: 2.3689\n",
      "Epoch 30/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.9980 - mae: 1.8860 - val_loss: 10.0387 - val_mae: 2.4243\n",
      "Epoch 31/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.7674 - mae: 1.8509 - val_loss: 10.7968 - val_mae: 2.5282\n",
      "Epoch 32/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.7841 - mae: 1.8312 - val_loss: 8.8234 - val_mae: 2.2821\n",
      "Epoch 33/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.7660 - mae: 1.8299 - val_loss: 9.9387 - val_mae: 2.4041\n",
      "Epoch 34/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.3842 - mae: 1.7480 - val_loss: 10.2974 - val_mae: 2.5010\n",
      "Epoch 35/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.1198 - mae: 1.7933 - val_loss: 9.3911 - val_mae: 2.3261\n",
      "Epoch 36/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.3286 - mae: 1.8249 - val_loss: 9.5795 - val_mae: 2.4209\n",
      "Epoch 37/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.1123 - mae: 1.8388 - val_loss: 9.3087 - val_mae: 2.3034\n",
      "Epoch 38/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.3203 - mae: 1.7843 - val_loss: 9.8138 - val_mae: 2.3557\n",
      "Epoch 39/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.4425 - mae: 1.8666 - val_loss: 11.1736 - val_mae: 2.5604\n",
      "Epoch 40/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.2795 - mae: 1.7737 - val_loss: 13.1420 - val_mae: 2.8187\n",
      "Epoch 41/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.0985 - mae: 1.7156 - val_loss: 10.6280 - val_mae: 2.4773\n",
      "Epoch 42/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.8645 - mae: 1.7055 - val_loss: 9.4080 - val_mae: 2.3386\n",
      "Epoch 43/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.8301 - mae: 1.7605 - val_loss: 9.6421 - val_mae: 2.3516\n",
      "Epoch 44/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.9237 - mae: 1.7182 - val_loss: 9.5092 - val_mae: 2.3855\n",
      "Epoch 45/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.4784 - mae: 1.6835 - val_loss: 16.2943 - val_mae: 3.1537\n",
      "Epoch 46/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.0647 - mae: 1.7492 - val_loss: 10.1065 - val_mae: 2.4227\n",
      "Epoch 47/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.7085 - mae: 1.7345 - val_loss: 9.0852 - val_mae: 2.2421\n",
      "Epoch 48/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.4350 - mae: 1.6814 - val_loss: 10.7776 - val_mae: 2.6007\n",
      "Epoch 49/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.7584 - mae: 1.6939 - val_loss: 11.2188 - val_mae: 2.4605\n",
      "Epoch 50/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.0302 - mae: 1.6354 - val_loss: 10.5168 - val_mae: 2.4091\n",
      "Epoch 51/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.2689 - mae: 1.6475 - val_loss: 9.6506 - val_mae: 2.3729\n",
      "Epoch 52/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.1746 - mae: 1.6670 - val_loss: 9.7254 - val_mae: 2.3728\n",
      "Epoch 53/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.2306 - mae: 1.6428 - val_loss: 11.9196 - val_mae: 2.5433\n",
      "Epoch 54/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.1654 - mae: 1.6775 - val_loss: 12.0531 - val_mae: 2.6553\n",
      "Epoch 55/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.8551 - mae: 1.6432 - val_loss: 9.4043 - val_mae: 2.2645\n",
      "Epoch 56/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.9133 - mae: 1.5632 - val_loss: 9.3332 - val_mae: 2.2667\n",
      "Epoch 57/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.7872 - mae: 1.6138 - val_loss: 10.4776 - val_mae: 2.4480\n",
      "Epoch 58/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.9050 - mae: 1.6113 - val_loss: 10.3196 - val_mae: 2.4171\n",
      "Epoch 59/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.6642 - mae: 1.5595 - val_loss: 10.2764 - val_mae: 2.4072\n",
      "Epoch 60/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.3019 - mae: 1.5793 - val_loss: 8.6146 - val_mae: 2.2392\n",
      "Epoch 61/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.6356 - mae: 1.5089 - val_loss: 10.1699 - val_mae: 2.3755\n",
      "Epoch 62/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.8362 - mae: 1.5689 - val_loss: 9.8725 - val_mae: 2.3629\n",
      "Epoch 63/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.1021 - mae: 1.5393 - val_loss: 12.6069 - val_mae: 2.5945\n",
      "Epoch 64/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.6683 - mae: 1.5818 - val_loss: 10.2033 - val_mae: 2.4543\n",
      "Epoch 65/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.4109 - mae: 1.5045 - val_loss: 13.1099 - val_mae: 2.7774\n",
      "Epoch 66/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.1901 - mae: 1.5494 - val_loss: 11.2686 - val_mae: 2.5021\n",
      "Epoch 67/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9668 - mae: 1.5125 - val_loss: 10.4391 - val_mae: 2.3430\n",
      "Epoch 68/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0764 - mae: 1.4426 - val_loss: 10.5327 - val_mae: 2.4088\n",
      "Epoch 69/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8154 - mae: 1.4603 - val_loss: 11.8832 - val_mae: 2.5154\n",
      "Epoch 70/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8753 - mae: 1.5203 - val_loss: 10.3973 - val_mae: 2.3143\n",
      "Epoch 71/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9184 - mae: 1.4525 - val_loss: 10.3370 - val_mae: 2.4203\n",
      "Epoch 72/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.7071 - mae: 1.4740 - val_loss: 10.0244 - val_mae: 2.3521\n",
      "Epoch 73/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.7164 - mae: 1.5213 - val_loss: 14.4122 - val_mae: 2.8230\n",
      "Epoch 74/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8976 - mae: 1.4872 - val_loss: 9.5813 - val_mae: 2.2648\n",
      "Epoch 75/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8675 - mae: 1.4622 - val_loss: 10.7467 - val_mae: 2.3642\n",
      "Epoch 76/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.7791 - mae: 1.4715 - val_loss: 10.4098 - val_mae: 2.4164\n",
      "Epoch 77/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8148 - mae: 1.4095 - val_loss: 10.3723 - val_mae: 2.4022\n",
      "Epoch 78/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2452 - mae: 1.3473 - val_loss: 11.2793 - val_mae: 2.4457\n",
      "Epoch 79/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4512 - mae: 1.4247 - val_loss: 12.1809 - val_mae: 2.5987\n",
      "Epoch 80/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.6038 - mae: 1.4095 - val_loss: 13.6493 - val_mae: 2.6285\n",
      "Epoch 81/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2465 - mae: 1.4369 - val_loss: 14.1148 - val_mae: 2.7818\n",
      "Epoch 82/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.6584 - mae: 1.4290 - val_loss: 9.6181 - val_mae: 2.3003\n",
      "Epoch 83/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3168 - mae: 1.3883 - val_loss: 12.0311 - val_mae: 2.6050\n",
      "Epoch 84/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.9193 - mae: 1.4118 - val_loss: 12.4659 - val_mae: 2.5683\n",
      "Epoch 85/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3723 - mae: 1.3942 - val_loss: 13.3896 - val_mae: 2.6727\n",
      "Epoch 86/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.1880 - mae: 1.3734 - val_loss: 10.6401 - val_mae: 2.3402\n",
      "Epoch 87/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7419 - mae: 1.3126 - val_loss: 10.0299 - val_mae: 2.2843\n",
      "Epoch 88/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0933 - mae: 1.3939 - val_loss: 12.3269 - val_mae: 2.5203\n",
      "Epoch 89/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0014 - mae: 1.3684 - val_loss: 13.9085 - val_mae: 2.7634\n",
      "Epoch 90/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7704 - mae: 1.2920 - val_loss: 11.3823 - val_mae: 2.5400\n",
      "Epoch 91/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.1629 - mae: 1.3244 - val_loss: 14.3943 - val_mae: 2.8578\n",
      "Epoch 92/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6005 - mae: 1.3099 - val_loss: 12.5116 - val_mae: 2.5148\n",
      "Epoch 93/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8308 - mae: 1.3818 - val_loss: 11.4288 - val_mae: 2.5209\n",
      "Epoch 94/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8063 - mae: 1.2694 - val_loss: 17.8168 - val_mae: 3.1669\n",
      "Epoch 95/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6957 - mae: 1.3184 - val_loss: 12.4503 - val_mae: 2.6225\n",
      "Epoch 96/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7044 - mae: 1.2828 - val_loss: 10.9857 - val_mae: 2.3498\n",
      "Epoch 97/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.9991 - mae: 1.3284 - val_loss: 9.8023 - val_mae: 2.3391\n",
      "Epoch 98/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3036 - mae: 1.2719 - val_loss: 13.3199 - val_mae: 2.5395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4388 - mae: 1.2476 - val_loss: 12.0640 - val_mae: 2.4474\n",
      "Epoch 100/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5540 - mae: 1.3324 - val_loss: 13.6711 - val_mae: 2.6233\n",
      "Epoch 101/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5648 - mae: 1.3374 - val_loss: 21.4129 - val_mae: 3.3407\n",
      "Epoch 102/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6827 - mae: 1.3148 - val_loss: 16.8491 - val_mae: 2.9555\n",
      "Epoch 103/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3470 - mae: 1.2365 - val_loss: 11.5883 - val_mae: 2.4363\n",
      "Epoch 104/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5581 - mae: 1.2618 - val_loss: 12.6364 - val_mae: 2.4358\n",
      "Epoch 105/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3431 - mae: 1.2496 - val_loss: 14.8024 - val_mae: 2.6639\n",
      "Epoch 106/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1861 - mae: 1.2751 - val_loss: 13.2326 - val_mae: 2.6969\n",
      "Epoch 107/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3852 - mae: 1.2348 - val_loss: 14.2148 - val_mae: 2.5722\n",
      "Epoch 108/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1713 - mae: 1.2472 - val_loss: 14.4701 - val_mae: 2.7318\n",
      "Epoch 109/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2017 - mae: 1.2521 - val_loss: 12.4447 - val_mae: 2.5828\n",
      "Epoch 110/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2090 - mae: 1.2097 - val_loss: 13.2602 - val_mae: 2.5981\n",
      "Epoch 111/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1085 - mae: 1.2331 - val_loss: 15.9585 - val_mae: 2.7371\n",
      "Epoch 112/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3328 - mae: 1.2562 - val_loss: 14.0075 - val_mae: 2.6498\n",
      "Epoch 113/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0589 - mae: 1.2189 - val_loss: 12.7361 - val_mae: 2.5731\n",
      "Epoch 114/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2090 - mae: 1.2434 - val_loss: 17.0858 - val_mae: 2.8639\n",
      "Epoch 115/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7414 - mae: 1.1741 - val_loss: 13.9489 - val_mae: 2.7703\n",
      "Epoch 116/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0007 - mae: 1.2336 - val_loss: 19.7683 - val_mae: 3.3654\n",
      "Epoch 117/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1394 - mae: 1.2357 - val_loss: 16.8960 - val_mae: 2.7569\n",
      "Epoch 118/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0310 - mae: 1.2642 - val_loss: 14.4736 - val_mae: 2.6692\n",
      "Epoch 119/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7857 - mae: 1.2248 - val_loss: 12.6311 - val_mae: 2.7087\n",
      "Epoch 120/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0405 - mae: 1.2556 - val_loss: 16.1156 - val_mae: 2.7988\n",
      "Epoch 121/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9193 - mae: 1.1965 - val_loss: 11.9984 - val_mae: 2.4286\n",
      "Epoch 122/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8210 - mae: 1.1868 - val_loss: 15.0425 - val_mae: 2.7105\n",
      "Epoch 123/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7792 - mae: 1.1704 - val_loss: 13.0866 - val_mae: 2.5419\n",
      "Epoch 124/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8036 - mae: 1.1885 - val_loss: 11.7511 - val_mae: 2.5178\n",
      "Epoch 125/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7512 - mae: 1.1605 - val_loss: 12.6549 - val_mae: 2.5183\n",
      "Epoch 126/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7311 - mae: 1.1508 - val_loss: 12.9465 - val_mae: 2.4997\n",
      "Epoch 127/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9380 - mae: 1.1394 - val_loss: 14.5440 - val_mae: 2.7038\n",
      "Epoch 128/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7746 - mae: 1.1295 - val_loss: 14.3643 - val_mae: 2.7195\n",
      "Epoch 129/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7201 - mae: 1.1606 - val_loss: 17.3264 - val_mae: 2.8464\n",
      "Epoch 130/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5363 - mae: 1.1538 - val_loss: 18.5767 - val_mae: 3.0212\n",
      "Epoch 131/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6151 - mae: 1.1536 - val_loss: 12.6382 - val_mae: 2.5161\n",
      "Epoch 132/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6857 - mae: 1.1418 - val_loss: 24.4084 - val_mae: 3.3106\n",
      "Epoch 133/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8329 - mae: 1.1662 - val_loss: 13.0094 - val_mae: 2.5772\n",
      "Epoch 134/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5337 - mae: 1.1309 - val_loss: 15.5824 - val_mae: 2.7867\n",
      "Epoch 135/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5245 - mae: 1.1135 - val_loss: 13.8605 - val_mae: 2.6432\n",
      "Epoch 136/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6366 - mae: 1.1491 - val_loss: 14.6313 - val_mae: 2.6606\n",
      "Epoch 137/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4705 - mae: 1.1663 - val_loss: 17.9806 - val_mae: 2.8288\n",
      "Epoch 138/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2540 - mae: 1.0845 - val_loss: 15.2256 - val_mae: 2.7335\n",
      "Epoch 139/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4802 - mae: 1.1286 - val_loss: 15.5239 - val_mae: 2.8356\n",
      "Epoch 140/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4318 - mae: 1.0943 - val_loss: 16.6505 - val_mae: 2.8324\n",
      "Epoch 141/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3634 - mae: 1.0886 - val_loss: 14.3880 - val_mae: 2.5858\n",
      "Epoch 142/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2350 - mae: 1.0705 - val_loss: 13.7589 - val_mae: 2.5485\n",
      "Epoch 143/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5135 - mae: 1.1128 - val_loss: 14.9677 - val_mae: 2.6921\n",
      "Epoch 144/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5593 - mae: 1.1136 - val_loss: 14.1248 - val_mae: 2.5220\n",
      "Epoch 145/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3585 - mae: 1.1031 - val_loss: 14.3216 - val_mae: 2.6650\n",
      "Epoch 146/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3504 - mae: 1.0837 - val_loss: 12.9108 - val_mae: 2.5253\n",
      "Epoch 147/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1907 - mae: 1.0401 - val_loss: 14.9915 - val_mae: 2.7041\n",
      "Epoch 148/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5656 - mae: 1.1243 - val_loss: 15.5083 - val_mae: 2.9169\n",
      "Epoch 149/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2671 - mae: 1.0829 - val_loss: 14.2518 - val_mae: 2.6431\n",
      "Epoch 150/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0274 - mae: 1.0598 - val_loss: 21.4712 - val_mae: 2.9812\n",
      "Epoch 151/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3090 - mae: 1.0931 - val_loss: 12.9903 - val_mae: 2.4688\n",
      "Epoch 152/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1583 - mae: 1.0409 - val_loss: 16.3760 - val_mae: 2.7240\n",
      "Epoch 153/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1918 - mae: 1.0092 - val_loss: 20.7734 - val_mae: 2.7948\n",
      "Epoch 154/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1803 - mae: 1.0492 - val_loss: 18.7062 - val_mae: 2.7730\n",
      "Epoch 155/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2369 - mae: 1.0751 - val_loss: 20.9888 - val_mae: 3.0208\n",
      "Epoch 156/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1060 - mae: 1.0627 - val_loss: 20.8666 - val_mae: 3.0946\n",
      "Epoch 157/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2578 - mae: 1.0573 - val_loss: 15.9347 - val_mae: 2.7820\n",
      "Epoch 158/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2055 - mae: 1.0402 - val_loss: 18.3262 - val_mae: 2.8517\n",
      "Epoch 159/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0407 - mae: 1.0683 - val_loss: 13.3942 - val_mae: 2.6292\n",
      "Epoch 160/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3053 - mae: 1.0833 - val_loss: 13.3374 - val_mae: 2.5313\n",
      "Epoch 161/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9548 - mae: 1.0278 - val_loss: 24.1794 - val_mae: 3.1486\n",
      "Epoch 162/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2822 - mae: 1.0484 - val_loss: 15.2949 - val_mae: 2.6719\n",
      "Epoch 163/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9963 - mae: 1.0266 - val_loss: 16.8373 - val_mae: 2.8235\n",
      "Epoch 164/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9701 - mae: 1.0410 - val_loss: 15.1988 - val_mae: 2.6897\n",
      "Epoch 165/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0181 - mae: 1.0219 - val_loss: 16.5315 - val_mae: 2.7889\n",
      "Epoch 166/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9325 - mae: 1.0255 - val_loss: 17.1417 - val_mae: 2.8024\n",
      "Epoch 167/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1087 - mae: 1.0332 - val_loss: 14.8544 - val_mae: 2.6105\n",
      "Epoch 168/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8679 - mae: 1.0401 - val_loss: 18.5111 - val_mae: 2.9391\n",
      "Epoch 169/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8532 - mae: 0.9889 - val_loss: 21.6296 - val_mae: 3.1213\n",
      "Epoch 170/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6950 - mae: 0.9332 - val_loss: 12.6312 - val_mae: 2.5771\n",
      "Epoch 171/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0960 - mae: 1.0233 - val_loss: 19.6264 - val_mae: 2.9985\n",
      "Epoch 172/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9876 - mae: 1.0807 - val_loss: 14.8915 - val_mae: 2.6771\n",
      "Epoch 173/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8233 - mae: 0.9660 - val_loss: 14.8538 - val_mae: 2.7041\n",
      "Epoch 174/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9090 - mae: 0.9708 - val_loss: 23.4364 - val_mae: 3.0537\n",
      "Epoch 175/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9128 - mae: 1.0270 - val_loss: 14.0403 - val_mae: 2.6318\n",
      "Epoch 176/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7855 - mae: 0.9956 - val_loss: 22.2045 - val_mae: 2.9895\n",
      "Epoch 177/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8992 - mae: 1.0092 - val_loss: 20.7657 - val_mae: 3.1446\n",
      "Epoch 178/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1220 - mae: 1.0577 - val_loss: 14.1392 - val_mae: 2.5595\n",
      "Epoch 179/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0561 - mae: 0.9959 - val_loss: 15.6194 - val_mae: 2.6201\n",
      "Epoch 180/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7964 - mae: 0.9569 - val_loss: 19.7505 - val_mae: 2.8331\n",
      "Epoch 181/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8680 - mae: 1.0296 - val_loss: 16.3031 - val_mae: 2.6761\n",
      "Epoch 182/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7515 - mae: 0.9527 - val_loss: 19.8845 - val_mae: 2.8460\n",
      "Epoch 183/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8149 - mae: 0.9734 - val_loss: 15.2256 - val_mae: 2.7593\n",
      "Epoch 184/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8760 - mae: 1.0011 - val_loss: 19.1836 - val_mae: 2.8575\n",
      "Epoch 185/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7654 - mae: 0.9581 - val_loss: 18.5172 - val_mae: 2.9540\n",
      "Epoch 186/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8945 - mae: 0.9883 - val_loss: 19.5964 - val_mae: 2.9135\n",
      "Epoch 187/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7867 - mae: 0.9807 - val_loss: 13.9024 - val_mae: 2.6502\n",
      "Epoch 188/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7134 - mae: 0.9747 - val_loss: 16.8877 - val_mae: 2.9149\n",
      "Epoch 189/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9079 - mae: 1.0201 - val_loss: 19.7084 - val_mae: 2.9540\n",
      "Epoch 190/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7487 - mae: 0.9632 - val_loss: 17.1985 - val_mae: 2.8494\n",
      "Epoch 191/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8511 - mae: 0.9679 - val_loss: 19.5490 - val_mae: 2.7681\n",
      "Epoch 192/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7955 - mae: 0.9781 - val_loss: 13.6232 - val_mae: 2.5664\n",
      "Epoch 193/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6772 - mae: 0.9138 - val_loss: 21.5307 - val_mae: 2.9559\n",
      "Epoch 194/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7999 - mae: 0.9625 - val_loss: 16.3153 - val_mae: 2.8608\n",
      "Epoch 195/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7919 - mae: 0.9707 - val_loss: 16.5724 - val_mae: 2.7614\n",
      "Epoch 196/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6667 - mae: 0.9401 - val_loss: 21.2009 - val_mae: 3.0593\n",
      "Epoch 197/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6116 - mae: 0.9206 - val_loss: 17.2977 - val_mae: 2.8437\n",
      "Epoch 198/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6706 - mae: 0.9432 - val_loss: 24.0445 - val_mae: 3.0202\n",
      "Epoch 199/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6361 - mae: 0.9386 - val_loss: 21.3505 - val_mae: 2.9714\n",
      "Epoch 200/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6247 - mae: 0.9393 - val_loss: 19.4514 - val_mae: 2.9556\n",
      "Epoch 201/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7929 - mae: 0.9855 - val_loss: 18.2884 - val_mae: 2.8484\n",
      "Epoch 202/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7505 - mae: 0.9837 - val_loss: 20.4668 - val_mae: 2.9883\n",
      "Epoch 203/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5804 - mae: 0.9080 - val_loss: 15.8264 - val_mae: 2.5262\n",
      "Epoch 204/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5423 - mae: 0.9408 - val_loss: 14.5252 - val_mae: 2.5443\n",
      "Epoch 205/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6783 - mae: 0.9930 - val_loss: 19.2053 - val_mae: 2.9374\n",
      "Epoch 206/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7400 - mae: 0.9358 - val_loss: 18.1395 - val_mae: 2.8097\n",
      "Epoch 207/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6592 - mae: 0.9684 - val_loss: 19.5717 - val_mae: 2.8213\n",
      "Epoch 208/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7532 - mae: 0.9683 - val_loss: 18.0665 - val_mae: 2.8269\n",
      "Epoch 209/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6101 - mae: 0.9302 - val_loss: 16.7997 - val_mae: 2.8003\n",
      "Epoch 210/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5365 - mae: 0.8902 - val_loss: 25.8302 - val_mae: 3.3309\n",
      "Epoch 211/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5640 - mae: 0.9447 - val_loss: 19.8543 - val_mae: 2.9521\n",
      "Epoch 212/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8128 - mae: 0.9831 - val_loss: 13.5873 - val_mae: 2.6673\n",
      "Epoch 213/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7249 - mae: 0.9416 - val_loss: 17.8192 - val_mae: 3.0104\n",
      "Epoch 214/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6687 - mae: 0.9256 - val_loss: 20.1783 - val_mae: 3.2016\n",
      "Epoch 215/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6713 - mae: 0.9416 - val_loss: 18.9815 - val_mae: 2.8628\n",
      "Epoch 216/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7106 - mae: 0.9685 - val_loss: 22.5076 - val_mae: 3.0543\n",
      "Epoch 217/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5300 - mae: 0.8931 - val_loss: 20.6166 - val_mae: 3.0041\n",
      "Epoch 218/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6537 - mae: 0.9302 - val_loss: 23.9859 - val_mae: 3.2035\n",
      "Epoch 219/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5632 - mae: 0.9546 - val_loss: 24.2063 - val_mae: 3.1938\n",
      "Epoch 220/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4967 - mae: 0.8907 - val_loss: 21.4065 - val_mae: 3.1251\n",
      "Epoch 221/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7304 - mae: 0.9472 - val_loss: 18.4146 - val_mae: 2.8710\n",
      "Epoch 222/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4954 - mae: 0.9187 - val_loss: 19.2847 - val_mae: 2.8633\n",
      "Epoch 223/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4982 - mae: 0.8673 - val_loss: 19.9004 - val_mae: 2.9483\n",
      "Epoch 224/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4977 - mae: 0.8916 - val_loss: 20.3308 - val_mae: 2.9945\n",
      "Epoch 225/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4343 - mae: 0.8762 - val_loss: 19.4718 - val_mae: 2.8523\n",
      "Epoch 226/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6883 - mae: 0.9368 - val_loss: 22.4039 - val_mae: 2.9770\n",
      "Epoch 227/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5608 - mae: 0.9221 - val_loss: 20.8212 - val_mae: 2.9240\n",
      "Epoch 228/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6046 - mae: 0.9399 - val_loss: 15.8030 - val_mae: 2.7618\n",
      "Epoch 229/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5921 - mae: 0.9133 - val_loss: 20.2047 - val_mae: 3.0453\n",
      "Epoch 230/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5017 - mae: 0.9100 - val_loss: 23.2050 - val_mae: 3.1710\n",
      "Epoch 231/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5060 - mae: 0.8892 - val_loss: 18.7078 - val_mae: 2.8586\n",
      "Epoch 232/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5095 - mae: 0.8908 - val_loss: 17.6248 - val_mae: 2.7945\n",
      "Epoch 233/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5973 - mae: 0.9040 - val_loss: 14.5890 - val_mae: 2.5780\n",
      "Epoch 234/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5091 - mae: 0.8857 - val_loss: 23.1898 - val_mae: 3.0519\n",
      "Epoch 235/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5446 - mae: 0.8368 - val_loss: 12.9477 - val_mae: 2.6091\n",
      "Epoch 236/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4828 - mae: 0.8757 - val_loss: 21.4398 - val_mae: 3.0557\n",
      "Epoch 237/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3756 - mae: 0.8726 - val_loss: 14.8896 - val_mae: 2.6952\n",
      "Epoch 238/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4751 - mae: 0.8797 - val_loss: 17.2682 - val_mae: 2.8703\n",
      "Epoch 239/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4045 - mae: 0.8447 - val_loss: 16.4663 - val_mae: 2.7916\n",
      "Epoch 240/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4652 - mae: 0.8846 - val_loss: 18.9270 - val_mae: 2.9078\n",
      "Epoch 241/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3869 - mae: 0.8878 - val_loss: 19.5614 - val_mae: 2.8310\n",
      "Epoch 242/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4084 - mae: 0.8950 - val_loss: 16.0367 - val_mae: 2.7914\n",
      "Epoch 243/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5757 - mae: 0.9104 - val_loss: 16.4930 - val_mae: 2.7162\n",
      "Epoch 244/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4399 - mae: 0.8729 - val_loss: 22.8291 - val_mae: 3.0160\n",
      "Epoch 245/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4469 - mae: 0.8604 - val_loss: 17.4549 - val_mae: 2.8802\n",
      "Epoch 246/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3747 - mae: 0.8533 - val_loss: 17.6993 - val_mae: 2.8768\n",
      "Epoch 247/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4853 - mae: 0.9064 - val_loss: 16.7397 - val_mae: 2.7458\n",
      "Epoch 248/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4038 - mae: 0.8619 - val_loss: 20.9117 - val_mae: 3.0551\n",
      "Epoch 249/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5067 - mae: 0.8852 - val_loss: 16.8996 - val_mae: 2.9761\n",
      "Epoch 250/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3610 - mae: 0.8718 - val_loss: 16.1926 - val_mae: 2.7615\n",
      "Epoch 251/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4451 - mae: 0.8879 - val_loss: 17.2254 - val_mae: 2.8333\n",
      "Epoch 252/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5395 - mae: 0.8852 - val_loss: 17.7541 - val_mae: 2.8698\n",
      "Epoch 253/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3136 - mae: 0.8278 - val_loss: 18.8540 - val_mae: 2.9402\n",
      "Epoch 254/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3772 - mae: 0.8358 - val_loss: 15.6060 - val_mae: 2.7060\n",
      "Epoch 255/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4243 - mae: 0.8486 - val_loss: 18.7699 - val_mae: 2.9564\n",
      "Epoch 256/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4450 - mae: 0.8606 - val_loss: 20.8027 - val_mae: 2.9600\n",
      "Epoch 257/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2327 - mae: 0.8214 - val_loss: 19.1972 - val_mae: 3.0151\n",
      "Epoch 258/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5131 - mae: 0.8734 - val_loss: 18.1950 - val_mae: 2.9231\n",
      "Epoch 259/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3819 - mae: 0.8534 - val_loss: 20.3322 - val_mae: 3.1382\n",
      "Epoch 260/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4500 - mae: 0.8395 - val_loss: 17.9431 - val_mae: 2.8595\n",
      "Epoch 261/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3365 - mae: 0.8638 - val_loss: 18.1432 - val_mae: 2.7918\n",
      "Epoch 262/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3938 - mae: 0.8326 - val_loss: 20.0707 - val_mae: 2.9360\n",
      "Epoch 263/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2694 - mae: 0.8092 - val_loss: 19.8929 - val_mae: 2.9730\n",
      "Epoch 264/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2350 - mae: 0.8299 - val_loss: 17.4127 - val_mae: 2.8582\n",
      "Epoch 265/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3854 - mae: 0.9008 - val_loss: 17.6311 - val_mae: 2.7457\n",
      "Epoch 266/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3040 - mae: 0.8413 - val_loss: 15.7785 - val_mae: 2.7622\n",
      "Epoch 267/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4560 - mae: 0.8899 - val_loss: 18.7260 - val_mae: 2.9410\n",
      "Epoch 268/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3365 - mae: 0.8584 - val_loss: 21.1512 - val_mae: 3.2158\n",
      "Epoch 269/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2936 - mae: 0.8547 - val_loss: 15.9682 - val_mae: 2.8101\n",
      "Epoch 270/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3825 - mae: 0.8627 - val_loss: 17.3976 - val_mae: 2.7857\n",
      "Epoch 271/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3411 - mae: 0.8535 - val_loss: 21.5474 - val_mae: 2.9870\n",
      "Epoch 272/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1835 - mae: 0.8068 - val_loss: 21.4436 - val_mae: 2.9034\n",
      "Epoch 273/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7128 - mae: 0.8969 - val_loss: 20.8165 - val_mae: 2.9269\n",
      "Epoch 274/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2497 - mae: 0.8301 - val_loss: 26.7721 - val_mae: 3.1446\n",
      "Epoch 275/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4256 - mae: 0.8767 - val_loss: 24.7570 - val_mae: 3.2131\n",
      "Epoch 276/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2666 - mae: 0.8274 - val_loss: 16.6003 - val_mae: 2.7971\n",
      "Epoch 277/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5599 - mae: 0.8938 - val_loss: 15.5284 - val_mae: 2.6027\n",
      "Epoch 278/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2596 - mae: 0.8361 - val_loss: 24.4997 - val_mae: 3.1227\n",
      "Epoch 279/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1530 - mae: 0.8036 - val_loss: 17.0910 - val_mae: 2.8559\n",
      "Epoch 280/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4065 - mae: 0.8868 - val_loss: 31.3206 - val_mae: 3.6064\n",
      "Epoch 281/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3501 - mae: 0.8418 - val_loss: 19.8343 - val_mae: 2.9862\n",
      "Epoch 282/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3792 - mae: 0.8284 - val_loss: 19.2415 - val_mae: 2.9179\n",
      "Epoch 283/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3466 - mae: 0.8497 - val_loss: 18.9660 - val_mae: 2.9315\n",
      "Epoch 284/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3335 - mae: 0.8341 - val_loss: 15.4106 - val_mae: 2.7375\n",
      "Epoch 285/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3000 - mae: 0.8565 - val_loss: 17.3762 - val_mae: 2.9194\n",
      "Epoch 286/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4676 - mae: 0.8408 - val_loss: 19.6985 - val_mae: 2.9258\n",
      "Epoch 287/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1257 - mae: 0.7790 - val_loss: 17.0211 - val_mae: 2.8022\n",
      "Epoch 288/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2437 - mae: 0.8045 - val_loss: 18.3473 - val_mae: 3.0321\n",
      "Epoch 289/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5025 - mae: 0.8818 - val_loss: 19.6079 - val_mae: 3.0421\n",
      "Epoch 290/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3379 - mae: 0.8533 - val_loss: 15.5509 - val_mae: 2.8623\n",
      "Epoch 291/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2601 - mae: 0.8374 - val_loss: 16.6813 - val_mae: 2.7945\n",
      "Epoch 292/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1148 - mae: 0.8045 - val_loss: 21.8703 - val_mae: 2.9040\n",
      "Epoch 293/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0979 - mae: 0.7781 - val_loss: 21.7645 - val_mae: 3.0305\n",
      "Epoch 294/500\n",
      "303/303 [==============================] - ETA: 0s - loss: 1.3045 - mae: 0.866 - 1s 2ms/step - loss: 1.3557 - mae: 0.8814 - val_loss: 14.2495 - val_mae: 2.6779\n",
      "Epoch 295/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2789 - mae: 0.8313 - val_loss: 16.1573 - val_mae: 2.7880\n",
      "Epoch 296/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1774 - mae: 0.7967 - val_loss: 17.0709 - val_mae: 2.8104\n",
      "Epoch 297/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1777 - mae: 0.7964 - val_loss: 29.3802 - val_mae: 3.3668\n",
      "Epoch 298/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2119 - mae: 0.8109 - val_loss: 14.3091 - val_mae: 2.6522\n",
      "Epoch 299/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4395 - mae: 0.8858 - val_loss: 22.8877 - val_mae: 3.2713\n",
      "Epoch 300/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4775 - mae: 0.8624 - val_loss: 17.8615 - val_mae: 2.8212\n",
      "Epoch 301/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2630 - mae: 0.8383 - val_loss: 23.5102 - val_mae: 3.0485\n",
      "Epoch 302/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2727 - mae: 0.8293 - val_loss: 16.6424 - val_mae: 2.7673\n",
      "Epoch 303/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3289 - mae: 0.8407 - val_loss: 16.3411 - val_mae: 2.7423\n",
      "Epoch 304/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3084 - mae: 0.8432 - val_loss: 22.7096 - val_mae: 3.0149\n",
      "Epoch 305/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3451 - mae: 0.8478 - val_loss: 23.3898 - val_mae: 3.1304\n",
      "Epoch 306/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2499 - mae: 0.8020 - val_loss: 19.5454 - val_mae: 2.9667\n",
      "Epoch 307/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2382 - mae: 0.8172 - val_loss: 19.6236 - val_mae: 2.8361\n",
      "Epoch 308/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3614 - mae: 0.8607 - val_loss: 15.5481 - val_mae: 2.5904\n",
      "Epoch 309/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2689 - mae: 0.7964 - val_loss: 18.8758 - val_mae: 2.9663\n",
      "Epoch 310/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1479 - mae: 0.7828 - val_loss: 18.8945 - val_mae: 2.7119\n",
      "Epoch 311/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3388 - mae: 0.8018 - val_loss: 18.6050 - val_mae: 2.7667\n",
      "Epoch 312/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0800 - mae: 0.7645 - val_loss: 22.6095 - val_mae: 2.8599\n",
      "Epoch 313/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3876 - mae: 0.8850 - val_loss: 20.0230 - val_mae: 2.8112\n",
      "Epoch 314/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2123 - mae: 0.7971 - val_loss: 15.7166 - val_mae: 2.6469\n",
      "Epoch 315/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1716 - mae: 0.7658 - val_loss: 17.4640 - val_mae: 2.7163\n",
      "Epoch 316/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1818 - mae: 0.8126 - val_loss: 19.7677 - val_mae: 3.0386\n",
      "Epoch 317/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1723 - mae: 0.8147 - val_loss: 21.2668 - val_mae: 2.9064\n",
      "Epoch 318/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1280 - mae: 0.7641 - val_loss: 17.0482 - val_mae: 2.6709\n",
      "Epoch 319/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2381 - mae: 0.8312 - val_loss: 24.3889 - val_mae: 3.1193\n",
      "Epoch 320/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1808 - mae: 0.8027 - val_loss: 14.3758 - val_mae: 2.6113\n",
      "Epoch 321/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2697 - mae: 0.7824 - val_loss: 21.0305 - val_mae: 3.0022\n",
      "Epoch 322/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2297 - mae: 0.8084 - val_loss: 15.7860 - val_mae: 2.6850\n",
      "Epoch 323/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1104 - mae: 0.7847 - val_loss: 17.1864 - val_mae: 2.8640\n",
      "Epoch 324/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2027 - mae: 0.8165 - val_loss: 18.3675 - val_mae: 2.7314\n",
      "Epoch 325/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1338 - mae: 0.7754 - val_loss: 20.9259 - val_mae: 2.9948\n",
      "Epoch 326/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0783 - mae: 0.7411 - val_loss: 17.1758 - val_mae: 2.7625\n",
      "Epoch 327/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2407 - mae: 0.8160 - val_loss: 16.7544 - val_mae: 2.7375\n",
      "Epoch 328/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2342 - mae: 0.7967 - val_loss: 16.6526 - val_mae: 2.8854\n",
      "Epoch 329/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1170 - mae: 0.7677 - val_loss: 17.6697 - val_mae: 2.8579\n",
      "Epoch 330/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1072 - mae: 0.7647 - val_loss: 19.5717 - val_mae: 2.8660\n",
      "Epoch 331/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2082 - mae: 0.8029 - val_loss: 20.7625 - val_mae: 3.0180\n",
      "Epoch 332/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0631 - mae: 0.7629 - val_loss: 27.2264 - val_mae: 3.2278\n",
      "Epoch 333/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1419 - mae: 0.7994 - val_loss: 13.1076 - val_mae: 2.5964\n",
      "Epoch 334/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1137 - mae: 0.7869 - val_loss: 19.7704 - val_mae: 2.8272\n",
      "Epoch 335/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3031 - mae: 0.8245 - val_loss: 21.7831 - val_mae: 3.0971\n",
      "Epoch 336/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2052 - mae: 0.8015 - val_loss: 21.3089 - val_mae: 2.9219\n",
      "Epoch 337/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1893 - mae: 0.7896 - val_loss: 16.3167 - val_mae: 2.6852\n",
      "Epoch 338/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0407 - mae: 0.7333 - val_loss: 19.7403 - val_mae: 2.8954\n",
      "Epoch 339/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0671 - mae: 0.7616 - val_loss: 22.9551 - val_mae: 3.2649\n",
      "Epoch 340/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0761 - mae: 0.7726 - val_loss: 19.0385 - val_mae: 2.8876\n",
      "Epoch 341/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1107 - mae: 0.7626 - val_loss: 22.6647 - val_mae: 2.9074\n",
      "Epoch 342/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9931 - mae: 0.7541 - val_loss: 20.7487 - val_mae: 3.0085\n",
      "Epoch 343/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1407 - mae: 0.8118 - val_loss: 21.7998 - val_mae: 2.8434\n",
      "Epoch 344/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1619 - mae: 0.7891 - val_loss: 15.9656 - val_mae: 2.6383\n",
      "Epoch 345/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0774 - mae: 0.7598 - val_loss: 15.9559 - val_mae: 2.7372\n",
      "Epoch 346/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0721 - mae: 0.7423 - val_loss: 21.2028 - val_mae: 3.1530\n",
      "Epoch 347/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2928 - mae: 0.8029 - val_loss: 17.6289 - val_mae: 2.7594\n",
      "Epoch 348/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9725 - mae: 0.7533 - val_loss: 19.5103 - val_mae: 2.9578\n",
      "Epoch 349/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1751 - mae: 0.7728 - val_loss: 19.4342 - val_mae: 2.8745\n",
      "Epoch 350/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1237 - mae: 0.7783 - val_loss: 16.5021 - val_mae: 2.7602\n",
      "Epoch 351/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0565 - mae: 0.7633 - val_loss: 18.1319 - val_mae: 2.7789\n",
      "Epoch 352/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2315 - mae: 0.7372 - val_loss: 14.7283 - val_mae: 2.5833\n",
      "Epoch 353/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1064 - mae: 0.7523 - val_loss: 15.1679 - val_mae: 2.6120\n",
      "Epoch 354/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1132 - mae: 0.7665 - val_loss: 22.2773 - val_mae: 2.9141\n",
      "Epoch 355/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1261 - mae: 0.7944 - val_loss: 20.3487 - val_mae: 2.9869\n",
      "Epoch 356/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0824 - mae: 0.7679 - val_loss: 19.7264 - val_mae: 2.8717\n",
      "Epoch 357/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4298 - mae: 0.7878 - val_loss: 19.8189 - val_mae: 2.8053\n",
      "Epoch 358/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0468 - mae: 0.7570 - val_loss: 22.6080 - val_mae: 3.1191\n",
      "Epoch 359/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0658 - mae: 0.7600 - val_loss: 18.1678 - val_mae: 2.9094\n",
      "Epoch 360/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1614 - mae: 0.7894 - val_loss: 15.1048 - val_mae: 2.7157\n",
      "Epoch 361/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0568 - mae: 0.7608 - val_loss: 15.5539 - val_mae: 2.7728\n",
      "Epoch 362/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0072 - mae: 0.7518 - val_loss: 18.0291 - val_mae: 2.8364\n",
      "Epoch 363/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9996 - mae: 0.7668 - val_loss: 17.3132 - val_mae: 2.8872\n",
      "Epoch 364/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1372 - mae: 0.7501 - val_loss: 16.1129 - val_mae: 2.7940\n",
      "Epoch 365/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0382 - mae: 0.7784 - val_loss: 16.2257 - val_mae: 2.6544\n",
      "Epoch 366/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9044 - mae: 0.7067 - val_loss: 13.4801 - val_mae: 2.4787\n",
      "Epoch 367/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1534 - mae: 0.7770 - val_loss: 18.3217 - val_mae: 2.9260\n",
      "Epoch 368/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0339 - mae: 0.7401 - val_loss: 21.5229 - val_mae: 3.1219\n",
      "Epoch 369/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0360 - mae: 0.7220 - val_loss: 15.6274 - val_mae: 2.6135\n",
      "Epoch 370/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1449 - mae: 0.7625 - val_loss: 16.4968 - val_mae: 2.7077\n",
      "Epoch 371/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0517 - mae: 0.7393 - val_loss: 20.2454 - val_mae: 2.8764\n",
      "Epoch 372/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0258 - mae: 0.7616 - val_loss: 21.8635 - val_mae: 2.9166\n",
      "Epoch 373/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0042 - mae: 0.7539 - val_loss: 14.9939 - val_mae: 2.6890\n",
      "Epoch 374/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9931 - mae: 0.7264 - val_loss: 14.8602 - val_mae: 2.6037\n",
      "Epoch 375/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9473 - mae: 0.7162 - val_loss: 23.8920 - val_mae: 3.0946\n",
      "Epoch 376/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0980 - mae: 0.7868 - val_loss: 18.6007 - val_mae: 2.8498\n",
      "Epoch 377/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0757 - mae: 0.7437 - val_loss: 20.6015 - val_mae: 2.9323\n",
      "Epoch 378/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8878 - mae: 0.7302 - val_loss: 18.2745 - val_mae: 2.9407\n",
      "Epoch 379/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9720 - mae: 0.7289 - val_loss: 17.4448 - val_mae: 2.6414\n",
      "Epoch 380/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9811 - mae: 0.7533 - val_loss: 26.3285 - val_mae: 3.2622\n",
      "Epoch 381/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0772 - mae: 0.7477 - val_loss: 14.8694 - val_mae: 2.5385\n",
      "Epoch 382/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0531 - mae: 0.7588 - val_loss: 20.3460 - val_mae: 2.9207\n",
      "Epoch 383/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9438 - mae: 0.7267 - val_loss: 15.1389 - val_mae: 2.6647\n",
      "Epoch 384/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9914 - mae: 0.7386 - val_loss: 15.1458 - val_mae: 2.6179\n",
      "Epoch 385/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0236 - mae: 0.7522 - val_loss: 18.1314 - val_mae: 2.8406\n",
      "Epoch 386/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1086 - mae: 0.7229 - val_loss: 18.5184 - val_mae: 2.9051\n",
      "Epoch 387/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0622 - mae: 0.7481 - val_loss: 15.4003 - val_mae: 2.6042\n",
      "Epoch 388/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9976 - mae: 0.7199 - val_loss: 18.9013 - val_mae: 2.8539\n",
      "Epoch 389/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0202 - mae: 0.7491 - val_loss: 16.2978 - val_mae: 2.6731\n",
      "Epoch 390/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0137 - mae: 0.7507 - val_loss: 15.2042 - val_mae: 2.6542\n",
      "Epoch 391/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1161 - mae: 0.7676 - val_loss: 15.0839 - val_mae: 2.6104\n",
      "Epoch 392/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9826 - mae: 0.7387 - val_loss: 15.7322 - val_mae: 2.7355\n",
      "Epoch 393/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0345 - mae: 0.7107 - val_loss: 16.1883 - val_mae: 2.6978\n",
      "Epoch 394/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1780 - mae: 0.7618 - val_loss: 14.9011 - val_mae: 2.6581\n",
      "Epoch 395/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0955 - mae: 0.7142 - val_loss: 19.9606 - val_mae: 2.8186\n",
      "Epoch 396/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9656 - mae: 0.7225 - val_loss: 17.0462 - val_mae: 2.8223\n",
      "Epoch 397/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9932 - mae: 0.7046 - val_loss: 17.9379 - val_mae: 2.8396\n",
      "Epoch 398/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0042 - mae: 0.7327 - val_loss: 15.2769 - val_mae: 2.6143\n",
      "Epoch 399/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9585 - mae: 0.7195 - val_loss: 17.1678 - val_mae: 2.6929\n",
      "Epoch 400/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9278 - mae: 0.7098 - val_loss: 19.0297 - val_mae: 2.9141\n",
      "Epoch 401/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0184 - mae: 0.7576 - val_loss: 19.1566 - val_mae: 2.7706\n",
      "Epoch 402/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9482 - mae: 0.7157 - val_loss: 16.2900 - val_mae: 2.7685\n",
      "Epoch 403/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0666 - mae: 0.7501 - val_loss: 18.2476 - val_mae: 2.9333\n",
      "Epoch 404/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9366 - mae: 0.7085 - val_loss: 14.9302 - val_mae: 2.6417\n",
      "Epoch 405/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9880 - mae: 0.7273 - val_loss: 24.7720 - val_mae: 3.1158\n",
      "Epoch 406/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9416 - mae: 0.7415 - val_loss: 16.1980 - val_mae: 2.6646\n",
      "Epoch 407/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0236 - mae: 0.7217 - val_loss: 18.3629 - val_mae: 2.9097\n",
      "Epoch 408/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9371 - mae: 0.6914 - val_loss: 17.4475 - val_mae: 2.7238\n",
      "Epoch 409/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0182 - mae: 0.7417 - val_loss: 16.6009 - val_mae: 2.6880\n",
      "Epoch 410/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0359 - mae: 0.7539 - val_loss: 17.6145 - val_mae: 2.9305\n",
      "Epoch 411/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0127 - mae: 0.7168 - val_loss: 19.5287 - val_mae: 2.8491\n",
      "Epoch 412/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0668 - mae: 0.7479 - val_loss: 20.1776 - val_mae: 3.0042\n",
      "Epoch 413/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9339 - mae: 0.7268 - val_loss: 18.3998 - val_mae: 2.8542\n",
      "Epoch 414/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9420 - mae: 0.7021 - val_loss: 19.4455 - val_mae: 2.8539\n",
      "Epoch 415/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9173 - mae: 0.7027 - val_loss: 21.0541 - val_mae: 3.0554\n",
      "Epoch 416/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9499 - mae: 0.7001 - val_loss: 17.4424 - val_mae: 2.9052\n",
      "Epoch 417/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0104 - mae: 0.7292 - val_loss: 14.8607 - val_mae: 2.6051\n",
      "Epoch 418/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9916 - mae: 0.7312 - val_loss: 20.4987 - val_mae: 2.8173\n",
      "Epoch 419/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9881 - mae: 0.7272 - val_loss: 19.0501 - val_mae: 2.9414\n",
      "Epoch 420/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0295 - mae: 0.7214 - val_loss: 19.3736 - val_mae: 2.8326\n",
      "Epoch 421/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0124 - mae: 0.7071 - val_loss: 21.6062 - val_mae: 2.9580\n",
      "Epoch 422/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9415 - mae: 0.6974 - val_loss: 21.5494 - val_mae: 3.0665\n",
      "Epoch 423/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1081 - mae: 0.7260 - val_loss: 16.0305 - val_mae: 2.7647\n",
      "Epoch 424/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0214 - mae: 0.7259 - val_loss: 14.9957 - val_mae: 2.5891\n",
      "Epoch 425/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8552 - mae: 0.6978 - val_loss: 14.8107 - val_mae: 2.6523\n",
      "Epoch 426/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9420 - mae: 0.7182 - val_loss: 23.8833 - val_mae: 3.1652\n",
      "Epoch 427/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9756 - mae: 0.6999 - val_loss: 14.0458 - val_mae: 2.5195\n",
      "Epoch 428/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8680 - mae: 0.6872 - val_loss: 19.7391 - val_mae: 2.8009\n",
      "Epoch 429/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0379 - mae: 0.7490 - val_loss: 20.2479 - val_mae: 2.9249\n",
      "Epoch 430/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8673 - mae: 0.6813 - val_loss: 17.3349 - val_mae: 2.7280\n",
      "Epoch 431/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8853 - mae: 0.6902 - val_loss: 16.0450 - val_mae: 2.7526\n",
      "Epoch 432/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9588 - mae: 0.7250 - val_loss: 16.9027 - val_mae: 2.7043\n",
      "Epoch 433/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8875 - mae: 0.6863 - val_loss: 16.4451 - val_mae: 2.6732\n",
      "Epoch 434/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9532 - mae: 0.6865 - val_loss: 25.1623 - val_mae: 3.0515\n",
      "Epoch 435/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8334 - mae: 0.6784 - val_loss: 20.7185 - val_mae: 2.8914\n",
      "Epoch 436/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9648 - mae: 0.7200 - val_loss: 16.7157 - val_mae: 2.6970\n",
      "Epoch 437/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8264 - mae: 0.6687 - val_loss: 24.4908 - val_mae: 3.1052\n",
      "Epoch 438/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0271 - mae: 0.7140 - val_loss: 19.6904 - val_mae: 2.8103\n",
      "Epoch 439/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0088 - mae: 0.6754 - val_loss: 21.3910 - val_mae: 2.8597\n",
      "Epoch 440/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8540 - mae: 0.6979 - val_loss: 21.0878 - val_mae: 2.9416\n",
      "Epoch 441/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0093 - mae: 0.7272 - val_loss: 20.1357 - val_mae: 3.0466\n",
      "Epoch 442/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7943 - mae: 0.6725 - val_loss: 14.8487 - val_mae: 2.6534\n",
      "Epoch 443/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9678 - mae: 0.7110 - val_loss: 16.4501 - val_mae: 2.7953\n",
      "Epoch 444/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9207 - mae: 0.6768 - val_loss: 18.0055 - val_mae: 2.8426\n",
      "Epoch 445/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9869 - mae: 0.7238 - val_loss: 22.7932 - val_mae: 3.1585\n",
      "Epoch 446/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9510 - mae: 0.7209 - val_loss: 18.7701 - val_mae: 2.8004\n",
      "Epoch 447/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8170 - mae: 0.6583 - val_loss: 19.1158 - val_mae: 2.9629\n",
      "Epoch 448/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0220 - mae: 0.7054 - val_loss: 21.1386 - val_mae: 2.9150\n",
      "Epoch 449/500\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.9051 - mae: 0.682 - 1s 2ms/step - loss: 0.8945 - mae: 0.6793 - val_loss: 22.3716 - val_mae: 2.9427\n",
      "Epoch 450/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8659 - mae: 0.6611 - val_loss: 18.2406 - val_mae: 2.7087\n",
      "Epoch 451/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8734 - mae: 0.6604 - val_loss: 17.4696 - val_mae: 2.7009\n",
      "Epoch 452/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9031 - mae: 0.7193 - val_loss: 18.6483 - val_mae: 2.8351\n",
      "Epoch 453/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9828 - mae: 0.7094 - val_loss: 18.7896 - val_mae: 2.8270\n",
      "Epoch 454/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8499 - mae: 0.7092 - val_loss: 19.8099 - val_mae: 2.9015\n",
      "Epoch 455/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9330 - mae: 0.7197 - val_loss: 19.4059 - val_mae: 2.8317\n",
      "Epoch 456/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9039 - mae: 0.6986 - val_loss: 19.8408 - val_mae: 2.9224\n",
      "Epoch 457/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8829 - mae: 0.7047 - val_loss: 22.2177 - val_mae: 2.9647\n",
      "Epoch 458/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8357 - mae: 0.6705 - val_loss: 26.5155 - val_mae: 3.2080\n",
      "Epoch 459/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8675 - mae: 0.6957 - val_loss: 18.2456 - val_mae: 2.7193\n",
      "Epoch 460/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7750 - mae: 0.6446 - val_loss: 21.4438 - val_mae: 3.0162\n",
      "Epoch 461/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0198 - mae: 0.7390 - val_loss: 16.8348 - val_mae: 2.7837\n",
      "Epoch 462/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8863 - mae: 0.7051 - val_loss: 19.9948 - val_mae: 2.8394\n",
      "Epoch 463/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8882 - mae: 0.6809 - val_loss: 20.2908 - val_mae: 2.9319\n",
      "Epoch 464/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8963 - mae: 0.6580 - val_loss: 19.4780 - val_mae: 2.9409\n",
      "Epoch 465/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7878 - mae: 0.6766 - val_loss: 24.5899 - val_mae: 3.1237\n",
      "Epoch 466/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8905 - mae: 0.7133 - val_loss: 17.7813 - val_mae: 2.7235\n",
      "Epoch 467/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9027 - mae: 0.6866 - val_loss: 24.6410 - val_mae: 3.1412\n",
      "Epoch 468/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9831 - mae: 0.7260 - val_loss: 20.8728 - val_mae: 3.0435\n",
      "Epoch 469/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7883 - mae: 0.6697 - val_loss: 18.5098 - val_mae: 2.8362\n",
      "Epoch 470/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9814 - mae: 0.7229 - val_loss: 25.9038 - val_mae: 3.1123\n",
      "Epoch 471/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8581 - mae: 0.6558 - val_loss: 20.8273 - val_mae: 2.9063\n",
      "Epoch 472/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8469 - mae: 0.6836 - val_loss: 20.3866 - val_mae: 2.8289\n",
      "Epoch 473/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8992 - mae: 0.6729 - val_loss: 18.5684 - val_mae: 2.8065\n",
      "Epoch 474/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8971 - mae: 0.7022 - val_loss: 24.6794 - val_mae: 3.0798\n",
      "Epoch 475/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8746 - mae: 0.6855 - val_loss: 23.0986 - val_mae: 3.1639\n",
      "Epoch 476/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9494 - mae: 0.7186 - val_loss: 19.2367 - val_mae: 2.9567\n",
      "Epoch 477/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9221 - mae: 0.7025 - val_loss: 18.5468 - val_mae: 2.7949\n",
      "Epoch 478/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8148 - mae: 0.6824 - val_loss: 21.2144 - val_mae: 3.0161\n",
      "Epoch 479/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8808 - mae: 0.6878 - val_loss: 18.8375 - val_mae: 2.7695\n",
      "Epoch 480/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9828 - mae: 0.6855 - val_loss: 18.8611 - val_mae: 2.7344\n",
      "Epoch 481/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7688 - mae: 0.6474 - val_loss: 22.1242 - val_mae: 3.2308\n",
      "Epoch 482/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9317 - mae: 0.6898 - val_loss: 18.8917 - val_mae: 2.8645\n",
      "Epoch 483/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8412 - mae: 0.6689 - val_loss: 18.0924 - val_mae: 2.7550\n",
      "Epoch 484/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8466 - mae: 0.6809 - val_loss: 21.7708 - val_mae: 3.1169\n",
      "Epoch 485/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8116 - mae: 0.6799 - val_loss: 18.7583 - val_mae: 2.7925\n",
      "Epoch 486/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7673 - mae: 0.6532 - val_loss: 18.3460 - val_mae: 2.9722\n",
      "Epoch 487/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8571 - mae: 0.6964 - val_loss: 20.4805 - val_mae: 2.9437\n",
      "Epoch 488/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8592 - mae: 0.6973 - val_loss: 18.7815 - val_mae: 2.8626\n",
      "Epoch 489/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7852 - mae: 0.6648 - val_loss: 17.2602 - val_mae: 2.7376\n",
      "Epoch 490/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8639 - mae: 0.6770 - val_loss: 22.2106 - val_mae: 2.9866\n",
      "Epoch 491/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8101 - mae: 0.6579 - val_loss: 17.0638 - val_mae: 2.6875\n",
      "Epoch 492/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0098 - mae: 0.7206 - val_loss: 19.2486 - val_mae: 2.8033\n",
      "Epoch 493/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7831 - mae: 0.6313 - val_loss: 20.3647 - val_mae: 2.8730\n",
      "Epoch 494/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8265 - mae: 0.6759 - val_loss: 15.4526 - val_mae: 2.6671\n",
      "Epoch 495/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8113 - mae: 0.6595 - val_loss: 23.2048 - val_mae: 3.0157\n",
      "Epoch 496/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6622 - mae: 0.6239 - val_loss: 19.7569 - val_mae: 2.8954\n",
      "Epoch 497/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8344 - mae: 0.6573 - val_loss: 19.8843 - val_mae: 2.9266\n",
      "Epoch 498/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8168 - mae: 0.6887 - val_loss: 17.3450 - val_mae: 2.8595\n",
      "Epoch 499/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8561 - mae: 0.6891 - val_loss: 23.6434 - val_mae: 3.0853\n",
      "Epoch 500/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7000 - mae: 0.6220 - val_loss: 21.4348 - val_mae: 2.9548\n",
      "처리중인 폴드: 2\n",
      "Train on 303 samples, validate on 101 samples\n",
      "Epoch 1/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 197.8339 - mae: 10.0946 - val_loss: 31.5135 - val_mae: 3.8011\n",
      "Epoch 2/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 29.8857 - mae: 3.6556 - val_loss: 24.3248 - val_mae: 3.2912\n",
      "Epoch 3/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 21.3870 - mae: 3.0884 - val_loss: 21.0652 - val_mae: 3.0827\n",
      "Epoch 4/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 16.8898 - mae: 2.7982 - val_loss: 19.8074 - val_mae: 2.8895\n",
      "Epoch 5/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 14.3319 - mae: 2.6545 - val_loss: 18.9747 - val_mae: 2.8767\n",
      "Epoch 6/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 13.0122 - mae: 2.4529 - val_loss: 16.7408 - val_mae: 2.6401\n",
      "Epoch 7/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 11.7433 - mae: 2.3827 - val_loss: 16.0908 - val_mae: 2.5465\n",
      "Epoch 8/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.6944 - mae: 2.3111 - val_loss: 17.1211 - val_mae: 2.8013\n",
      "Epoch 9/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.6045 - mae: 2.2514 - val_loss: 16.9113 - val_mae: 2.7231\n",
      "Epoch 10/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.7602 - mae: 2.1672 - val_loss: 17.3428 - val_mae: 2.8300\n",
      "Epoch 11/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.3334 - mae: 2.2024 - val_loss: 15.7984 - val_mae: 2.5722\n",
      "Epoch 12/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.1987 - mae: 2.1442 - val_loss: 14.4819 - val_mae: 2.5384\n",
      "Epoch 13/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.8262 - mae: 2.1253 - val_loss: 16.4414 - val_mae: 2.7829\n",
      "Epoch 14/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.1867 - mae: 2.0308 - val_loss: 16.8147 - val_mae: 2.6308\n",
      "Epoch 15/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.4515 - mae: 1.9991 - val_loss: 14.9579 - val_mae: 2.6060\n",
      "Epoch 16/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.7979 - mae: 2.0162 - val_loss: 15.2115 - val_mae: 2.6707\n",
      "Epoch 17/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.8200 - mae: 1.8964 - val_loss: 14.5683 - val_mae: 2.6246\n",
      "Epoch 18/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.7687 - mae: 1.9703 - val_loss: 15.8422 - val_mae: 2.5040\n",
      "Epoch 19/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 7.6579 - mae: 1.9265 - val_loss: 17.2180 - val_mae: 2.9304\n",
      "Epoch 20/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.4354 - mae: 1.8693 - val_loss: 15.6177 - val_mae: 2.7508\n",
      "Epoch 21/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.9649 - mae: 1.9023 - val_loss: 15.5490 - val_mae: 2.6893\n",
      "Epoch 22/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.3351 - mae: 1.8788 - val_loss: 14.8484 - val_mae: 2.5320\n",
      "Epoch 23/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.6986 - mae: 1.8179 - val_loss: 15.4843 - val_mae: 2.5994\n",
      "Epoch 24/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.7263 - mae: 1.8551 - val_loss: 14.4665 - val_mae: 2.5202\n",
      "Epoch 25/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.7780 - mae: 1.8279 - val_loss: 16.0250 - val_mae: 2.7010\n",
      "Epoch 26/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.6771 - mae: 1.7709 - val_loss: 15.7308 - val_mae: 2.5745\n",
      "Epoch 27/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.3423 - mae: 1.8100 - val_loss: 15.0725 - val_mae: 2.6631\n",
      "Epoch 28/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.4002 - mae: 1.7697 - val_loss: 15.7352 - val_mae: 2.5943\n",
      "Epoch 29/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.9673 - mae: 1.7565 - val_loss: 16.3974 - val_mae: 2.6893\n",
      "Epoch 30/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.8550 - mae: 1.6956 - val_loss: 15.4918 - val_mae: 2.6243\n",
      "Epoch 31/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.2282 - mae: 1.7298 - val_loss: 15.4871 - val_mae: 2.5473\n",
      "Epoch 32/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.0344 - mae: 1.7868 - val_loss: 14.5074 - val_mae: 2.4873\n",
      "Epoch 33/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.0609 - mae: 1.6870 - val_loss: 15.2541 - val_mae: 2.5590\n",
      "Epoch 34/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.6221 - mae: 1.6715 - val_loss: 16.1379 - val_mae: 2.8190\n",
      "Epoch 35/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.5311 - mae: 1.6997 - val_loss: 15.0236 - val_mae: 2.5956\n",
      "Epoch 36/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.7886 - mae: 1.6946 - val_loss: 16.0491 - val_mae: 2.7500\n",
      "Epoch 37/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.4150 - mae: 1.6640 - val_loss: 15.0711 - val_mae: 2.5761\n",
      "Epoch 38/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9849 - mae: 1.6303 - val_loss: 16.8409 - val_mae: 2.6174\n",
      "Epoch 39/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.8868 - mae: 1.6550 - val_loss: 15.1021 - val_mae: 2.5896\n",
      "Epoch 40/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.2887 - mae: 1.6415 - val_loss: 14.0189 - val_mae: 2.4247\n",
      "Epoch 41/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.2347 - mae: 1.6289 - val_loss: 15.0462 - val_mae: 2.5419\n",
      "Epoch 42/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.2987 - mae: 1.6087 - val_loss: 16.7165 - val_mae: 2.7720\n",
      "Epoch 43/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.1058 - mae: 1.6342 - val_loss: 15.0376 - val_mae: 2.6595\n",
      "Epoch 44/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.1028 - mae: 1.5093 - val_loss: 13.9590 - val_mae: 2.5283\n",
      "Epoch 45/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9961 - mae: 1.5500 - val_loss: 16.1510 - val_mae: 2.7079\n",
      "Epoch 46/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9773 - mae: 1.5412 - val_loss: 15.7116 - val_mae: 2.6236\n",
      "Epoch 47/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9093 - mae: 1.5077 - val_loss: 15.8416 - val_mae: 2.6593\n",
      "Epoch 48/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8227 - mae: 1.5872 - val_loss: 14.8756 - val_mae: 2.5489\n",
      "Epoch 49/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8955 - mae: 1.5670 - val_loss: 15.4194 - val_mae: 2.6460\n",
      "Epoch 50/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.6804 - mae: 1.5083 - val_loss: 16.1688 - val_mae: 2.6550\n",
      "Epoch 51/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3406 - mae: 1.4746 - val_loss: 16.6130 - val_mae: 2.8636\n",
      "Epoch 52/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3037 - mae: 1.4890 - val_loss: 15.1802 - val_mae: 2.5303\n",
      "Epoch 53/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3313 - mae: 1.5469 - val_loss: 16.1599 - val_mae: 2.5892\n",
      "Epoch 54/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3125 - mae: 1.4776 - val_loss: 16.8589 - val_mae: 2.7479\n",
      "Epoch 55/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4432 - mae: 1.4843 - val_loss: 16.4513 - val_mae: 2.6982\n",
      "Epoch 56/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.1300 - mae: 1.4239 - val_loss: 17.3608 - val_mae: 2.7181\n",
      "Epoch 57/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3260 - mae: 1.4056 - val_loss: 15.7849 - val_mae: 2.6215\n",
      "Epoch 58/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2791 - mae: 1.4690 - val_loss: 15.0204 - val_mae: 2.5510\n",
      "Epoch 59/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3702 - mae: 1.4296 - val_loss: 17.0786 - val_mae: 2.7498\n",
      "Epoch 60/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0692 - mae: 1.4025 - val_loss: 17.5261 - val_mae: 2.8224\n",
      "Epoch 61/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.1509 - mae: 1.3952 - val_loss: 16.9523 - val_mae: 2.8717\n",
      "Epoch 62/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.9991 - mae: 1.3883 - val_loss: 13.3744 - val_mae: 2.3520\n",
      "Epoch 63/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0565 - mae: 1.4552 - val_loss: 16.1127 - val_mae: 2.6747\n",
      "Epoch 64/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8479 - mae: 1.4093 - val_loss: 17.0123 - val_mae: 2.6493\n",
      "Epoch 65/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4397 - mae: 1.4183 - val_loss: 15.7604 - val_mae: 2.6453\n",
      "Epoch 66/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0466 - mae: 1.4347 - val_loss: 16.3925 - val_mae: 2.6500\n",
      "Epoch 67/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8354 - mae: 1.3469 - val_loss: 16.3968 - val_mae: 2.7406\n",
      "Epoch 68/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6895 - mae: 1.3753 - val_loss: 18.3505 - val_mae: 2.9204\n",
      "Epoch 69/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8532 - mae: 1.4034 - val_loss: 17.2058 - val_mae: 2.6819\n",
      "Epoch 70/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6921 - mae: 1.3624 - val_loss: 15.4054 - val_mae: 2.5284\n",
      "Epoch 71/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8118 - mae: 1.3926 - val_loss: 16.5342 - val_mae: 2.6791\n",
      "Epoch 72/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7222 - mae: 1.3119 - val_loss: 17.9294 - val_mae: 2.8891\n",
      "Epoch 73/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8503 - mae: 1.3866 - val_loss: 15.0437 - val_mae: 2.6154\n",
      "Epoch 74/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5904 - mae: 1.3185 - val_loss: 14.6773 - val_mae: 2.4983\n",
      "Epoch 75/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5745 - mae: 1.3423 - val_loss: 17.4803 - val_mae: 2.8003\n",
      "Epoch 76/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4764 - mae: 1.3446 - val_loss: 16.0702 - val_mae: 2.5555\n",
      "Epoch 77/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2295 - mae: 1.3218 - val_loss: 18.8941 - val_mae: 3.0911\n",
      "Epoch 78/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5726 - mae: 1.2880 - val_loss: 15.2822 - val_mae: 2.4225\n",
      "Epoch 79/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2633 - mae: 1.2771 - val_loss: 16.9174 - val_mae: 2.8140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6490 - mae: 1.3537 - val_loss: 14.1834 - val_mae: 2.4434\n",
      "Epoch 81/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3185 - mae: 1.3007 - val_loss: 16.4556 - val_mae: 2.6862\n",
      "Epoch 82/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2695 - mae: 1.2805 - val_loss: 15.0279 - val_mae: 2.4777\n",
      "Epoch 83/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2566 - mae: 1.2956 - val_loss: 15.9824 - val_mae: 2.6298\n",
      "Epoch 84/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1339 - mae: 1.2587 - val_loss: 16.4654 - val_mae: 2.7291\n",
      "Epoch 85/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2643 - mae: 1.2649 - val_loss: 16.0622 - val_mae: 2.6096\n",
      "Epoch 86/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1144 - mae: 1.2236 - val_loss: 19.3090 - val_mae: 2.9478\n",
      "Epoch 87/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3791 - mae: 1.2684 - val_loss: 16.2994 - val_mae: 2.6907\n",
      "Epoch 88/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0189 - mae: 1.2507 - val_loss: 17.6601 - val_mae: 2.7408\n",
      "Epoch 89/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8961 - mae: 1.2593 - val_loss: 17.1228 - val_mae: 2.7099\n",
      "Epoch 90/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2584 - mae: 1.2344 - val_loss: 16.0273 - val_mae: 2.6127\n",
      "Epoch 91/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9410 - mae: 1.2111 - val_loss: 17.9946 - val_mae: 2.7987\n",
      "Epoch 92/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8671 - mae: 1.2418 - val_loss: 17.5336 - val_mae: 2.7623\n",
      "Epoch 93/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8801 - mae: 1.2008 - val_loss: 15.3311 - val_mae: 2.6357\n",
      "Epoch 94/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0093 - mae: 1.2607 - val_loss: 13.8601 - val_mae: 2.4455\n",
      "Epoch 95/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7357 - mae: 1.2018 - val_loss: 16.2666 - val_mae: 2.6145\n",
      "Epoch 96/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9808 - mae: 1.2090 - val_loss: 17.1317 - val_mae: 2.7447\n",
      "Epoch 97/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8942 - mae: 1.2001 - val_loss: 15.8661 - val_mae: 2.6558\n",
      "Epoch 98/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8365 - mae: 1.2033 - val_loss: 17.5277 - val_mae: 2.7587\n",
      "Epoch 99/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5325 - mae: 1.1351 - val_loss: 17.0637 - val_mae: 2.7681\n",
      "Epoch 100/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6207 - mae: 1.1647 - val_loss: 16.7111 - val_mae: 2.7550\n",
      "Epoch 101/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9507 - mae: 1.2374 - val_loss: 15.6803 - val_mae: 2.7086\n",
      "Epoch 102/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6219 - mae: 1.1518 - val_loss: 17.3538 - val_mae: 2.8007\n",
      "Epoch 103/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7577 - mae: 1.1637 - val_loss: 17.2935 - val_mae: 2.9195\n",
      "Epoch 104/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5812 - mae: 1.1754 - val_loss: 14.7381 - val_mae: 2.6138\n",
      "Epoch 105/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6605 - mae: 1.1064 - val_loss: 16.0412 - val_mae: 2.7301\n",
      "Epoch 106/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6143 - mae: 1.1824 - val_loss: 14.2802 - val_mae: 2.4202\n",
      "Epoch 107/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6552 - mae: 1.1868 - val_loss: 15.9542 - val_mae: 2.6412\n",
      "Epoch 108/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4676 - mae: 1.1323 - val_loss: 18.0809 - val_mae: 2.7620\n",
      "Epoch 109/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5722 - mae: 1.1403 - val_loss: 15.7247 - val_mae: 2.6043\n",
      "Epoch 110/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3449 - mae: 1.1326 - val_loss: 18.5982 - val_mae: 2.9025\n",
      "Epoch 111/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4288 - mae: 1.1298 - val_loss: 14.5592 - val_mae: 2.5672\n",
      "Epoch 112/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6747 - mae: 1.1465 - val_loss: 15.5849 - val_mae: 2.6715\n",
      "Epoch 113/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4636 - mae: 1.1053 - val_loss: 15.3034 - val_mae: 2.6646\n",
      "Epoch 114/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7528 - mae: 1.1917 - val_loss: 15.0132 - val_mae: 2.5990\n",
      "Epoch 115/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6021 - mae: 1.1232 - val_loss: 15.1713 - val_mae: 2.6053\n",
      "Epoch 116/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3162 - mae: 1.0995 - val_loss: 15.7706 - val_mae: 2.7032\n",
      "Epoch 117/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3556 - mae: 1.1245 - val_loss: 16.7634 - val_mae: 2.7269\n",
      "Epoch 118/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1686 - mae: 1.0858 - val_loss: 18.6810 - val_mae: 2.9799\n",
      "Epoch 119/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4500 - mae: 1.1295 - val_loss: 14.1653 - val_mae: 2.5127\n",
      "Epoch 120/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3870 - mae: 1.1505 - val_loss: 15.4284 - val_mae: 2.5878\n",
      "Epoch 121/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3700 - mae: 1.1355 - val_loss: 14.6386 - val_mae: 2.6016\n",
      "Epoch 122/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2803 - mae: 1.0902 - val_loss: 15.5967 - val_mae: 2.6842\n",
      "Epoch 123/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3629 - mae: 1.1288 - val_loss: 15.0375 - val_mae: 2.6359\n",
      "Epoch 124/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2241 - mae: 1.1089 - val_loss: 15.9187 - val_mae: 2.5971\n",
      "Epoch 125/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1937 - mae: 1.0804 - val_loss: 13.9533 - val_mae: 2.5953\n",
      "Epoch 126/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1171 - mae: 1.0763 - val_loss: 15.5809 - val_mae: 2.6709\n",
      "Epoch 127/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1956 - mae: 1.1261 - val_loss: 16.1939 - val_mae: 2.6842\n",
      "Epoch 128/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3952 - mae: 1.0997 - val_loss: 15.6540 - val_mae: 2.7326\n",
      "Epoch 129/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3237 - mae: 1.0430 - val_loss: 17.6648 - val_mae: 2.7653\n",
      "Epoch 130/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1822 - mae: 1.0965 - val_loss: 15.2735 - val_mae: 2.6966\n",
      "Epoch 131/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1065 - mae: 1.0536 - val_loss: 14.6504 - val_mae: 2.6805\n",
      "Epoch 132/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0179 - mae: 1.0429 - val_loss: 13.5984 - val_mae: 2.5583\n",
      "Epoch 133/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2577 - mae: 1.0865 - val_loss: 15.1420 - val_mae: 2.6270\n",
      "Epoch 134/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9913 - mae: 1.0641 - val_loss: 17.0314 - val_mae: 2.7554\n",
      "Epoch 135/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2202 - mae: 1.0867 - val_loss: 15.9179 - val_mae: 2.7329\n",
      "Epoch 136/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1051 - mae: 1.0839 - val_loss: 15.9184 - val_mae: 2.6726\n",
      "Epoch 137/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2906 - mae: 1.0519 - val_loss: 15.4708 - val_mae: 2.6516\n",
      "Epoch 138/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1167 - mae: 1.0786 - val_loss: 15.4891 - val_mae: 2.6808\n",
      "Epoch 139/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9176 - mae: 0.9766 - val_loss: 16.6156 - val_mae: 2.8168\n",
      "Epoch 140/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0195 - mae: 1.0417 - val_loss: 17.2496 - val_mae: 2.9415\n",
      "Epoch 141/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1022 - mae: 1.0781 - val_loss: 17.6932 - val_mae: 2.8166\n",
      "Epoch 142/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1237 - mae: 1.0837 - val_loss: 16.8397 - val_mae: 2.6561\n",
      "Epoch 143/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9078 - mae: 1.0033 - val_loss: 17.4513 - val_mae: 2.8102\n",
      "Epoch 144/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8248 - mae: 1.0219 - val_loss: 15.8596 - val_mae: 2.5920\n",
      "Epoch 145/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9783 - mae: 1.0374 - val_loss: 14.5941 - val_mae: 2.5815\n",
      "Epoch 146/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9386 - mae: 1.0227 - val_loss: 15.4067 - val_mae: 2.7518\n",
      "Epoch 147/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9181 - mae: 1.0238 - val_loss: 14.3492 - val_mae: 2.5625\n",
      "Epoch 148/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8040 - mae: 1.0166 - val_loss: 13.8601 - val_mae: 2.4508\n",
      "Epoch 149/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8512 - mae: 1.0123 - val_loss: 13.5282 - val_mae: 2.5604\n",
      "Epoch 150/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8177 - mae: 0.9758 - val_loss: 16.1055 - val_mae: 2.7221\n",
      "Epoch 151/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0336 - mae: 1.0447 - val_loss: 14.6613 - val_mae: 2.5963\n",
      "Epoch 152/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8857 - mae: 0.9943 - val_loss: 17.1442 - val_mae: 2.7715\n",
      "Epoch 153/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0779 - mae: 1.0342 - val_loss: 13.8474 - val_mae: 2.5090\n",
      "Epoch 154/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9015 - mae: 0.9889 - val_loss: 15.0883 - val_mae: 2.6356\n",
      "Epoch 155/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8319 - mae: 0.9729 - val_loss: 15.7503 - val_mae: 2.6343\n",
      "Epoch 156/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7584 - mae: 0.9801 - val_loss: 16.2991 - val_mae: 2.8406\n",
      "Epoch 157/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8569 - mae: 0.9942 - val_loss: 15.4572 - val_mae: 2.6355\n",
      "Epoch 158/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8215 - mae: 1.0105 - val_loss: 13.7568 - val_mae: 2.5594\n",
      "Epoch 159/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7295 - mae: 0.9656 - val_loss: 13.4897 - val_mae: 2.4370\n",
      "Epoch 160/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7278 - mae: 0.9895 - val_loss: 15.2771 - val_mae: 2.6583\n",
      "Epoch 161/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7336 - mae: 0.9700 - val_loss: 15.1175 - val_mae: 2.5441\n",
      "Epoch 162/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7620 - mae: 0.9386 - val_loss: 18.2099 - val_mae: 2.9273\n",
      "Epoch 163/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9770 - mae: 1.0055 - val_loss: 15.3994 - val_mae: 2.6448\n",
      "Epoch 164/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8203 - mae: 0.9843 - val_loss: 16.1564 - val_mae: 2.6836\n",
      "Epoch 165/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7812 - mae: 0.9866 - val_loss: 14.3780 - val_mae: 2.5665\n",
      "Epoch 166/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5934 - mae: 0.9435 - val_loss: 16.9165 - val_mae: 2.8400\n",
      "Epoch 167/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9092 - mae: 0.9554 - val_loss: 17.6701 - val_mae: 2.9768\n",
      "Epoch 168/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8876 - mae: 1.0386 - val_loss: 15.7807 - val_mae: 2.5927\n",
      "Epoch 169/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6663 - mae: 0.9621 - val_loss: 16.2650 - val_mae: 2.8643\n",
      "Epoch 170/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5717 - mae: 0.9207 - val_loss: 17.9818 - val_mae: 2.8510\n",
      "Epoch 171/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8070 - mae: 1.0007 - val_loss: 16.1708 - val_mae: 2.7330\n",
      "Epoch 172/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7368 - mae: 0.9641 - val_loss: 16.3994 - val_mae: 2.8197\n",
      "Epoch 173/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7856 - mae: 0.9409 - val_loss: 15.0239 - val_mae: 2.6685\n",
      "Epoch 174/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7983 - mae: 0.9702 - val_loss: 15.0330 - val_mae: 2.7363\n",
      "Epoch 175/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6236 - mae: 0.9375 - val_loss: 13.6517 - val_mae: 2.6333\n",
      "Epoch 176/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7064 - mae: 0.9476 - val_loss: 13.4030 - val_mae: 2.5401\n",
      "Epoch 177/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6742 - mae: 0.9404 - val_loss: 15.9237 - val_mae: 2.8700\n",
      "Epoch 178/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6759 - mae: 0.9558 - val_loss: 14.9816 - val_mae: 2.6134\n",
      "Epoch 179/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7270 - mae: 0.9325 - val_loss: 13.7069 - val_mae: 2.5338\n",
      "Epoch 180/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5218 - mae: 0.8813 - val_loss: 17.0282 - val_mae: 2.9990\n",
      "Epoch 181/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6334 - mae: 0.9259 - val_loss: 14.5400 - val_mae: 2.6919\n",
      "Epoch 182/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5279 - mae: 0.9265 - val_loss: 15.0592 - val_mae: 2.7013\n",
      "Epoch 183/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6780 - mae: 0.9652 - val_loss: 14.6155 - val_mae: 2.6698\n",
      "Epoch 184/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6634 - mae: 0.9113 - val_loss: 13.3444 - val_mae: 2.4132\n",
      "Epoch 185/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4862 - mae: 0.9252 - val_loss: 15.5272 - val_mae: 2.7905\n",
      "Epoch 186/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6615 - mae: 0.9368 - val_loss: 15.5651 - val_mae: 2.6753\n",
      "Epoch 187/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4896 - mae: 0.9075 - val_loss: 13.7926 - val_mae: 2.5895\n",
      "Epoch 188/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5572 - mae: 0.8798 - val_loss: 13.8034 - val_mae: 2.6123\n",
      "Epoch 189/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5030 - mae: 0.9237 - val_loss: 15.4460 - val_mae: 2.7916\n",
      "Epoch 190/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4558 - mae: 0.9208 - val_loss: 13.5910 - val_mae: 2.6460\n",
      "Epoch 191/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4666 - mae: 0.9198 - val_loss: 12.8749 - val_mae: 2.4085\n",
      "Epoch 192/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5167 - mae: 0.9229 - val_loss: 13.6159 - val_mae: 2.6087\n",
      "Epoch 193/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4228 - mae: 0.8658 - val_loss: 14.8952 - val_mae: 2.6813\n",
      "Epoch 194/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5166 - mae: 0.9110 - val_loss: 12.4411 - val_mae: 2.4410\n",
      "Epoch 195/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4498 - mae: 0.8919 - val_loss: 14.1641 - val_mae: 2.6744\n",
      "Epoch 196/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4018 - mae: 0.9054 - val_loss: 12.5072 - val_mae: 2.5746\n",
      "Epoch 197/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5973 - mae: 0.9548 - val_loss: 13.8801 - val_mae: 2.7431\n",
      "Epoch 198/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4477 - mae: 0.8860 - val_loss: 14.3820 - val_mae: 2.7291\n",
      "Epoch 199/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3940 - mae: 0.8816 - val_loss: 14.1236 - val_mae: 2.7146\n",
      "Epoch 200/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4334 - mae: 0.8692 - val_loss: 14.2242 - val_mae: 2.5521\n",
      "Epoch 201/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3803 - mae: 0.8829 - val_loss: 13.8776 - val_mae: 2.6180\n",
      "Epoch 202/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3957 - mae: 0.8679 - val_loss: 14.0819 - val_mae: 2.5160\n",
      "Epoch 203/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3873 - mae: 0.8381 - val_loss: 15.0262 - val_mae: 2.6014\n",
      "Epoch 204/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4922 - mae: 0.9348 - val_loss: 13.4997 - val_mae: 2.6002\n",
      "Epoch 205/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3789 - mae: 0.8864 - val_loss: 14.5719 - val_mae: 2.5466\n",
      "Epoch 206/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4730 - mae: 0.8973 - val_loss: 13.0070 - val_mae: 2.5695\n",
      "Epoch 207/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3732 - mae: 0.8566 - val_loss: 13.5300 - val_mae: 2.5561\n",
      "Epoch 208/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5428 - mae: 0.8883 - val_loss: 12.8821 - val_mae: 2.5253\n",
      "Epoch 209/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4382 - mae: 0.8688 - val_loss: 14.2346 - val_mae: 2.6649\n",
      "Epoch 210/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3484 - mae: 0.8783 - val_loss: 14.0379 - val_mae: 2.6113\n",
      "Epoch 211/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2921 - mae: 0.8452 - val_loss: 12.4441 - val_mae: 2.4128\n",
      "Epoch 212/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4100 - mae: 0.8431 - val_loss: 14.1391 - val_mae: 2.6198\n",
      "Epoch 213/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2941 - mae: 0.8764 - val_loss: 13.6223 - val_mae: 2.5783\n",
      "Epoch 214/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3090 - mae: 0.8247 - val_loss: 14.7546 - val_mae: 2.6590\n",
      "Epoch 215/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3534 - mae: 0.8761 - val_loss: 13.8214 - val_mae: 2.5592\n",
      "Epoch 216/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1316 - mae: 0.7909 - val_loss: 18.0001 - val_mae: 3.0789\n",
      "Epoch 217/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2091 - mae: 0.8153 - val_loss: 12.5595 - val_mae: 2.4567\n",
      "Epoch 218/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3539 - mae: 0.8236 - val_loss: 13.7373 - val_mae: 2.6229\n",
      "Epoch 219/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2333 - mae: 0.8057 - val_loss: 13.4116 - val_mae: 2.5414\n",
      "Epoch 220/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4819 - mae: 0.8961 - val_loss: 16.0896 - val_mae: 2.6633\n",
      "Epoch 221/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2502 - mae: 0.8087 - val_loss: 12.8839 - val_mae: 2.5192\n",
      "Epoch 222/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2832 - mae: 0.8431 - val_loss: 15.3832 - val_mae: 2.7928\n",
      "Epoch 223/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1917 - mae: 0.8003 - val_loss: 13.6518 - val_mae: 2.5317\n",
      "Epoch 224/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2789 - mae: 0.8453 - val_loss: 13.9577 - val_mae: 2.5135\n",
      "Epoch 225/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2147 - mae: 0.8045 - val_loss: 14.7155 - val_mae: 2.8055\n",
      "Epoch 226/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2685 - mae: 0.8147 - val_loss: 12.5670 - val_mae: 2.5717\n",
      "Epoch 227/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2676 - mae: 0.8421 - val_loss: 13.4049 - val_mae: 2.6041\n",
      "Epoch 228/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2795 - mae: 0.8206 - val_loss: 13.8014 - val_mae: 2.5904\n",
      "Epoch 229/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1427 - mae: 0.7760 - val_loss: 13.8332 - val_mae: 2.7292\n",
      "Epoch 230/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2751 - mae: 0.8324 - val_loss: 13.7531 - val_mae: 2.5850\n",
      "Epoch 231/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1836 - mae: 0.8162 - val_loss: 14.3033 - val_mae: 2.6296\n",
      "Epoch 232/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3822 - mae: 0.8333 - val_loss: 14.1704 - val_mae: 2.5974\n",
      "Epoch 233/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1567 - mae: 0.7998 - val_loss: 13.8353 - val_mae: 2.6222\n",
      "Epoch 234/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2131 - mae: 0.8177 - val_loss: 13.4221 - val_mae: 2.6343\n",
      "Epoch 235/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2777 - mae: 0.8204 - val_loss: 12.9301 - val_mae: 2.4779\n",
      "Epoch 236/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1382 - mae: 0.8351 - val_loss: 12.4480 - val_mae: 2.5628\n",
      "Epoch 237/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2050 - mae: 0.8140 - val_loss: 14.7498 - val_mae: 2.6143\n",
      "Epoch 238/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3330 - mae: 0.8606 - val_loss: 12.1601 - val_mae: 2.4795\n",
      "Epoch 239/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2429 - mae: 0.8294 - val_loss: 13.7445 - val_mae: 2.6046\n",
      "Epoch 240/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1530 - mae: 0.7801 - val_loss: 14.3428 - val_mae: 2.7972\n",
      "Epoch 241/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2495 - mae: 0.8330 - val_loss: 12.8077 - val_mae: 2.5688\n",
      "Epoch 242/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1142 - mae: 0.7778 - val_loss: 12.7016 - val_mae: 2.5346\n",
      "Epoch 243/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1617 - mae: 0.8012 - val_loss: 13.1605 - val_mae: 2.5271\n",
      "Epoch 244/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1211 - mae: 0.8130 - val_loss: 12.1915 - val_mae: 2.5117\n",
      "Epoch 245/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0640 - mae: 0.7988 - val_loss: 11.2121 - val_mae: 2.4021\n",
      "Epoch 246/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1935 - mae: 0.8028 - val_loss: 13.5157 - val_mae: 2.6926\n",
      "Epoch 247/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1032 - mae: 0.7854 - val_loss: 11.9304 - val_mae: 2.4990\n",
      "Epoch 248/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0879 - mae: 0.8141 - val_loss: 12.7577 - val_mae: 2.6075\n",
      "Epoch 249/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0973 - mae: 0.7507 - val_loss: 13.7691 - val_mae: 2.6838\n",
      "Epoch 250/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0935 - mae: 0.7845 - val_loss: 12.2418 - val_mae: 2.4825\n",
      "Epoch 251/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2375 - mae: 0.8265 - val_loss: 13.0263 - val_mae: 2.5929\n",
      "Epoch 252/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1604 - mae: 0.7799 - val_loss: 13.3827 - val_mae: 2.6008\n",
      "Epoch 253/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1353 - mae: 0.7744 - val_loss: 12.9879 - val_mae: 2.5999\n",
      "Epoch 254/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2280 - mae: 0.8078 - val_loss: 12.8294 - val_mae: 2.5682\n",
      "Epoch 255/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0210 - mae: 0.7727 - val_loss: 13.8967 - val_mae: 2.7051\n",
      "Epoch 256/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1916 - mae: 0.7992 - val_loss: 14.0239 - val_mae: 2.6534\n",
      "Epoch 257/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1143 - mae: 0.7837 - val_loss: 13.5807 - val_mae: 2.7019\n",
      "Epoch 258/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1082 - mae: 0.7768 - val_loss: 13.7691 - val_mae: 2.6327\n",
      "Epoch 259/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0555 - mae: 0.7679 - val_loss: 13.3707 - val_mae: 2.5913\n",
      "Epoch 260/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2268 - mae: 0.8127 - val_loss: 14.2038 - val_mae: 2.7187\n",
      "Epoch 261/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0490 - mae: 0.7487 - val_loss: 14.3984 - val_mae: 2.5945\n",
      "Epoch 262/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0688 - mae: 0.7535 - val_loss: 13.3635 - val_mae: 2.6105\n",
      "Epoch 263/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0315 - mae: 0.7734 - val_loss: 14.5595 - val_mae: 2.7208\n",
      "Epoch 264/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1727 - mae: 0.8110 - val_loss: 12.2636 - val_mae: 2.4017\n",
      "Epoch 265/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1801 - mae: 0.8158 - val_loss: 13.8914 - val_mae: 2.5265\n",
      "Epoch 266/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1162 - mae: 0.7756 - val_loss: 13.8043 - val_mae: 2.5823\n",
      "Epoch 267/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0020 - mae: 0.7476 - val_loss: 13.5190 - val_mae: 2.6173\n",
      "Epoch 268/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0058 - mae: 0.7487 - val_loss: 13.2407 - val_mae: 2.5288\n",
      "Epoch 269/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9788 - mae: 0.7384 - val_loss: 13.1878 - val_mae: 2.5387\n",
      "Epoch 270/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0825 - mae: 0.7313 - val_loss: 13.3279 - val_mae: 2.7060\n",
      "Epoch 271/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0539 - mae: 0.7538 - val_loss: 13.4599 - val_mae: 2.5675\n",
      "Epoch 272/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0091 - mae: 0.7378 - val_loss: 14.0023 - val_mae: 2.7910\n",
      "Epoch 273/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0001 - mae: 0.7469 - val_loss: 14.0442 - val_mae: 2.6765\n",
      "Epoch 274/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0545 - mae: 0.7702 - val_loss: 13.8237 - val_mae: 2.6643\n",
      "Epoch 275/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0841 - mae: 0.7951 - val_loss: 14.3584 - val_mae: 2.6323\n",
      "Epoch 276/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1013 - mae: 0.7594 - val_loss: 13.3338 - val_mae: 2.4985\n",
      "Epoch 277/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9769 - mae: 0.7344 - val_loss: 12.7799 - val_mae: 2.4575\n",
      "Epoch 278/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9697 - mae: 0.7547 - val_loss: 12.7671 - val_mae: 2.4834\n",
      "Epoch 279/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0583 - mae: 0.7623 - val_loss: 13.7278 - val_mae: 2.6577\n",
      "Epoch 280/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0447 - mae: 0.7606 - val_loss: 13.0582 - val_mae: 2.5764\n",
      "Epoch 281/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9587 - mae: 0.7275 - val_loss: 13.7554 - val_mae: 2.5441\n",
      "Epoch 282/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0060 - mae: 0.7618 - val_loss: 13.3893 - val_mae: 2.6869\n",
      "Epoch 283/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9480 - mae: 0.7335 - val_loss: 13.9587 - val_mae: 2.7116\n",
      "Epoch 284/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1891 - mae: 0.7664 - val_loss: 12.8068 - val_mae: 2.5231\n",
      "Epoch 285/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9262 - mae: 0.7356 - val_loss: 12.5428 - val_mae: 2.5152\n",
      "Epoch 286/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9377 - mae: 0.7189 - val_loss: 13.1300 - val_mae: 2.5625\n",
      "Epoch 287/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0089 - mae: 0.7221 - val_loss: 12.4322 - val_mae: 2.4767\n",
      "Epoch 288/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1049 - mae: 0.7655 - val_loss: 12.8529 - val_mae: 2.5073\n",
      "Epoch 289/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9611 - mae: 0.7310 - val_loss: 13.2355 - val_mae: 2.5103\n",
      "Epoch 290/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1014 - mae: 0.7878 - val_loss: 13.5411 - val_mae: 2.4785\n",
      "Epoch 291/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8849 - mae: 0.6700 - val_loss: 13.2915 - val_mae: 2.5724\n",
      "Epoch 292/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9063 - mae: 0.7165 - val_loss: 13.7868 - val_mae: 2.5812\n",
      "Epoch 293/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8891 - mae: 0.7005 - val_loss: 13.5172 - val_mae: 2.6378\n",
      "Epoch 294/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9599 - mae: 0.7183 - val_loss: 14.1468 - val_mae: 2.4609\n",
      "Epoch 295/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9787 - mae: 0.7030 - val_loss: 13.6831 - val_mae: 2.6161\n",
      "Epoch 296/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0382 - mae: 0.7554 - val_loss: 13.1096 - val_mae: 2.5208\n",
      "Epoch 297/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0103 - mae: 0.7358 - val_loss: 12.3744 - val_mae: 2.5087\n",
      "Epoch 298/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0245 - mae: 0.7481 - val_loss: 12.2336 - val_mae: 2.4944\n",
      "Epoch 299/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9626 - mae: 0.7421 - val_loss: 12.4659 - val_mae: 2.4205\n",
      "Epoch 300/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9948 - mae: 0.7170 - val_loss: 13.2246 - val_mae: 2.5366\n",
      "Epoch 301/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9227 - mae: 0.7275 - val_loss: 13.9264 - val_mae: 2.5057\n",
      "Epoch 302/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8747 - mae: 0.6912 - val_loss: 13.9688 - val_mae: 2.6191\n",
      "Epoch 303/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0515 - mae: 0.7390 - val_loss: 12.6648 - val_mae: 2.5210\n",
      "Epoch 304/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9767 - mae: 0.7307 - val_loss: 14.0079 - val_mae: 2.5676\n",
      "Epoch 305/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8327 - mae: 0.7021 - val_loss: 13.5326 - val_mae: 2.7084\n",
      "Epoch 306/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9920 - mae: 0.7305 - val_loss: 13.3790 - val_mae: 2.4852\n",
      "Epoch 307/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9189 - mae: 0.7099 - val_loss: 13.7808 - val_mae: 2.6516\n",
      "Epoch 308/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0513 - mae: 0.7641 - val_loss: 13.0955 - val_mae: 2.5338\n",
      "Epoch 309/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0094 - mae: 0.7569 - val_loss: 12.7633 - val_mae: 2.4961\n",
      "Epoch 310/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8836 - mae: 0.7021 - val_loss: 12.2139 - val_mae: 2.4403\n",
      "Epoch 311/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9396 - mae: 0.7251 - val_loss: 13.2507 - val_mae: 2.5272\n",
      "Epoch 312/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9124 - mae: 0.6935 - val_loss: 12.8029 - val_mae: 2.5055\n",
      "Epoch 313/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9000 - mae: 0.6901 - val_loss: 13.4250 - val_mae: 2.4936\n",
      "Epoch 314/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8760 - mae: 0.6988 - val_loss: 14.6834 - val_mae: 2.5770\n",
      "Epoch 315/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8547 - mae: 0.6984 - val_loss: 14.0352 - val_mae: 2.5001\n",
      "Epoch 316/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0441 - mae: 0.7457 - val_loss: 13.2090 - val_mae: 2.4970\n",
      "Epoch 317/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8489 - mae: 0.6835 - val_loss: 14.3355 - val_mae: 2.4379\n",
      "Epoch 318/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9218 - mae: 0.7090 - val_loss: 13.0011 - val_mae: 2.5200\n",
      "Epoch 319/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9835 - mae: 0.7265 - val_loss: 13.1295 - val_mae: 2.5354\n",
      "Epoch 320/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8029 - mae: 0.6708 - val_loss: 14.0306 - val_mae: 2.5577\n",
      "Epoch 321/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8751 - mae: 0.6987 - val_loss: 13.6919 - val_mae: 2.4350\n",
      "Epoch 322/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9418 - mae: 0.7018 - val_loss: 13.2544 - val_mae: 2.4687\n",
      "Epoch 323/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0001 - mae: 0.7354 - val_loss: 13.6091 - val_mae: 2.5904\n",
      "Epoch 324/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8866 - mae: 0.6723 - val_loss: 13.1717 - val_mae: 2.5690\n",
      "Epoch 325/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8735 - mae: 0.7030 - val_loss: 13.8483 - val_mae: 2.4990\n",
      "Epoch 326/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9845 - mae: 0.7106 - val_loss: 13.5845 - val_mae: 2.5980\n",
      "Epoch 327/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8173 - mae: 0.6867 - val_loss: 13.7886 - val_mae: 2.6480\n",
      "Epoch 328/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0166 - mae: 0.7174 - val_loss: 13.9106 - val_mae: 2.6172\n",
      "Epoch 329/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8758 - mae: 0.7170 - val_loss: 15.0026 - val_mae: 2.5701\n",
      "Epoch 330/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9370 - mae: 0.6961 - val_loss: 14.3923 - val_mae: 2.7030\n",
      "Epoch 331/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9211 - mae: 0.7012 - val_loss: 13.0566 - val_mae: 2.6206\n",
      "Epoch 332/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9230 - mae: 0.7030 - val_loss: 13.1737 - val_mae: 2.5173\n",
      "Epoch 333/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7791 - mae: 0.6654 - val_loss: 13.2161 - val_mae: 2.5764\n",
      "Epoch 334/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9232 - mae: 0.7206 - val_loss: 12.3732 - val_mae: 2.4790\n",
      "Epoch 335/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9217 - mae: 0.7061 - val_loss: 13.4164 - val_mae: 2.5072\n",
      "Epoch 336/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9240 - mae: 0.7192 - val_loss: 12.8469 - val_mae: 2.5142\n",
      "Epoch 337/500\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.6291 - mae: 0.624 - 1s 2ms/step - loss: 0.6590 - mae: 0.6370 - val_loss: 13.4371 - val_mae: 2.5540\n",
      "Epoch 338/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9794 - mae: 0.7303 - val_loss: 13.3589 - val_mae: 2.5296\n",
      "Epoch 339/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8843 - mae: 0.7075 - val_loss: 13.7721 - val_mae: 2.5703\n",
      "Epoch 340/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8817 - mae: 0.7046 - val_loss: 12.9078 - val_mae: 2.4461\n",
      "Epoch 341/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8217 - mae: 0.6790 - val_loss: 12.5670 - val_mae: 2.4835\n",
      "Epoch 342/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8983 - mae: 0.6780 - val_loss: 14.1970 - val_mae: 2.5661\n",
      "Epoch 343/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9181 - mae: 0.7167 - val_loss: 13.4742 - val_mae: 2.5064\n",
      "Epoch 344/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8233 - mae: 0.6702 - val_loss: 12.9147 - val_mae: 2.4617\n",
      "Epoch 345/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8981 - mae: 0.6811 - val_loss: 14.0574 - val_mae: 2.5400\n",
      "Epoch 346/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7845 - mae: 0.6664 - val_loss: 13.6035 - val_mae: 2.6514\n",
      "Epoch 347/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7606 - mae: 0.6792 - val_loss: 14.0084 - val_mae: 2.7254\n",
      "Epoch 348/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8835 - mae: 0.7082 - val_loss: 14.3150 - val_mae: 2.6646\n",
      "Epoch 349/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8106 - mae: 0.6723 - val_loss: 13.3407 - val_mae: 2.4933\n",
      "Epoch 350/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8401 - mae: 0.7015 - val_loss: 13.5258 - val_mae: 2.5348\n",
      "Epoch 351/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8594 - mae: 0.6904 - val_loss: 14.7978 - val_mae: 2.6935\n",
      "Epoch 352/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8647 - mae: 0.7084 - val_loss: 13.7523 - val_mae: 2.6590\n",
      "Epoch 353/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8842 - mae: 0.6769 - val_loss: 12.4663 - val_mae: 2.5955\n",
      "Epoch 354/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8238 - mae: 0.6917 - val_loss: 13.9342 - val_mae: 2.5567\n",
      "Epoch 355/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9553 - mae: 0.7134 - val_loss: 13.9347 - val_mae: 2.5464\n",
      "Epoch 356/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7885 - mae: 0.6572 - val_loss: 14.0316 - val_mae: 2.5405\n",
      "Epoch 357/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8841 - mae: 0.6850 - val_loss: 13.8128 - val_mae: 2.5532\n",
      "Epoch 358/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7604 - mae: 0.6252 - val_loss: 13.7984 - val_mae: 2.6378\n",
      "Epoch 359/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8425 - mae: 0.6888 - val_loss: 13.5010 - val_mae: 2.6043\n",
      "Epoch 360/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8110 - mae: 0.6790 - val_loss: 13.0556 - val_mae: 2.5073\n",
      "Epoch 361/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8889 - mae: 0.7231 - val_loss: 13.7211 - val_mae: 2.5962\n",
      "Epoch 362/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8217 - mae: 0.6581 - val_loss: 12.7408 - val_mae: 2.4822\n",
      "Epoch 363/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8872 - mae: 0.7047 - val_loss: 13.7939 - val_mae: 2.5771\n",
      "Epoch 364/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8117 - mae: 0.6678 - val_loss: 14.9181 - val_mae: 2.6860\n",
      "Epoch 365/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8061 - mae: 0.6853 - val_loss: 13.9360 - val_mae: 2.5756\n",
      "Epoch 366/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8189 - mae: 0.6380 - val_loss: 13.9428 - val_mae: 2.5690\n",
      "Epoch 367/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7807 - mae: 0.6794 - val_loss: 15.0310 - val_mae: 2.6242\n",
      "Epoch 368/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8192 - mae: 0.6743 - val_loss: 13.9300 - val_mae: 2.5723\n",
      "Epoch 369/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8175 - mae: 0.6623 - val_loss: 14.8118 - val_mae: 2.6679\n",
      "Epoch 370/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8795 - mae: 0.6854 - val_loss: 13.5432 - val_mae: 2.5241\n",
      "Epoch 371/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7447 - mae: 0.6224 - val_loss: 14.1840 - val_mae: 2.5854\n",
      "Epoch 372/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8685 - mae: 0.6737 - val_loss: 13.4316 - val_mae: 2.6610\n",
      "Epoch 373/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7995 - mae: 0.6646 - val_loss: 13.4551 - val_mae: 2.5673\n",
      "Epoch 374/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9974 - mae: 0.7172 - val_loss: 13.4369 - val_mae: 2.4466\n",
      "Epoch 375/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8369 - mae: 0.6968 - val_loss: 14.6304 - val_mae: 2.5215\n",
      "Epoch 376/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8040 - mae: 0.6590 - val_loss: 13.8286 - val_mae: 2.6803\n",
      "Epoch 377/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7605 - mae: 0.6637 - val_loss: 12.9643 - val_mae: 2.5589\n",
      "Epoch 378/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8021 - mae: 0.6530 - val_loss: 12.9693 - val_mae: 2.5420\n",
      "Epoch 379/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8279 - mae: 0.6643 - val_loss: 13.4856 - val_mae: 2.5576\n",
      "Epoch 380/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8185 - mae: 0.6896 - val_loss: 12.7410 - val_mae: 2.4699\n",
      "Epoch 381/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7695 - mae: 0.6510 - val_loss: 13.6735 - val_mae: 2.5276\n",
      "Epoch 382/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7715 - mae: 0.6396 - val_loss: 12.7471 - val_mae: 2.4088\n",
      "Epoch 383/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8054 - mae: 0.6867 - val_loss: 13.6893 - val_mae: 2.5571\n",
      "Epoch 384/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8156 - mae: 0.6539 - val_loss: 13.6390 - val_mae: 2.6067\n",
      "Epoch 385/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8777 - mae: 0.6923 - val_loss: 12.7746 - val_mae: 2.5302\n",
      "Epoch 386/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8040 - mae: 0.6587 - val_loss: 13.7181 - val_mae: 2.5376\n",
      "Epoch 387/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9003 - mae: 0.6763 - val_loss: 12.8181 - val_mae: 2.4919\n",
      "Epoch 388/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6682 - mae: 0.5988 - val_loss: 12.7340 - val_mae: 2.4462\n",
      "Epoch 389/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7916 - mae: 0.6685 - val_loss: 13.6298 - val_mae: 2.5668\n",
      "Epoch 390/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8219 - mae: 0.6608 - val_loss: 13.3278 - val_mae: 2.5704\n",
      "Epoch 391/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9064 - mae: 0.6889 - val_loss: 12.8901 - val_mae: 2.4938\n",
      "Epoch 392/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7632 - mae: 0.6545 - val_loss: 13.1455 - val_mae: 2.5061\n",
      "Epoch 393/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8195 - mae: 0.6887 - val_loss: 12.0660 - val_mae: 2.4425\n",
      "Epoch 394/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6872 - mae: 0.6411 - val_loss: 13.6572 - val_mae: 2.5810\n",
      "Epoch 395/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8302 - mae: 0.6413 - val_loss: 13.3962 - val_mae: 2.6160\n",
      "Epoch 396/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7617 - mae: 0.6576 - val_loss: 13.8951 - val_mae: 2.4811\n",
      "Epoch 397/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7184 - mae: 0.6297 - val_loss: 14.0582 - val_mae: 2.5686\n",
      "Epoch 398/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8411 - mae: 0.6832 - val_loss: 13.1526 - val_mae: 2.4926\n",
      "Epoch 399/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7020 - mae: 0.6273 - val_loss: 13.3577 - val_mae: 2.4644\n",
      "Epoch 400/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7768 - mae: 0.6542 - val_loss: 13.6585 - val_mae: 2.5966\n",
      "Epoch 401/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7743 - mae: 0.6641 - val_loss: 13.8796 - val_mae: 2.4955\n",
      "Epoch 402/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7762 - mae: 0.6561 - val_loss: 12.5820 - val_mae: 2.4837\n",
      "Epoch 403/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7324 - mae: 0.6359 - val_loss: 14.1204 - val_mae: 2.5942\n",
      "Epoch 404/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8070 - mae: 0.6730 - val_loss: 13.6468 - val_mae: 2.6129\n",
      "Epoch 405/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8444 - mae: 0.6580 - val_loss: 13.5966 - val_mae: 2.5192\n",
      "Epoch 406/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7211 - mae: 0.6338 - val_loss: 14.2640 - val_mae: 2.5366\n",
      "Epoch 407/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8419 - mae: 0.6913 - val_loss: 14.0145 - val_mae: 2.6120\n",
      "Epoch 408/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7704 - mae: 0.6583 - val_loss: 13.5525 - val_mae: 2.6098\n",
      "Epoch 409/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7888 - mae: 0.6600 - val_loss: 13.9760 - val_mae: 2.5788\n",
      "Epoch 410/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7128 - mae: 0.6214 - val_loss: 13.4885 - val_mae: 2.5249\n",
      "Epoch 411/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8310 - mae: 0.6654 - val_loss: 13.5248 - val_mae: 2.6193\n",
      "Epoch 412/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7398 - mae: 0.6453 - val_loss: 13.1470 - val_mae: 2.5837\n",
      "Epoch 413/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7082 - mae: 0.6308 - val_loss: 14.5542 - val_mae: 2.6652\n",
      "Epoch 414/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7024 - mae: 0.6247 - val_loss: 14.0698 - val_mae: 2.5784\n",
      "Epoch 415/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7430 - mae: 0.6562 - val_loss: 13.6638 - val_mae: 2.5198\n",
      "Epoch 416/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7688 - mae: 0.6314 - val_loss: 13.4796 - val_mae: 2.5540\n",
      "Epoch 417/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6444 - mae: 0.5808 - val_loss: 13.1107 - val_mae: 2.5072\n",
      "Epoch 418/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7868 - mae: 0.6585 - val_loss: 15.3255 - val_mae: 2.6340\n",
      "Epoch 419/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6846 - mae: 0.6125 - val_loss: 14.7743 - val_mae: 2.7254\n",
      "Epoch 420/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7537 - mae: 0.6389 - val_loss: 13.9068 - val_mae: 2.5997\n",
      "Epoch 421/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6989 - mae: 0.6179 - val_loss: 13.3407 - val_mae: 2.5701\n",
      "Epoch 422/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6743 - mae: 0.6241 - val_loss: 15.2862 - val_mae: 2.6637\n",
      "Epoch 423/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6325 - mae: 0.6022 - val_loss: 13.2761 - val_mae: 2.4697\n",
      "Epoch 424/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8565 - mae: 0.6566 - val_loss: 14.0799 - val_mae: 2.6093\n",
      "Epoch 425/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7386 - mae: 0.6181 - val_loss: 13.8587 - val_mae: 2.6757\n",
      "Epoch 426/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7061 - mae: 0.6213 - val_loss: 13.5376 - val_mae: 2.6478\n",
      "Epoch 427/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8312 - mae: 0.6325 - val_loss: 12.8904 - val_mae: 2.5083\n",
      "Epoch 428/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7099 - mae: 0.6289 - val_loss: 13.0743 - val_mae: 2.6222\n",
      "Epoch 429/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7465 - mae: 0.6177 - val_loss: 13.8304 - val_mae: 2.6265\n",
      "Epoch 430/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7890 - mae: 0.6626 - val_loss: 13.7125 - val_mae: 2.6350\n",
      "Epoch 431/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7792 - mae: 0.6498 - val_loss: 14.3199 - val_mae: 2.5863\n",
      "Epoch 432/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6660 - mae: 0.6279 - val_loss: 13.7123 - val_mae: 2.5511\n",
      "Epoch 433/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8108 - mae: 0.6603 - val_loss: 14.1274 - val_mae: 2.6781\n",
      "Epoch 434/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7599 - mae: 0.6700 - val_loss: 14.8259 - val_mae: 2.7046\n",
      "Epoch 435/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6483 - mae: 0.6113 - val_loss: 13.1446 - val_mae: 2.5744\n",
      "Epoch 436/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6714 - mae: 0.5887 - val_loss: 13.2653 - val_mae: 2.5153\n",
      "Epoch 437/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7280 - mae: 0.6087 - val_loss: 13.7672 - val_mae: 2.6030\n",
      "Epoch 438/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7211 - mae: 0.6436 - val_loss: 13.6575 - val_mae: 2.6651\n",
      "Epoch 439/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7320 - mae: 0.6486 - val_loss: 12.9065 - val_mae: 2.5702\n",
      "Epoch 440/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6796 - mae: 0.6329 - val_loss: 13.9506 - val_mae: 2.6133\n",
      "Epoch 441/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7577 - mae: 0.6395 - val_loss: 13.7249 - val_mae: 2.5230\n",
      "Epoch 442/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7255 - mae: 0.6254 - val_loss: 12.7518 - val_mae: 2.5733\n",
      "Epoch 443/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7875 - mae: 0.6450 - val_loss: 13.2542 - val_mae: 2.5918\n",
      "Epoch 444/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7751 - mae: 0.6446 - val_loss: 14.6614 - val_mae: 2.6358\n",
      "Epoch 445/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6470 - mae: 0.6026 - val_loss: 12.9268 - val_mae: 2.5404\n",
      "Epoch 446/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6826 - mae: 0.6293 - val_loss: 13.3891 - val_mae: 2.5316\n",
      "Epoch 447/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7578 - mae: 0.6102 - val_loss: 13.4533 - val_mae: 2.5610\n",
      "Epoch 448/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7780 - mae: 0.6524 - val_loss: 14.1828 - val_mae: 2.5789\n",
      "Epoch 449/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7457 - mae: 0.6225 - val_loss: 13.9471 - val_mae: 2.6395\n",
      "Epoch 450/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7201 - mae: 0.6344 - val_loss: 15.2899 - val_mae: 2.6757\n",
      "Epoch 451/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7301 - mae: 0.6367 - val_loss: 14.2344 - val_mae: 2.5908\n",
      "Epoch 452/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6595 - mae: 0.6099 - val_loss: 13.5902 - val_mae: 2.5452\n",
      "Epoch 453/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7344 - mae: 0.6419 - val_loss: 12.4556 - val_mae: 2.4593\n",
      "Epoch 454/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6880 - mae: 0.6276 - val_loss: 14.0604 - val_mae: 2.6233\n",
      "Epoch 455/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6912 - mae: 0.6182 - val_loss: 13.3029 - val_mae: 2.5183\n",
      "Epoch 456/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8145 - mae: 0.6081 - val_loss: 13.6745 - val_mae: 2.5674\n",
      "Epoch 457/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7834 - mae: 0.6720 - val_loss: 12.9899 - val_mae: 2.5392\n",
      "Epoch 458/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6418 - mae: 0.5951 - val_loss: 13.1139 - val_mae: 2.5538\n",
      "Epoch 459/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7508 - mae: 0.6144 - val_loss: 13.5953 - val_mae: 2.6386\n",
      "Epoch 460/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6680 - mae: 0.5957 - val_loss: 13.1947 - val_mae: 2.5005\n",
      "Epoch 461/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5896 - mae: 0.5915 - val_loss: 14.8453 - val_mae: 2.6996\n",
      "Epoch 462/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7459 - mae: 0.6000 - val_loss: 13.4771 - val_mae: 2.5776\n",
      "Epoch 463/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7348 - mae: 0.6295 - val_loss: 13.9110 - val_mae: 2.5964\n",
      "Epoch 464/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5831 - mae: 0.5938 - val_loss: 13.1508 - val_mae: 2.5416\n",
      "Epoch 465/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7383 - mae: 0.6371 - val_loss: 14.4116 - val_mae: 2.6616\n",
      "Epoch 466/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6878 - mae: 0.6173 - val_loss: 14.0579 - val_mae: 2.6425\n",
      "Epoch 467/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7205 - mae: 0.5987 - val_loss: 12.9516 - val_mae: 2.5681\n",
      "Epoch 468/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6833 - mae: 0.6349 - val_loss: 13.5114 - val_mae: 2.5874\n",
      "Epoch 469/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7072 - mae: 0.5852 - val_loss: 13.5916 - val_mae: 2.5810\n",
      "Epoch 470/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6824 - mae: 0.6175 - val_loss: 14.1445 - val_mae: 2.6388\n",
      "Epoch 471/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6942 - mae: 0.6175 - val_loss: 13.5014 - val_mae: 2.5239\n",
      "Epoch 472/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6819 - mae: 0.5932 - val_loss: 13.4355 - val_mae: 2.5744\n",
      "Epoch 473/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6920 - mae: 0.6318 - val_loss: 13.5438 - val_mae: 2.5444\n",
      "Epoch 474/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6401 - mae: 0.5830 - val_loss: 13.4525 - val_mae: 2.5512\n",
      "Epoch 475/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6458 - mae: 0.5927 - val_loss: 13.4262 - val_mae: 2.6075\n",
      "Epoch 476/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6622 - mae: 0.6117 - val_loss: 12.7259 - val_mae: 2.5342\n",
      "Epoch 477/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6722 - mae: 0.6248 - val_loss: 13.6448 - val_mae: 2.5520\n",
      "Epoch 478/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6524 - mae: 0.6053 - val_loss: 13.5254 - val_mae: 2.5155\n",
      "Epoch 479/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6824 - mae: 0.6070 - val_loss: 13.2498 - val_mae: 2.5157\n",
      "Epoch 480/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6620 - mae: 0.6000 - val_loss: 13.9690 - val_mae: 2.6045\n",
      "Epoch 481/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6954 - mae: 0.5998 - val_loss: 13.0732 - val_mae: 2.4821\n",
      "Epoch 482/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7486 - mae: 0.5973 - val_loss: 13.1837 - val_mae: 2.4639\n",
      "Epoch 483/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6812 - mae: 0.6269 - val_loss: 13.7524 - val_mae: 2.6047\n",
      "Epoch 484/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6269 - mae: 0.5901 - val_loss: 13.5017 - val_mae: 2.6022\n",
      "Epoch 485/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6283 - mae: 0.5948 - val_loss: 13.9854 - val_mae: 2.5922\n",
      "Epoch 486/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6943 - mae: 0.6179 - val_loss: 13.7355 - val_mae: 2.6500\n",
      "Epoch 487/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6645 - mae: 0.5811 - val_loss: 13.0787 - val_mae: 2.5164\n",
      "Epoch 488/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6097 - mae: 0.5758 - val_loss: 12.4764 - val_mae: 2.4519\n",
      "Epoch 489/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6187 - mae: 0.5839 - val_loss: 13.3587 - val_mae: 2.6528\n",
      "Epoch 490/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6695 - mae: 0.6062 - val_loss: 13.4076 - val_mae: 2.5661\n",
      "Epoch 491/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5863 - mae: 0.5993 - val_loss: 14.9748 - val_mae: 2.6218\n",
      "Epoch 492/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6606 - mae: 0.6069 - val_loss: 14.5707 - val_mae: 2.5883\n",
      "Epoch 493/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7427 - mae: 0.6029 - val_loss: 13.1942 - val_mae: 2.4901\n",
      "Epoch 494/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6405 - mae: 0.6071 - val_loss: 13.5025 - val_mae: 2.5777\n",
      "Epoch 495/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7295 - mae: 0.6162 - val_loss: 12.7500 - val_mae: 2.4559\n",
      "Epoch 496/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6622 - mae: 0.5957 - val_loss: 12.7138 - val_mae: 2.5339\n",
      "Epoch 497/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7128 - mae: 0.5902 - val_loss: 13.7639 - val_mae: 2.5297\n",
      "Epoch 498/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6971 - mae: 0.5935 - val_loss: 13.9780 - val_mae: 2.6606\n",
      "Epoch 499/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6762 - mae: 0.6075 - val_loss: 12.8123 - val_mae: 2.5667\n",
      "Epoch 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6563 - mae: 0.6076 - val_loss: 13.5841 - val_mae: 2.5898\n",
      "처리중인 폴드: 3\n",
      "Train on 303 samples, validate on 101 samples\n",
      "Epoch 1/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 181.2792 - mae: 10.0099 - val_loss: 71.0099 - val_mae: 5.9144\n",
      "Epoch 2/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 30.6490 - mae: 3.6866 - val_loss: 36.3712 - val_mae: 3.9523\n",
      "Epoch 3/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 20.8679 - mae: 3.0247 - val_loss: 28.9456 - val_mae: 3.4059\n",
      "Epoch 4/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 16.7571 - mae: 2.6854 - val_loss: 24.5942 - val_mae: 3.2426\n",
      "Epoch 5/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 14.9198 - mae: 2.4829 - val_loss: 20.7112 - val_mae: 3.0124\n",
      "Epoch 6/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 13.4612 - mae: 2.3132 - val_loss: 20.0344 - val_mae: 3.1247\n",
      "Epoch 7/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 12.6401 - mae: 2.3958 - val_loss: 20.1650 - val_mae: 3.0688\n",
      "Epoch 8/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 12.1110 - mae: 2.2748 - val_loss: 17.9637 - val_mae: 2.7781\n",
      "Epoch 9/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 11.5573 - mae: 2.2348 - val_loss: 18.0921 - val_mae: 2.7542\n",
      "Epoch 10/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 11.3235 - mae: 2.2464 - val_loss: 15.2803 - val_mae: 2.5640\n",
      "Epoch 11/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.6178 - mae: 2.1167 - val_loss: 17.8290 - val_mae: 2.9278\n",
      "Epoch 12/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 10.2595 - mae: 2.1138 - val_loss: 17.8917 - val_mae: 2.8362\n",
      "Epoch 13/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.4747 - mae: 2.0762 - val_loss: 18.6921 - val_mae: 3.2444\n",
      "Epoch 14/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.9535 - mae: 2.1520 - val_loss: 15.1614 - val_mae: 2.6478\n",
      "Epoch 15/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.6484 - mae: 2.0264 - val_loss: 14.9944 - val_mae: 2.6161\n",
      "Epoch 16/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.1652 - mae: 2.0634 - val_loss: 14.8983 - val_mae: 2.6382\n",
      "Epoch 17/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 9.6098 - mae: 1.9926 - val_loss: 14.6629 - val_mae: 2.6923\n",
      "Epoch 18/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.7877 - mae: 2.0220 - val_loss: 13.4017 - val_mae: 2.4971\n",
      "Epoch 19/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.5526 - mae: 1.9936 - val_loss: 14.2833 - val_mae: 2.6748\n",
      "Epoch 20/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.3897 - mae: 1.9468 - val_loss: 12.9963 - val_mae: 2.4666\n",
      "Epoch 21/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.2009 - mae: 1.9328 - val_loss: 13.7713 - val_mae: 2.7319\n",
      "Epoch 22/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.4071 - mae: 1.9686 - val_loss: 14.7797 - val_mae: 2.7438\n",
      "Epoch 23/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.5473 - mae: 1.9106 - val_loss: 12.7861 - val_mae: 2.4952\n",
      "Epoch 24/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.9831 - mae: 1.9310 - val_loss: 12.4002 - val_mae: 2.4846\n",
      "Epoch 25/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.6551 - mae: 1.8189 - val_loss: 12.9357 - val_mae: 2.5807\n",
      "Epoch 26/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.5391 - mae: 1.9049 - val_loss: 13.0354 - val_mae: 2.5565\n",
      "Epoch 27/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.5090 - mae: 1.8691 - val_loss: 12.7155 - val_mae: 2.5016\n",
      "Epoch 28/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.9776 - mae: 1.8512 - val_loss: 12.2161 - val_mae: 2.4626\n",
      "Epoch 29/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.6070 - mae: 1.8886 - val_loss: 11.3765 - val_mae: 2.3346\n",
      "Epoch 30/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.5870 - mae: 1.8231 - val_loss: 13.1078 - val_mae: 2.6353\n",
      "Epoch 31/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.3414 - mae: 1.7614 - val_loss: 12.7334 - val_mae: 2.5067\n",
      "Epoch 32/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.2464 - mae: 1.8186 - val_loss: 12.2126 - val_mae: 2.4626\n",
      "Epoch 33/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.4899 - mae: 1.8336 - val_loss: 12.1836 - val_mae: 2.4415\n",
      "Epoch 34/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.0089 - mae: 1.7527 - val_loss: 12.4385 - val_mae: 2.4907\n",
      "Epoch 35/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.4159 - mae: 1.8115 - val_loss: 12.5005 - val_mae: 2.5208\n",
      "Epoch 36/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.9746 - mae: 1.7249 - val_loss: 12.1435 - val_mae: 2.4563\n",
      "Epoch 37/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.9235 - mae: 1.6885 - val_loss: 13.7286 - val_mae: 2.6366\n",
      "Epoch 38/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.9842 - mae: 1.7440 - val_loss: 12.9737 - val_mae: 2.5671\n",
      "Epoch 39/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.5570 - mae: 1.7375 - val_loss: 11.6896 - val_mae: 2.4136\n",
      "Epoch 40/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.4299 - mae: 1.6669 - val_loss: 11.9139 - val_mae: 2.4438\n",
      "Epoch 41/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.3775 - mae: 1.6909 - val_loss: 15.3746 - val_mae: 2.9430\n",
      "Epoch 42/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.7031 - mae: 1.7091 - val_loss: 12.3095 - val_mae: 2.4884\n",
      "Epoch 43/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.4671 - mae: 1.7266 - val_loss: 11.9471 - val_mae: 2.4640\n",
      "Epoch 44/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.3665 - mae: 1.6516 - val_loss: 12.3303 - val_mae: 2.4348\n",
      "Epoch 45/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.2340 - mae: 1.6826 - val_loss: 13.0737 - val_mae: 2.6576\n",
      "Epoch 46/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.5900 - mae: 1.7176 - val_loss: 12.1228 - val_mae: 2.4360\n",
      "Epoch 47/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.2151 - mae: 1.7234 - val_loss: 12.6695 - val_mae: 2.5263\n",
      "Epoch 48/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.0663 - mae: 1.6401 - val_loss: 12.4738 - val_mae: 2.4642\n",
      "Epoch 49/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.9025 - mae: 1.6714 - val_loss: 12.7481 - val_mae: 2.5172\n",
      "Epoch 50/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.1179 - mae: 1.6535 - val_loss: 11.9554 - val_mae: 2.4612\n",
      "Epoch 51/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.7087 - mae: 1.6197 - val_loss: 13.5675 - val_mae: 2.7208\n",
      "Epoch 52/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.8586 - mae: 1.6185 - val_loss: 12.1076 - val_mae: 2.3890\n",
      "Epoch 53/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.8448 - mae: 1.6607 - val_loss: 12.8837 - val_mae: 2.5546\n",
      "Epoch 54/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.9482 - mae: 1.6197 - val_loss: 12.4938 - val_mae: 2.5203\n",
      "Epoch 55/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.8527 - mae: 1.6006 - val_loss: 12.0857 - val_mae: 2.4491\n",
      "Epoch 56/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 6.1745 - mae: 1.5779 - val_loss: 12.5394 - val_mae: 2.4231\n",
      "Epoch 57/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.5517 - mae: 1.5742 - val_loss: 13.1187 - val_mae: 2.5541\n",
      "Epoch 58/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.5237 - mae: 1.5486 - val_loss: 12.8739 - val_mae: 2.5481\n",
      "Epoch 59/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.4453 - mae: 1.5778 - val_loss: 13.0668 - val_mae: 2.7520\n",
      "Epoch 60/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.5374 - mae: 1.5850 - val_loss: 12.4442 - val_mae: 2.4975\n",
      "Epoch 61/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.3945 - mae: 1.5764 - val_loss: 13.3806 - val_mae: 2.5599\n",
      "Epoch 62/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.5667 - mae: 1.5913 - val_loss: 11.8406 - val_mae: 2.3641\n",
      "Epoch 63/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.1023 - mae: 1.5212 - val_loss: 11.9772 - val_mae: 2.4789\n",
      "Epoch 64/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.3670 - mae: 1.5443 - val_loss: 12.6248 - val_mae: 2.6105\n",
      "Epoch 65/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.2115 - mae: 1.4950 - val_loss: 12.5117 - val_mae: 2.5038\n",
      "Epoch 66/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.3683 - mae: 1.5744 - val_loss: 12.0653 - val_mae: 2.4923\n",
      "Epoch 67/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8725 - mae: 1.5222 - val_loss: 11.2810 - val_mae: 2.4273\n",
      "Epoch 68/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0392 - mae: 1.4988 - val_loss: 11.6876 - val_mae: 2.4739\n",
      "Epoch 69/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8565 - mae: 1.4801 - val_loss: 11.5425 - val_mae: 2.3968\n",
      "Epoch 70/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0298 - mae: 1.4556 - val_loss: 11.3946 - val_mae: 2.4581\n",
      "Epoch 71/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0672 - mae: 1.5027 - val_loss: 13.5961 - val_mae: 2.7210\n",
      "Epoch 72/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0046 - mae: 1.5059 - val_loss: 11.8947 - val_mae: 2.4888\n",
      "Epoch 73/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 5.0686 - mae: 1.4999 - val_loss: 11.7855 - val_mae: 2.3925\n",
      "Epoch 74/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.6892 - mae: 1.5017 - val_loss: 11.3930 - val_mae: 2.4011\n",
      "Epoch 75/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.9441 - mae: 1.4847 - val_loss: 14.3640 - val_mae: 2.9110\n",
      "Epoch 76/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.6565 - mae: 1.3761 - val_loss: 13.7075 - val_mae: 2.8222\n",
      "Epoch 77/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.5284 - mae: 1.4328 - val_loss: 13.2925 - val_mae: 2.6079\n",
      "Epoch 78/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.6799 - mae: 1.4400 - val_loss: 11.7628 - val_mae: 2.4178\n",
      "Epoch 79/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.8813 - mae: 1.4555 - val_loss: 10.9982 - val_mae: 2.4163\n",
      "Epoch 80/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.5426 - mae: 1.4504 - val_loss: 11.0287 - val_mae: 2.3569\n",
      "Epoch 81/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.7356 - mae: 1.4208 - val_loss: 11.5880 - val_mae: 2.3927\n",
      "Epoch 82/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.6015 - mae: 1.3822 - val_loss: 13.6059 - val_mae: 2.6801\n",
      "Epoch 83/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3984 - mae: 1.3614 - val_loss: 11.9400 - val_mae: 2.4339\n",
      "Epoch 84/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4811 - mae: 1.3844 - val_loss: 10.8532 - val_mae: 2.3979\n",
      "Epoch 85/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3188 - mae: 1.3520 - val_loss: 10.5702 - val_mae: 2.4213\n",
      "Epoch 86/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4408 - mae: 1.3670 - val_loss: 11.7542 - val_mae: 2.4061\n",
      "Epoch 87/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.5809 - mae: 1.4173 - val_loss: 12.4915 - val_mae: 2.5096\n",
      "Epoch 88/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2592 - mae: 1.3604 - val_loss: 13.6163 - val_mae: 2.6734\n",
      "Epoch 89/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.5020 - mae: 1.3650 - val_loss: 12.7714 - val_mae: 2.5451\n",
      "Epoch 90/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3639 - mae: 1.3690 - val_loss: 11.9299 - val_mae: 2.5163\n",
      "Epoch 91/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.3851 - mae: 1.4069 - val_loss: 12.3524 - val_mae: 2.6451\n",
      "Epoch 92/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4016 - mae: 1.3755 - val_loss: 12.7678 - val_mae: 2.5441\n",
      "Epoch 93/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2961 - mae: 1.3142 - val_loss: 12.4454 - val_mae: 2.4705\n",
      "Epoch 94/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8975 - mae: 1.3419 - val_loss: 11.8415 - val_mae: 2.4446\n",
      "Epoch 95/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0345 - mae: 1.2936 - val_loss: 12.8856 - val_mae: 2.5051\n",
      "Epoch 96/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2151 - mae: 1.2976 - val_loss: 13.2395 - val_mae: 2.7487\n",
      "Epoch 97/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.9526 - mae: 1.3189 - val_loss: 13.1138 - val_mae: 2.6530\n",
      "Epoch 98/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.9998 - mae: 1.3078 - val_loss: 13.6860 - val_mae: 2.5829\n",
      "Epoch 99/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8611 - mae: 1.3729 - val_loss: 11.8301 - val_mae: 2.4207\n",
      "Epoch 100/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.4339 - mae: 1.2860 - val_loss: 12.1777 - val_mae: 2.4287\n",
      "Epoch 101/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8195 - mae: 1.2663 - val_loss: 12.2557 - val_mae: 2.4359\n",
      "Epoch 102/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0323 - mae: 1.3149 - val_loss: 12.7205 - val_mae: 2.7280\n",
      "Epoch 103/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.2021 - mae: 1.3619 - val_loss: 12.3927 - val_mae: 2.4435\n",
      "Epoch 104/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0701 - mae: 1.3411 - val_loss: 12.2655 - val_mae: 2.6530\n",
      "Epoch 105/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0613 - mae: 1.3366 - val_loss: 11.9623 - val_mae: 2.4432\n",
      "Epoch 106/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8110 - mae: 1.2750 - val_loss: 12.9380 - val_mae: 2.6255\n",
      "Epoch 107/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7926 - mae: 1.3197 - val_loss: 13.3929 - val_mae: 2.7517\n",
      "Epoch 108/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8960 - mae: 1.2786 - val_loss: 11.8863 - val_mae: 2.5042\n",
      "Epoch 109/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7336 - mae: 1.2758 - val_loss: 11.6300 - val_mae: 2.5041\n",
      "Epoch 110/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 4.0618 - mae: 1.3328 - val_loss: 12.3875 - val_mae: 2.5915\n",
      "Epoch 111/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.9032 - mae: 1.3280 - val_loss: 12.1914 - val_mae: 2.4760\n",
      "Epoch 112/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7491 - mae: 1.2573 - val_loss: 12.2667 - val_mae: 2.4926\n",
      "Epoch 113/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8615 - mae: 1.3069 - val_loss: 15.5147 - val_mae: 2.8788\n",
      "Epoch 114/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6625 - mae: 1.2725 - val_loss: 13.0020 - val_mae: 2.6011\n",
      "Epoch 115/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.5049 - mae: 1.2216 - val_loss: 12.9331 - val_mae: 2.5479\n",
      "Epoch 116/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6993 - mae: 1.2191 - val_loss: 12.4341 - val_mae: 2.5988\n",
      "Epoch 117/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6204 - mae: 1.2749 - val_loss: 13.0666 - val_mae: 2.6219\n",
      "Epoch 118/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8143 - mae: 1.2847 - val_loss: 12.2862 - val_mae: 2.5570\n",
      "Epoch 119/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6514 - mae: 1.2672 - val_loss: 14.9933 - val_mae: 2.8389\n",
      "Epoch 120/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6105 - mae: 1.2369 - val_loss: 13.7387 - val_mae: 2.7549\n",
      "Epoch 121/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.8481 - mae: 1.2191 - val_loss: 11.7357 - val_mae: 2.4716\n",
      "Epoch 122/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6910 - mae: 1.2359 - val_loss: 11.8983 - val_mae: 2.5181\n",
      "Epoch 123/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6171 - mae: 1.1875 - val_loss: 12.9138 - val_mae: 2.6740\n",
      "Epoch 124/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.7982 - mae: 1.2674 - val_loss: 11.7511 - val_mae: 2.6181\n",
      "Epoch 125/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6894 - mae: 1.2375 - val_loss: 12.8292 - val_mae: 2.6725\n",
      "Epoch 126/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3367 - mae: 1.1846 - val_loss: 13.4072 - val_mae: 2.6569\n",
      "Epoch 127/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2560 - mae: 1.2093 - val_loss: 13.0210 - val_mae: 2.6261\n",
      "Epoch 128/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6023 - mae: 1.2670 - val_loss: 13.2011 - val_mae: 2.6346\n",
      "Epoch 129/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.6415 - mae: 1.1965 - val_loss: 14.1347 - val_mae: 2.6780\n",
      "Epoch 130/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3313 - mae: 1.2496 - val_loss: 13.2968 - val_mae: 2.6342\n",
      "Epoch 131/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2655 - mae: 1.1707 - val_loss: 14.1533 - val_mae: 2.7472\n",
      "Epoch 132/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4321 - mae: 1.1754 - val_loss: 13.5662 - val_mae: 2.6338\n",
      "Epoch 133/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4769 - mae: 1.2032 - val_loss: 12.1935 - val_mae: 2.4647\n",
      "Epoch 134/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3681 - mae: 1.2284 - val_loss: 14.2992 - val_mae: 2.7413\n",
      "Epoch 135/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2566 - mae: 1.2302 - val_loss: 13.7091 - val_mae: 2.7702\n",
      "Epoch 136/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4750 - mae: 1.2131 - val_loss: 13.3935 - val_mae: 2.7356\n",
      "Epoch 137/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1742 - mae: 1.1203 - val_loss: 13.5915 - val_mae: 2.6442\n",
      "Epoch 138/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0909 - mae: 1.1636 - val_loss: 13.0433 - val_mae: 2.5981\n",
      "Epoch 139/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2991 - mae: 1.1315 - val_loss: 12.8742 - val_mae: 2.6437\n",
      "Epoch 140/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2569 - mae: 1.1988 - val_loss: 16.0486 - val_mae: 2.9623\n",
      "Epoch 141/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2650 - mae: 1.1519 - val_loss: 12.4870 - val_mae: 2.5834\n",
      "Epoch 142/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1562 - mae: 1.1829 - val_loss: 12.4466 - val_mae: 2.6602\n",
      "Epoch 143/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2567 - mae: 1.1716 - val_loss: 13.3332 - val_mae: 2.6625\n",
      "Epoch 144/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.4973 - mae: 1.1705 - val_loss: 12.4817 - val_mae: 2.5995\n",
      "Epoch 145/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.3669 - mae: 1.2024 - val_loss: 12.8626 - val_mae: 2.6184\n",
      "Epoch 146/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1432 - mae: 1.1401 - val_loss: 13.1823 - val_mae: 2.6908\n",
      "Epoch 147/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0988 - mae: 1.1905 - val_loss: 14.5283 - val_mae: 2.7937\n",
      "Epoch 148/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1746 - mae: 1.2169 - val_loss: 13.1469 - val_mae: 2.6347\n",
      "Epoch 149/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0294 - mae: 1.1250 - val_loss: 13.1532 - val_mae: 2.7060\n",
      "Epoch 150/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0366 - mae: 1.1587 - val_loss: 12.7988 - val_mae: 2.6096\n",
      "Epoch 151/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1659 - mae: 1.2252 - val_loss: 15.0171 - val_mae: 2.9520\n",
      "Epoch 152/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.1629 - mae: 1.1563 - val_loss: 13.6102 - val_mae: 2.7255\n",
      "Epoch 153/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2159 - mae: 1.1277 - val_loss: 11.6510 - val_mae: 2.5057\n",
      "Epoch 154/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7828 - mae: 1.1343 - val_loss: 14.6504 - val_mae: 2.7474\n",
      "Epoch 155/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9369 - mae: 1.1219 - val_loss: 13.0042 - val_mae: 2.7009\n",
      "Epoch 156/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.2221 - mae: 1.1795 - val_loss: 14.4027 - val_mae: 2.8190\n",
      "Epoch 157/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9294 - mae: 1.1399 - val_loss: 14.5486 - val_mae: 2.8182\n",
      "Epoch 158/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9655 - mae: 1.1478 - val_loss: 14.7654 - val_mae: 2.8077\n",
      "Epoch 159/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8987 - mae: 1.1605 - val_loss: 14.8470 - val_mae: 2.8524\n",
      "Epoch 160/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9667 - mae: 1.1139 - val_loss: 13.0305 - val_mae: 2.5736\n",
      "Epoch 161/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8106 - mae: 1.1435 - val_loss: 12.9266 - val_mae: 2.6105\n",
      "Epoch 162/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8532 - mae: 1.0959 - val_loss: 14.2987 - val_mae: 2.7871\n",
      "Epoch 163/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0249 - mae: 1.1087 - val_loss: 12.5990 - val_mae: 2.5815\n",
      "Epoch 164/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8615 - mae: 1.1416 - val_loss: 13.5452 - val_mae: 2.6955\n",
      "Epoch 165/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8072 - mae: 1.1232 - val_loss: 13.5015 - val_mae: 2.6364\n",
      "Epoch 166/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9463 - mae: 1.0953 - val_loss: 14.5594 - val_mae: 2.7954\n",
      "Epoch 167/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9590 - mae: 1.1180 - val_loss: 13.5726 - val_mae: 2.6951\n",
      "Epoch 168/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9688 - mae: 1.1234 - val_loss: 14.7211 - val_mae: 2.8051\n",
      "Epoch 169/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6859 - mae: 1.0884 - val_loss: 14.9958 - val_mae: 2.8018\n",
      "Epoch 170/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8163 - mae: 1.1005 - val_loss: 17.0948 - val_mae: 3.1304\n",
      "Epoch 171/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8391 - mae: 1.1139 - val_loss: 14.7380 - val_mae: 2.8385\n",
      "Epoch 172/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9560 - mae: 1.0800 - val_loss: 14.6736 - val_mae: 2.7685\n",
      "Epoch 173/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8932 - mae: 1.1328 - val_loss: 13.7561 - val_mae: 2.7096\n",
      "Epoch 174/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6956 - mae: 1.0955 - val_loss: 13.7245 - val_mae: 2.7577\n",
      "Epoch 175/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8575 - mae: 1.1234 - val_loss: 15.2525 - val_mae: 2.8305\n",
      "Epoch 176/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5980 - mae: 1.0563 - val_loss: 16.4018 - val_mae: 2.9416\n",
      "Epoch 177/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.9899 - mae: 1.1049 - val_loss: 13.8081 - val_mae: 2.7881\n",
      "Epoch 178/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 3.0445 - mae: 1.1138 - val_loss: 14.7486 - val_mae: 2.8727\n",
      "Epoch 179/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7382 - mae: 1.1005 - val_loss: 14.1234 - val_mae: 2.7138\n",
      "Epoch 180/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8219 - mae: 1.0680 - val_loss: 14.6539 - val_mae: 2.7767\n",
      "Epoch 181/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7467 - mae: 1.0817 - val_loss: 13.9497 - val_mae: 2.6743\n",
      "Epoch 182/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6305 - mae: 1.0699 - val_loss: 15.4405 - val_mae: 2.8540\n",
      "Epoch 183/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5579 - mae: 1.0667 - val_loss: 15.6990 - val_mae: 2.8148\n",
      "Epoch 184/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5508 - mae: 1.0748 - val_loss: 13.7370 - val_mae: 2.7285\n",
      "Epoch 185/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5899 - mae: 1.0877 - val_loss: 14.2904 - val_mae: 2.7185\n",
      "Epoch 186/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5135 - mae: 1.0814 - val_loss: 16.4084 - val_mae: 2.9439\n",
      "Epoch 187/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5291 - mae: 1.0343 - val_loss: 14.1273 - val_mae: 2.6912\n",
      "Epoch 188/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6259 - mae: 1.0883 - val_loss: 16.6955 - val_mae: 3.0709\n",
      "Epoch 189/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4374 - mae: 1.0785 - val_loss: 15.9211 - val_mae: 2.9821\n",
      "Epoch 190/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.7617 - mae: 1.0669 - val_loss: 14.9992 - val_mae: 2.8165\n",
      "Epoch 191/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5714 - mae: 1.0951 - val_loss: 14.9567 - val_mae: 2.8628\n",
      "Epoch 192/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6161 - mae: 1.0930 - val_loss: 15.1623 - val_mae: 2.7874\n",
      "Epoch 193/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6013 - mae: 1.0348 - val_loss: 16.3158 - val_mae: 2.9310\n",
      "Epoch 194/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.6384 - mae: 0.9989 - val_loss: 13.9268 - val_mae: 2.6588\n",
      "Epoch 195/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5092 - mae: 1.0341 - val_loss: 14.5351 - val_mae: 2.7248\n",
      "Epoch 196/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3725 - mae: 1.0903 - val_loss: 15.5989 - val_mae: 2.8575\n",
      "Epoch 197/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5126 - mae: 1.0115 - val_loss: 15.1619 - val_mae: 2.8852\n",
      "Epoch 198/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4940 - mae: 1.0761 - val_loss: 14.8211 - val_mae: 2.7762\n",
      "Epoch 199/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1190 - mae: 0.9735 - val_loss: 15.1104 - val_mae: 2.7855\n",
      "Epoch 200/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3309 - mae: 1.0724 - val_loss: 14.0986 - val_mae: 2.7350\n",
      "Epoch 201/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3048 - mae: 1.0013 - val_loss: 15.6499 - val_mae: 2.8781\n",
      "Epoch 202/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2367 - mae: 1.0326 - val_loss: 18.2556 - val_mae: 3.1642\n",
      "Epoch 203/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4237 - mae: 1.0611 - val_loss: 14.9059 - val_mae: 2.7872\n",
      "Epoch 204/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.5907 - mae: 1.1006 - val_loss: 16.3180 - val_mae: 2.9167\n",
      "Epoch 205/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1550 - mae: 0.9912 - val_loss: 15.5306 - val_mae: 2.8459\n",
      "Epoch 206/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2231 - mae: 1.0236 - val_loss: 15.4339 - val_mae: 2.8729\n",
      "Epoch 207/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2710 - mae: 1.0228 - val_loss: 14.9899 - val_mae: 2.7960\n",
      "Epoch 208/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2359 - mae: 0.9649 - val_loss: 15.1441 - val_mae: 2.8599\n",
      "Epoch 209/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3174 - mae: 1.0237 - val_loss: 16.5927 - val_mae: 2.9791\n",
      "Epoch 210/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2569 - mae: 1.0143 - val_loss: 14.2163 - val_mae: 2.7531\n",
      "Epoch 211/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2113 - mae: 1.0416 - val_loss: 14.4670 - val_mae: 2.7635\n",
      "Epoch 212/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.3909 - mae: 1.0378 - val_loss: 16.4476 - val_mae: 2.9860\n",
      "Epoch 213/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0664 - mae: 1.0219 - val_loss: 17.0880 - val_mae: 3.0539\n",
      "Epoch 214/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.4486 - mae: 1.0273 - val_loss: 15.1610 - val_mae: 2.8274\n",
      "Epoch 215/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1728 - mae: 0.9953 - val_loss: 14.5122 - val_mae: 2.7656\n",
      "Epoch 216/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9967 - mae: 1.0023 - val_loss: 13.9655 - val_mae: 2.6648\n",
      "Epoch 217/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0030 - mae: 0.9872 - val_loss: 16.7621 - val_mae: 3.0186\n",
      "Epoch 218/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2340 - mae: 0.9778 - val_loss: 17.4603 - val_mae: 3.1860\n",
      "Epoch 219/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2698 - mae: 1.0602 - val_loss: 14.7602 - val_mae: 2.7237\n",
      "Epoch 220/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2182 - mae: 1.0128 - val_loss: 15.4836 - val_mae: 2.8368\n",
      "Epoch 221/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1944 - mae: 1.0176 - val_loss: 16.1220 - val_mae: 2.9258\n",
      "Epoch 222/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9980 - mae: 1.0022 - val_loss: 15.0959 - val_mae: 2.8463\n",
      "Epoch 223/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1649 - mae: 0.9716 - val_loss: 14.1535 - val_mae: 2.6778\n",
      "Epoch 224/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1874 - mae: 1.0266 - val_loss: 14.7368 - val_mae: 2.7440\n",
      "Epoch 225/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0955 - mae: 0.9751 - val_loss: 16.1057 - val_mae: 2.9119\n",
      "Epoch 226/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8625 - mae: 0.9926 - val_loss: 16.6404 - val_mae: 3.0130\n",
      "Epoch 227/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0799 - mae: 1.0006 - val_loss: 15.7026 - val_mae: 2.8886\n",
      "Epoch 228/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9422 - mae: 0.9289 - val_loss: 14.2356 - val_mae: 2.6928\n",
      "Epoch 229/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9492 - mae: 0.9277 - val_loss: 15.4014 - val_mae: 2.7973\n",
      "Epoch 230/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9750 - mae: 0.9570 - val_loss: 17.7436 - val_mae: 3.0513\n",
      "Epoch 231/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1294 - mae: 0.9604 - val_loss: 16.4662 - val_mae: 2.9299\n",
      "Epoch 232/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9338 - mae: 0.9325 - val_loss: 16.0076 - val_mae: 2.8234\n",
      "Epoch 233/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1444 - mae: 0.9468 - val_loss: 15.1877 - val_mae: 2.8248\n",
      "Epoch 234/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9105 - mae: 0.9702 - val_loss: 15.3580 - val_mae: 2.7573\n",
      "Epoch 235/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2679 - mae: 1.0384 - val_loss: 15.7888 - val_mae: 2.9195\n",
      "Epoch 236/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.1056 - mae: 0.9857 - val_loss: 14.0750 - val_mae: 2.7254\n",
      "Epoch 237/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0629 - mae: 0.9766 - val_loss: 16.0443 - val_mae: 2.8456\n",
      "Epoch 238/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8833 - mae: 0.9377 - val_loss: 16.4385 - val_mae: 2.9125\n",
      "Epoch 239/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8165 - mae: 0.9488 - val_loss: 14.5070 - val_mae: 2.7456\n",
      "Epoch 240/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0412 - mae: 1.0071 - val_loss: 14.0877 - val_mae: 2.6384\n",
      "Epoch 241/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0469 - mae: 0.9653 - val_loss: 16.0104 - val_mae: 2.9264\n",
      "Epoch 242/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9079 - mae: 0.9643 - val_loss: 15.8165 - val_mae: 2.8733\n",
      "Epoch 243/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9281 - mae: 0.9794 - val_loss: 16.2231 - val_mae: 2.8836\n",
      "Epoch 244/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9243 - mae: 0.9603 - val_loss: 14.1311 - val_mae: 2.6282\n",
      "Epoch 245/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8394 - mae: 0.9820 - val_loss: 14.5875 - val_mae: 2.7270\n",
      "Epoch 246/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9790 - mae: 0.9677 - val_loss: 16.3006 - val_mae: 2.9417\n",
      "Epoch 247/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9292 - mae: 0.9453 - val_loss: 14.9081 - val_mae: 2.7346\n",
      "Epoch 248/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8836 - mae: 0.9502 - val_loss: 15.7871 - val_mae: 2.8299\n",
      "Epoch 249/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8449 - mae: 0.9905 - val_loss: 14.1257 - val_mae: 2.7296\n",
      "Epoch 250/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9498 - mae: 0.9900 - val_loss: 16.3822 - val_mae: 2.9958\n",
      "Epoch 251/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6901 - mae: 0.9348 - val_loss: 15.0413 - val_mae: 2.7391\n",
      "Epoch 252/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.2250 - mae: 0.9628 - val_loss: 15.2486 - val_mae: 2.8002\n",
      "Epoch 253/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8939 - mae: 0.9602 - val_loss: 14.8643 - val_mae: 2.7240\n",
      "Epoch 254/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7519 - mae: 0.9095 - val_loss: 16.1966 - val_mae: 2.8962\n",
      "Epoch 255/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9669 - mae: 0.9833 - val_loss: 15.5887 - val_mae: 2.8871\n",
      "Epoch 256/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9829 - mae: 0.9615 - val_loss: 15.6395 - val_mae: 2.8755\n",
      "Epoch 257/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8100 - mae: 0.9401 - val_loss: 16.0998 - val_mae: 2.9468\n",
      "Epoch 258/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8603 - mae: 0.9361 - val_loss: 16.4878 - val_mae: 2.9054\n",
      "Epoch 259/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7031 - mae: 0.9237 - val_loss: 14.2525 - val_mae: 2.6575\n",
      "Epoch 260/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7940 - mae: 0.9492 - val_loss: 16.3264 - val_mae: 2.8656\n",
      "Epoch 261/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7924 - mae: 0.8904 - val_loss: 16.1304 - val_mae: 2.9962\n",
      "Epoch 262/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7837 - mae: 0.9384 - val_loss: 16.2928 - val_mae: 2.9154\n",
      "Epoch 263/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6560 - mae: 0.9346 - val_loss: 14.6018 - val_mae: 2.8024\n",
      "Epoch 264/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6234 - mae: 0.8995 - val_loss: 16.0882 - val_mae: 2.9039\n",
      "Epoch 265/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0078 - mae: 0.9864 - val_loss: 14.9994 - val_mae: 2.7787\n",
      "Epoch 266/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8157 - mae: 0.9513 - val_loss: 15.0046 - val_mae: 2.7325\n",
      "Epoch 267/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7253 - mae: 0.9288 - val_loss: 17.0504 - val_mae: 3.0468\n",
      "Epoch 268/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8032 - mae: 0.9665 - val_loss: 16.1671 - val_mae: 2.9204\n",
      "Epoch 269/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7633 - mae: 0.9418 - val_loss: 15.6877 - val_mae: 2.8706\n",
      "Epoch 270/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7634 - mae: 0.9323 - val_loss: 17.2882 - val_mae: 2.9851\n",
      "Epoch 271/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6705 - mae: 0.8976 - val_loss: 15.5754 - val_mae: 2.8382\n",
      "Epoch 272/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7268 - mae: 0.9137 - val_loss: 16.0752 - val_mae: 2.9750\n",
      "Epoch 273/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6569 - mae: 0.9207 - val_loss: 16.5159 - val_mae: 2.9043\n",
      "Epoch 274/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6970 - mae: 0.9299 - val_loss: 17.3276 - val_mae: 2.9901\n",
      "Epoch 275/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6483 - mae: 0.9073 - val_loss: 14.8204 - val_mae: 2.7394\n",
      "Epoch 276/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6559 - mae: 0.9168 - val_loss: 15.5430 - val_mae: 2.7681\n",
      "Epoch 277/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7636 - mae: 0.9172 - val_loss: 15.6002 - val_mae: 2.8062\n",
      "Epoch 278/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6743 - mae: 0.9495 - val_loss: 17.3334 - val_mae: 3.0092\n",
      "Epoch 279/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.8330 - mae: 0.9585 - val_loss: 16.9008 - val_mae: 2.9228\n",
      "Epoch 280/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7127 - mae: 0.9359 - val_loss: 16.3805 - val_mae: 2.9869\n",
      "Epoch 281/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6662 - mae: 0.9161 - val_loss: 15.8631 - val_mae: 2.8714\n",
      "Epoch 282/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6016 - mae: 0.9130 - val_loss: 14.6504 - val_mae: 2.6663\n",
      "Epoch 283/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5593 - mae: 0.9157 - val_loss: 16.9696 - val_mae: 3.0829\n",
      "Epoch 284/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6532 - mae: 0.9127 - val_loss: 15.3050 - val_mae: 2.7463\n",
      "Epoch 285/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5679 - mae: 0.8702 - val_loss: 16.0990 - val_mae: 2.8805\n",
      "Epoch 286/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4541 - mae: 0.8834 - val_loss: 16.8384 - val_mae: 2.9231\n",
      "Epoch 287/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6357 - mae: 0.9175 - val_loss: 15.7285 - val_mae: 2.8112\n",
      "Epoch 288/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4595 - mae: 0.8721 - val_loss: 16.1437 - val_mae: 2.8624\n",
      "Epoch 289/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5643 - mae: 0.9155 - val_loss: 16.4817 - val_mae: 2.8789\n",
      "Epoch 290/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5361 - mae: 0.8848 - val_loss: 15.3668 - val_mae: 2.7959\n",
      "Epoch 291/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5876 - mae: 0.8624 - val_loss: 15.6874 - val_mae: 2.8173\n",
      "Epoch 292/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5847 - mae: 0.9067 - val_loss: 16.2859 - val_mae: 2.8062\n",
      "Epoch 293/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5763 - mae: 0.8664 - val_loss: 14.9367 - val_mae: 2.7646\n",
      "Epoch 294/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.6102 - mae: 0.8959 - val_loss: 15.4525 - val_mae: 2.7496\n",
      "Epoch 295/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5494 - mae: 0.8669 - val_loss: 15.8744 - val_mae: 2.7904\n",
      "Epoch 296/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5725 - mae: 0.8841 - val_loss: 14.4765 - val_mae: 2.6089\n",
      "Epoch 297/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5874 - mae: 0.9043 - val_loss: 15.9898 - val_mae: 2.7999\n",
      "Epoch 298/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4621 - mae: 0.9020 - val_loss: 16.0235 - val_mae: 2.8388\n",
      "Epoch 299/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5922 - mae: 0.9209 - val_loss: 15.5085 - val_mae: 2.7781\n",
      "Epoch 300/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4414 - mae: 0.8690 - val_loss: 16.3086 - val_mae: 2.8704\n",
      "Epoch 301/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5883 - mae: 0.8808 - val_loss: 15.6737 - val_mae: 2.8626\n",
      "Epoch 302/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5580 - mae: 0.8728 - val_loss: 15.8243 - val_mae: 2.8638\n",
      "Epoch 303/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5064 - mae: 0.9141 - val_loss: 14.6064 - val_mae: 2.7411\n",
      "Epoch 304/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5221 - mae: 0.8803 - val_loss: 15.1004 - val_mae: 2.7895\n",
      "Epoch 305/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4184 - mae: 0.8557 - val_loss: 14.4219 - val_mae: 2.6653\n",
      "Epoch 306/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3208 - mae: 0.8551 - val_loss: 16.3528 - val_mae: 2.8964\n",
      "Epoch 307/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5926 - mae: 0.8875 - val_loss: 15.1169 - val_mae: 2.7954\n",
      "Epoch 308/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5101 - mae: 0.8657 - val_loss: 13.9902 - val_mae: 2.5647\n",
      "Epoch 309/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3507 - mae: 0.8694 - val_loss: 14.2376 - val_mae: 2.6170\n",
      "Epoch 310/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4511 - mae: 0.8838 - val_loss: 15.2421 - val_mae: 2.7375\n",
      "Epoch 311/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4131 - mae: 0.8703 - val_loss: 15.1161 - val_mae: 2.7256\n",
      "Epoch 312/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4836 - mae: 0.8801 - val_loss: 15.5182 - val_mae: 2.7973\n",
      "Epoch 313/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2613 - mae: 0.8003 - val_loss: 17.8702 - val_mae: 3.0027\n",
      "Epoch 314/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3858 - mae: 0.8756 - val_loss: 15.7296 - val_mae: 2.8654\n",
      "Epoch 315/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3663 - mae: 0.8782 - val_loss: 15.7432 - val_mae: 2.8312\n",
      "Epoch 316/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3877 - mae: 0.8532 - val_loss: 17.0529 - val_mae: 2.9726\n",
      "Epoch 317/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4195 - mae: 0.8402 - val_loss: 14.5310 - val_mae: 2.6533\n",
      "Epoch 318/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4925 - mae: 0.8470 - val_loss: 14.7702 - val_mae: 2.7257\n",
      "Epoch 319/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3295 - mae: 0.8530 - val_loss: 14.4926 - val_mae: 2.6667\n",
      "Epoch 320/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3621 - mae: 0.8782 - val_loss: 16.7055 - val_mae: 2.9774\n",
      "Epoch 321/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3716 - mae: 0.8312 - val_loss: 17.4122 - val_mae: 2.9312\n",
      "Epoch 322/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3643 - mae: 0.8766 - val_loss: 15.9330 - val_mae: 2.8771\n",
      "Epoch 323/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3319 - mae: 0.8230 - val_loss: 16.2362 - val_mae: 2.9206\n",
      "Epoch 324/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2760 - mae: 0.7909 - val_loss: 15.5130 - val_mae: 2.8621\n",
      "Epoch 325/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3522 - mae: 0.8388 - val_loss: 13.1073 - val_mae: 2.5958\n",
      "Epoch 326/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2417 - mae: 0.8330 - val_loss: 14.6448 - val_mae: 2.7480\n",
      "Epoch 327/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3226 - mae: 0.8442 - val_loss: 14.6776 - val_mae: 2.7741\n",
      "Epoch 328/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3064 - mae: 0.8424 - val_loss: 15.3394 - val_mae: 2.9219\n",
      "Epoch 329/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3141 - mae: 0.8295 - val_loss: 15.3708 - val_mae: 2.8539\n",
      "Epoch 330/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3186 - mae: 0.8568 - val_loss: 16.2269 - val_mae: 2.9055\n",
      "Epoch 331/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3261 - mae: 0.8498 - val_loss: 17.0521 - val_mae: 2.9148\n",
      "Epoch 332/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2804 - mae: 0.8109 - val_loss: 15.8661 - val_mae: 2.9000\n",
      "Epoch 333/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.5088 - mae: 0.8552 - val_loss: 14.9175 - val_mae: 2.7857\n",
      "Epoch 334/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4330 - mae: 0.8500 - val_loss: 15.0091 - val_mae: 2.7471\n",
      "Epoch 335/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2059 - mae: 0.7979 - val_loss: 15.4108 - val_mae: 2.8359\n",
      "Epoch 336/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3541 - mae: 0.7986 - val_loss: 15.8719 - val_mae: 2.9099\n",
      "Epoch 337/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4106 - mae: 0.8480 - val_loss: 15.7832 - val_mae: 2.7754\n",
      "Epoch 338/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1694 - mae: 0.7831 - val_loss: 16.1522 - val_mae: 2.8715\n",
      "Epoch 339/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3286 - mae: 0.8658 - val_loss: 14.6888 - val_mae: 2.6789\n",
      "Epoch 340/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3870 - mae: 0.8186 - val_loss: 15.3932 - val_mae: 2.7000\n",
      "Epoch 341/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2828 - mae: 0.8387 - val_loss: 15.8357 - val_mae: 2.8487\n",
      "Epoch 342/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2956 - mae: 0.8304 - val_loss: 16.5316 - val_mae: 2.8812\n",
      "Epoch 343/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2836 - mae: 0.7997 - val_loss: 14.0017 - val_mae: 2.6029\n",
      "Epoch 344/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2714 - mae: 0.8407 - val_loss: 13.9770 - val_mae: 2.5887\n",
      "Epoch 345/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2948 - mae: 0.8043 - val_loss: 15.0049 - val_mae: 2.7507\n",
      "Epoch 346/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3415 - mae: 0.7889 - val_loss: 15.3452 - val_mae: 2.7664\n",
      "Epoch 347/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3505 - mae: 0.7946 - val_loss: 15.0039 - val_mae: 2.7519\n",
      "Epoch 348/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4186 - mae: 0.8518 - val_loss: 15.0563 - val_mae: 2.7706\n",
      "Epoch 349/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3392 - mae: 0.8387 - val_loss: 13.8710 - val_mae: 2.6774\n",
      "Epoch 350/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.4448 - mae: 0.8295 - val_loss: 13.5793 - val_mae: 2.6177\n",
      "Epoch 351/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3062 - mae: 0.8628 - val_loss: 13.9838 - val_mae: 2.6659\n",
      "Epoch 352/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2352 - mae: 0.8392 - val_loss: 14.0416 - val_mae: 2.6215\n",
      "Epoch 353/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1677 - mae: 0.7903 - val_loss: 16.2501 - val_mae: 3.0015\n",
      "Epoch 354/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3373 - mae: 0.8822 - val_loss: 13.8751 - val_mae: 2.6692\n",
      "Epoch 355/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1228 - mae: 0.7470 - val_loss: 14.8994 - val_mae: 2.7880\n",
      "Epoch 356/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3603 - mae: 0.8669 - val_loss: 15.3800 - val_mae: 2.9115\n",
      "Epoch 357/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3104 - mae: 0.8417 - val_loss: 14.7280 - val_mae: 2.8143\n",
      "Epoch 358/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3312 - mae: 0.8531 - val_loss: 13.9598 - val_mae: 2.7018\n",
      "Epoch 359/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0873 - mae: 0.7818 - val_loss: 14.0727 - val_mae: 2.6455\n",
      "Epoch 360/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3505 - mae: 0.8658 - val_loss: 14.9214 - val_mae: 2.7312\n",
      "Epoch 361/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3535 - mae: 0.8301 - val_loss: 14.1160 - val_mae: 2.6619\n",
      "Epoch 362/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1934 - mae: 0.7849 - val_loss: 14.0484 - val_mae: 2.6703\n",
      "Epoch 363/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2227 - mae: 0.8262 - val_loss: 12.6351 - val_mae: 2.5028\n",
      "Epoch 364/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2433 - mae: 0.8408 - val_loss: 14.2122 - val_mae: 2.6671\n",
      "Epoch 365/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2790 - mae: 0.8021 - val_loss: 14.6973 - val_mae: 2.7144\n",
      "Epoch 366/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1940 - mae: 0.8087 - val_loss: 15.5446 - val_mae: 2.8980\n",
      "Epoch 367/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1780 - mae: 0.8022 - val_loss: 13.3730 - val_mae: 2.6197\n",
      "Epoch 368/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0360 - mae: 0.7658 - val_loss: 16.7821 - val_mae: 2.9143\n",
      "Epoch 369/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2002 - mae: 0.8259 - val_loss: 14.0435 - val_mae: 2.6290\n",
      "Epoch 370/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2938 - mae: 0.8271 - val_loss: 15.7009 - val_mae: 2.8570\n",
      "Epoch 371/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1070 - mae: 0.7759 - val_loss: 14.7853 - val_mae: 2.7130\n",
      "Epoch 372/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2045 - mae: 0.8142 - val_loss: 14.8935 - val_mae: 2.7223\n",
      "Epoch 373/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1321 - mae: 0.7508 - val_loss: 16.4992 - val_mae: 2.9366\n",
      "Epoch 374/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2017 - mae: 0.8055 - val_loss: 14.9200 - val_mae: 2.7286\n",
      "Epoch 375/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2195 - mae: 0.8167 - val_loss: 15.3814 - val_mae: 2.6838\n",
      "Epoch 376/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2992 - mae: 0.8130 - val_loss: 16.4079 - val_mae: 2.8134\n",
      "Epoch 377/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2394 - mae: 0.8129 - val_loss: 14.5600 - val_mae: 2.6684\n",
      "Epoch 378/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1055 - mae: 0.7654 - val_loss: 16.0588 - val_mae: 2.7976\n",
      "Epoch 379/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1011 - mae: 0.7755 - val_loss: 15.1431 - val_mae: 2.7085\n",
      "Epoch 380/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0675 - mae: 0.7705 - val_loss: 15.5449 - val_mae: 2.8233\n",
      "Epoch 381/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3348 - mae: 0.8051 - val_loss: 16.0697 - val_mae: 2.9081\n",
      "Epoch 382/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1210 - mae: 0.7774 - val_loss: 15.5333 - val_mae: 2.8474\n",
      "Epoch 383/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0628 - mae: 0.7674 - val_loss: 14.8279 - val_mae: 2.6459\n",
      "Epoch 384/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3669 - mae: 0.8289 - val_loss: 15.9365 - val_mae: 2.7888\n",
      "Epoch 385/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2245 - mae: 0.7628 - val_loss: 16.7958 - val_mae: 2.8602\n",
      "Epoch 386/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1571 - mae: 0.7737 - val_loss: 14.5632 - val_mae: 2.6782\n",
      "Epoch 387/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1020 - mae: 0.7764 - val_loss: 15.6229 - val_mae: 2.7705\n",
      "Epoch 388/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1653 - mae: 0.7701 - val_loss: 14.0323 - val_mae: 2.5509\n",
      "Epoch 389/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2168 - mae: 0.8088 - val_loss: 16.5647 - val_mae: 2.9859\n",
      "Epoch 390/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0610 - mae: 0.7532 - val_loss: 14.8784 - val_mae: 2.7503\n",
      "Epoch 391/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0705 - mae: 0.7604 - val_loss: 14.6886 - val_mae: 2.6652\n",
      "Epoch 392/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1512 - mae: 0.7521 - val_loss: 13.8985 - val_mae: 2.6109\n",
      "Epoch 393/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3061 - mae: 0.8167 - val_loss: 14.2530 - val_mae: 2.6437\n",
      "Epoch 394/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1375 - mae: 0.7622 - val_loss: 14.4258 - val_mae: 2.6824\n",
      "Epoch 395/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0222 - mae: 0.7620 - val_loss: 14.2785 - val_mae: 2.6032\n",
      "Epoch 396/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0733 - mae: 0.7612 - val_loss: 14.0775 - val_mae: 2.6059\n",
      "Epoch 397/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1894 - mae: 0.7615 - val_loss: 16.0423 - val_mae: 2.9182\n",
      "Epoch 398/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1110 - mae: 0.7647 - val_loss: 15.2978 - val_mae: 2.7166\n",
      "Epoch 399/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1434 - mae: 0.7982 - val_loss: 16.6489 - val_mae: 2.8753\n",
      "Epoch 400/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0485 - mae: 0.7542 - val_loss: 17.8304 - val_mae: 3.0028\n",
      "Epoch 401/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1401 - mae: 0.7705 - val_loss: 15.0720 - val_mae: 2.7004\n",
      "Epoch 402/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1374 - mae: 0.7638 - val_loss: 16.1126 - val_mae: 2.8274\n",
      "Epoch 403/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1001 - mae: 0.7768 - val_loss: 14.7604 - val_mae: 2.6616\n",
      "Epoch 404/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0548 - mae: 0.7774 - val_loss: 15.7426 - val_mae: 2.8292\n",
      "Epoch 405/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2079 - mae: 0.7803 - val_loss: 14.8692 - val_mae: 2.7259\n",
      "Epoch 406/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1340 - mae: 0.7591 - val_loss: 15.6456 - val_mae: 2.7662\n",
      "Epoch 407/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0129 - mae: 0.7304 - val_loss: 16.4173 - val_mae: 2.8446\n",
      "Epoch 408/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1584 - mae: 0.8174 - val_loss: 15.1978 - val_mae: 2.8197\n",
      "Epoch 409/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0315 - mae: 0.7700 - val_loss: 15.1092 - val_mae: 2.6646\n",
      "Epoch 410/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1609 - mae: 0.7736 - val_loss: 15.2889 - val_mae: 2.7903\n",
      "Epoch 411/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0998 - mae: 0.7491 - val_loss: 14.1567 - val_mae: 2.6111\n",
      "Epoch 412/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1141 - mae: 0.7619 - val_loss: 15.1014 - val_mae: 2.7994\n",
      "Epoch 413/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0836 - mae: 0.7818 - val_loss: 14.3018 - val_mae: 2.6839\n",
      "Epoch 414/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0679 - mae: 0.7625 - val_loss: 14.5719 - val_mae: 2.6885\n",
      "Epoch 415/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1806 - mae: 0.7864 - val_loss: 14.6603 - val_mae: 2.7593\n",
      "Epoch 416/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0486 - mae: 0.7752 - val_loss: 14.0295 - val_mae: 2.6616\n",
      "Epoch 417/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0319 - mae: 0.7132 - val_loss: 13.9163 - val_mae: 2.6315\n",
      "Epoch 418/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0130 - mae: 0.7406 - val_loss: 14.6594 - val_mae: 2.6473\n",
      "Epoch 419/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0181 - mae: 0.7331 - val_loss: 16.2807 - val_mae: 2.8603\n",
      "Epoch 420/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1806 - mae: 0.7877 - val_loss: 14.2537 - val_mae: 2.6120\n",
      "Epoch 421/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0789 - mae: 0.7653 - val_loss: 15.6661 - val_mae: 2.7818\n",
      "Epoch 422/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9902 - mae: 0.7558 - val_loss: 13.9855 - val_mae: 2.5952\n",
      "Epoch 423/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0361 - mae: 0.7370 - val_loss: 14.0912 - val_mae: 2.6750\n",
      "Epoch 424/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0771 - mae: 0.7460 - val_loss: 14.4070 - val_mae: 2.6940\n",
      "Epoch 425/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9870 - mae: 0.7076 - val_loss: 16.1074 - val_mae: 2.8678\n",
      "Epoch 426/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1403 - mae: 0.7760 - val_loss: 15.1173 - val_mae: 2.7940\n",
      "Epoch 427/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0221 - mae: 0.7283 - val_loss: 14.9405 - val_mae: 2.7231\n",
      "Epoch 428/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0543 - mae: 0.7667 - val_loss: 14.9867 - val_mae: 2.6896\n",
      "Epoch 429/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1367 - mae: 0.7540 - val_loss: 15.9604 - val_mae: 2.8044\n",
      "Epoch 430/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0466 - mae: 0.7634 - val_loss: 14.9962 - val_mae: 2.8054\n",
      "Epoch 431/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0705 - mae: 0.7238 - val_loss: 16.0184 - val_mae: 2.8321\n",
      "Epoch 432/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0130 - mae: 0.7346 - val_loss: 15.4581 - val_mae: 2.8304\n",
      "Epoch 433/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0775 - mae: 0.7790 - val_loss: 14.3994 - val_mae: 2.6555\n",
      "Epoch 434/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8935 - mae: 0.7167 - val_loss: 15.4522 - val_mae: 2.8814\n",
      "Epoch 435/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9649 - mae: 0.7280 - val_loss: 15.2318 - val_mae: 2.7562\n",
      "Epoch 436/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0966 - mae: 0.7595 - val_loss: 15.2079 - val_mae: 2.7224\n",
      "Epoch 437/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9443 - mae: 0.7057 - val_loss: 15.9368 - val_mae: 2.8151\n",
      "Epoch 438/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.1550 - mae: 0.7608 - val_loss: 16.8936 - val_mae: 2.9458\n",
      "Epoch 439/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0426 - mae: 0.7398 - val_loss: 15.2463 - val_mae: 2.7940\n",
      "Epoch 440/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0724 - mae: 0.7628 - val_loss: 16.3261 - val_mae: 2.8947\n",
      "Epoch 441/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9137 - mae: 0.7121 - val_loss: 16.2787 - val_mae: 2.8374\n",
      "Epoch 442/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0080 - mae: 0.7382 - val_loss: 14.8094 - val_mae: 2.7695\n",
      "Epoch 443/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0576 - mae: 0.7227 - val_loss: 15.1328 - val_mae: 2.7634\n",
      "Epoch 444/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0535 - mae: 0.7523 - val_loss: 15.7872 - val_mae: 2.8074\n",
      "Epoch 445/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0878 - mae: 0.7677 - val_loss: 14.9729 - val_mae: 2.6689\n",
      "Epoch 446/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0883 - mae: 0.7697 - val_loss: 15.3549 - val_mae: 2.7468\n",
      "Epoch 447/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0127 - mae: 0.7545 - val_loss: 16.8284 - val_mae: 3.0019\n",
      "Epoch 448/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0437 - mae: 0.7662 - val_loss: 15.3301 - val_mae: 2.8176\n",
      "Epoch 449/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9646 - mae: 0.7390 - val_loss: 14.3387 - val_mae: 2.6452\n",
      "Epoch 450/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9558 - mae: 0.7031 - val_loss: 14.5417 - val_mae: 2.6665\n",
      "Epoch 451/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9927 - mae: 0.7452 - val_loss: 15.5944 - val_mae: 2.7832\n",
      "Epoch 452/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0854 - mae: 0.7627 - val_loss: 15.2364 - val_mae: 2.7878\n",
      "Epoch 453/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9737 - mae: 0.7399 - val_loss: 16.9615 - val_mae: 2.9451\n",
      "Epoch 454/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9926 - mae: 0.7272 - val_loss: 14.5498 - val_mae: 2.6653\n",
      "Epoch 455/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8761 - mae: 0.7094 - val_loss: 14.4470 - val_mae: 2.6551\n",
      "Epoch 456/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9911 - mae: 0.7093 - val_loss: 15.1858 - val_mae: 2.7580\n",
      "Epoch 457/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0054 - mae: 0.7178 - val_loss: 14.6557 - val_mae: 2.7395\n",
      "Epoch 458/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0362 - mae: 0.7331 - val_loss: 15.6615 - val_mae: 2.8310\n",
      "Epoch 459/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0246 - mae: 0.7466 - val_loss: 15.1060 - val_mae: 2.7055\n",
      "Epoch 460/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9548 - mae: 0.7297 - val_loss: 15.5137 - val_mae: 2.7287\n",
      "Epoch 461/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0377 - mae: 0.7241 - val_loss: 16.5620 - val_mae: 2.8621\n",
      "Epoch 462/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0051 - mae: 0.7160 - val_loss: 15.4679 - val_mae: 2.7491\n",
      "Epoch 463/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9651 - mae: 0.7309 - val_loss: 15.6315 - val_mae: 2.7193\n",
      "Epoch 464/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9915 - mae: 0.7270 - val_loss: 15.6996 - val_mae: 2.8658\n",
      "Epoch 465/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0253 - mae: 0.7365 - val_loss: 15.1658 - val_mae: 2.7472\n",
      "Epoch 466/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0451 - mae: 0.7557 - val_loss: 15.7092 - val_mae: 2.7640\n",
      "Epoch 467/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0331 - mae: 0.7564 - val_loss: 15.3501 - val_mae: 2.7743\n",
      "Epoch 468/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9624 - mae: 0.7491 - val_loss: 15.0735 - val_mae: 2.7598\n",
      "Epoch 469/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9616 - mae: 0.7242 - val_loss: 15.1351 - val_mae: 2.7592\n",
      "Epoch 470/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9336 - mae: 0.7027 - val_loss: 13.7440 - val_mae: 2.5717\n",
      "Epoch 471/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9193 - mae: 0.7116 - val_loss: 15.3389 - val_mae: 2.8197\n",
      "Epoch 472/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0525 - mae: 0.7353 - val_loss: 15.3220 - val_mae: 2.8350\n",
      "Epoch 473/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9447 - mae: 0.6833 - val_loss: 15.2560 - val_mae: 2.7735\n",
      "Epoch 474/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9990 - mae: 0.7324 - val_loss: 15.4209 - val_mae: 2.8235\n",
      "Epoch 475/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9299 - mae: 0.6985 - val_loss: 16.0530 - val_mae: 2.8294\n",
      "Epoch 476/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0607 - mae: 0.7685 - val_loss: 13.9466 - val_mae: 2.5799\n",
      "Epoch 477/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8922 - mae: 0.6699 - val_loss: 14.4380 - val_mae: 2.7001\n",
      "Epoch 478/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0020 - mae: 0.7207 - val_loss: 14.3238 - val_mae: 2.6330\n",
      "Epoch 479/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9329 - mae: 0.7009 - val_loss: 15.8651 - val_mae: 2.9008\n",
      "Epoch 480/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9523 - mae: 0.7192 - val_loss: 16.5990 - val_mae: 3.0115\n",
      "Epoch 481/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0200 - mae: 0.7354 - val_loss: 15.0559 - val_mae: 2.7292\n",
      "Epoch 482/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9255 - mae: 0.7250 - val_loss: 15.0290 - val_mae: 2.7438\n",
      "Epoch 483/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9522 - mae: 0.7050 - val_loss: 14.7123 - val_mae: 2.6920\n",
      "Epoch 484/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9789 - mae: 0.6918 - val_loss: 14.4818 - val_mae: 2.6892\n",
      "Epoch 485/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8555 - mae: 0.6637 - val_loss: 14.6096 - val_mae: 2.7258\n",
      "Epoch 486/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9162 - mae: 0.6980 - val_loss: 16.0498 - val_mae: 2.9381\n",
      "Epoch 487/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9812 - mae: 0.7286 - val_loss: 14.9127 - val_mae: 2.7323\n",
      "Epoch 488/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8413 - mae: 0.6850 - val_loss: 13.4205 - val_mae: 2.6132\n",
      "Epoch 489/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9496 - mae: 0.6689 - val_loss: 15.4923 - val_mae: 2.7895\n",
      "Epoch 490/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9610 - mae: 0.6963 - val_loss: 15.1365 - val_mae: 2.7860\n",
      "Epoch 491/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9079 - mae: 0.7003 - val_loss: 14.4618 - val_mae: 2.7048\n",
      "Epoch 492/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9671 - mae: 0.7037 - val_loss: 15.4348 - val_mae: 2.7391\n",
      "Epoch 493/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9008 - mae: 0.7114 - val_loss: 15.9167 - val_mae: 2.8068\n",
      "Epoch 494/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8798 - mae: 0.6935 - val_loss: 15.2506 - val_mae: 2.7718\n",
      "Epoch 495/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7316 - mae: 0.6600 - val_loss: 15.2777 - val_mae: 2.6471\n",
      "Epoch 496/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9629 - mae: 0.6996 - val_loss: 13.9154 - val_mae: 2.6248\n",
      "Epoch 497/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9089 - mae: 0.6879 - val_loss: 15.5301 - val_mae: 2.8193\n",
      "Epoch 498/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9648 - mae: 0.7056 - val_loss: 14.6245 - val_mae: 2.6803\n",
      "Epoch 499/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8630 - mae: 0.7078 - val_loss: 14.7108 - val_mae: 2.6887\n",
      "Epoch 500/500\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9146 - mae: 0.6849 - val_loss: 14.2924 - val_mae: 2.7438\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('처리중인 폴드: {}'.format(i))\n",
    "    val_data = train_data[i * num_val_samples : (i+1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples : (i+1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "    [train_data[:i * num_val_samples],\n",
    "    train_data[(i+1) * num_val_samples:]], axis = 0\n",
    "    )\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "    [train_targets[:i * num_val_samples],\n",
    "    train_targets[(i+1) * num_val_samples:]], axis = 0\n",
    "    )\n",
    "    \n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                       validation_data = (val_data, val_targets),\n",
    "                       epochs = num_epochs, batch_size=1, verbose=0)\n",
    "    \n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 폴드에 대해 에포크의 MAE 점수 평균을 계산한다.\n",
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd5xU1fn48c8zsxXYQll6WWkCIiJiQRALFhSjiRpL4jfRGFGjRmPyM6LGGE1siSUmJlGTWGJP7AUVEcUGuCC9lwWkLnULbJs9vz9u2Tszd2ZnYWeXZZ7367WvnXJ39tzZ2fuc8pxzxBiDUkqp1BVo6QIopZRqWRoIlFIqxWkgUEqpFKeBQCmlUpwGAqWUSnFpLV2AxurUqZMpLCxs6WIopVSrMnv27G3GmAK/51pdICgsLKSoqKili6GUUq2KiKyN9Zx2DSmlVIrTQKCUUilOA4FSSqU4DQRKKZXiNBAopVSK00CglFIpTgOBUkqluJQJBMs2l/HQh8vYVl7V0kVRSqkDStIDgYgEReQbEXnH57nLRKRERObaXz9NVjlWbi3n0Y9XsqOiOlm/QimlWqXmmFl8A7AEyI3x/MvGmOuSXQgR63udbsSjlFJhktoiEJGewATgn8n8PYkI2IFA44BSSoVLdtfQI8DNQF2cY84Xkfki8j8R6eV3gIhMFJEiESkqKSnZp4KI3STQFoFSSoVLWiAQkbOBrcaY2XEOexsoNMYMAz4CnvE7yBjzhDFmpDFmZEGB7+J5DZfHfa19+nGllDpoJbNFMBo4R0SKgZeAU0TkOe8BxpjtxhgnjedJ4KhkFSZgtwg0ECilVLikBQJjzCRjTE9jTCFwMfCxMeZS7zEi0s1z9xysQeWk0MFipZTy1+z7EYjIXUCRMeYt4Ocicg5QC+wALkvW73VbBMn6BUop1Uo1SyAwxnwCfGLfvsPz+CRgUnOUAW0RKKWUr5SZWaxjBEop5S9lAkF91pBGAqWU8kqZQKBjBEop5S+FAoH1va5OQ4FSSnmlTCCoHyxu2WIopdSBJmUCQX3XkEYCpZTySplAoEtMKKWUv5QJBIGApo8qpZSflAkETotAJ5QppVS41AkEmj6qlFK+UiYQBHSJCaWU8pUygcBtEWggUEqpMCkTCHSrSqWU8pcygUBwtqps4YIopdQBJnUCgdsi0EiglFJeKRcItEWglFLhUiYQOEtMaAKpUkqFS5lAoC0CpZTylzKBQHcoU0opfykUCKzvOqFMKaXCpUwgwE0f1UCglFJeKRMInBaBUkqpcCkTCJwlJrRFoJRS4VImEOgSE0op5S9lAoEuMaGUUv5SJxDoEhNKKeUrBQNBy5ZDKaUONCkTCAI6WKyUUr5SLhBoGFBKqXApEwhEZxYrpZSvlAsEGgeUUipc6gQCdM9ipZTykzKBwJ1Q1rLFUEqpA07KBAJ3iQmdUaaUUmGSHghEJCgi34jIOz7PZYrIyyKyUkRmikhhssqhLQKllPLXHC2CG4AlMZ67AthpjOkPPAzcn6xC1C86l6zfoJRSrVNSA4GI9AQmAP+Mcci5wDP27f8B40QkKQtG6xITSinlL9ktgkeAm4G6GM/3ANYDGGNqgd1Ax8iDRGSiiBSJSFFJSck+FUS3qlRKKX9JCwQicjaw1RgzO95hPo9FXaqNMU8YY0YaY0YWFBTsW3ns7zqhTCmlwiWzRTAaOEdEioGXgFNE5LmIY74FegGISBqQB+xIRmF0iQmllPKXtEBgjJlkjOlpjCkELgY+NsZcGnHYW8CP7dsX2Mck5VqtS0wopZS/tOb+hSJyF1BkjHkL+BfwHxFZidUSuDh5v9f6rnFAKaXCNUsgMMZ8Anxi377D83gl8P3mKIMuMaGUUv5SZmax7lmslFL+UigQ6IQypZTykzKBQAeLlVLKXwoFAk0fVUopPykTCMBqFehgsVJKhUupQBAQ0cFipZSKkFKBQNAxAqWUipRSgSAgomMESikVIaUCgYi2CJRSKlLKBQKNA0opFS6lAoE1WKyRQCmlvFIqEFiDxS1dCqWUOrDEDAQicrPn9vcjnrsnmYVKFk0fVUqpaPFaBN4loSdFPDc+CWVJPh0sVkqpKPECgcS47Xe/VXAWnlNKKVUvXiAwMW773W8VNH1UKaWixduY5ggRKcWq/Wfbt7HvZyW9ZEmgYwRKKRUtZiAwxgSbsyDNIaAtAqWUitKo9FERaSsiPxSRd5NVoOQSTR9VSqkIDQYCEckQke+KyCvAJuBU4B9JL1kSWNtVaiRQSimvmF1DInIacAlwBjAN+A9wjDHm8mYqW5MTgbq6li6FUkodWOINFn8AfAaMMcasARCRPzdLqZLEWn1UWwRKKeUVLxAchTWp7CMRWQ28BLTqAWRdYkIppaLFHCMwxnxjjPm1MaYfcCdwJJAhIpNFZGJzFbApiaaPKqVUlISyhowxXxhjrgN6AI8Ao5JaqiQJBHTPYqWUihRvsHhEjKdKgL8kpzjJJYjOI1BKqQjxxgiKgEVYF34IX1/IAKckq1DJEhBNHlVKqUjxAsEvgfOBvVgDxa8bY8qbpVRJIqITypRSKlK8weKHjTFjgOuAXsBUEXlFRIY3W+mamLVVpUYCpZTyanCw2J5D8CbwIXAMMDDZhUoWQfcsVkqpSPEGi/tizSM4F1iP1T30B2NMZTOVrcnphDKllIoWb4xgJTAfqzVQCvQGfib25i7GmIeSXrompktMKKVUtHiB4C7qk2zaNUNZkk5bBEopFS3efgR3NmM5moVmDSmlVLRG7UfQGCKSJSKzRGSeiCwSkd/5HHOZiJSIyFz766fJKg84g8UaCZRSyite19D+qgJOMcaUi0g68LmITDbGzIg47mV7+Yqks5aYaI7fpJRSrUfSAoGxqt7OBLR0+6tFL8O6xIRSSkVrMBCISCbWDONC7/HGmLsS+NkgMBvoDzxmjJnpc9j5IjIWWA78whiz3ud1JgITAXr37t3Qr41Jl5hQSqloiYwRvIk1l6AWqPB8NcgYEzLGDAd6AseIyNCIQ94GCo0xw4CPgGdivM4TxpiRxpiRBQUFifxqfzpYrJRSURLpGuppjBm/P7/EGLNLRD4BxgMLPY9v9xz2JHD//vyehgR0iQmllIqSSIvgSxE5vLEvLCIFIpJv387G2vR+acQx3Tx3zwGWNPb3NKpM6GCxUkpFSqRFMAa4TETWYGUC2VmYZlgDP9cNeMYeJwgArxhj3hGRu4AiY8xbwM9F5BysbqcdwGX7eB4JCYgOFiulVKREAsGZ+/LCxpj5WNtbRj5+h+f2JGDSvrz+vgjoVpVKKRUlkdVH1wL5wHfsr3z7sdZH0BaBUkpFaDAQiMgNwPNAZ/vrORG5PtkFSwZNH1VKqWiJdA1dARxrjKkAEJH7ga9ohfsWC4IxuvyoUkp5JZI1JEDIcz9E+P7FrYYuMaGUUtESaRE8BcwUkdft+98F/pW8IiWPLjGhlFLRGgwExpiH7MlgY7BaApcbY75JdsGSIRgQQjq1WCmlwsTbqjLXGFMqIh2AYvvLea6DMWZH8ovXtDLSAlTV6hiBUkp5xWsRvACcjbVonLcaLfb9vkksV1JkpAWoDmkgUEopr3g7lJ1tfz+k+YqTXJnBAFU1GgiUUsorkXkEUxN5rDXITNcWgVJKRYo3RpAFtAE6iUh76lNGc4HuzVC2JpcRDFCtYwRKKRUm3hjBVcCNWBf92dQHglLgsSSXKyky0jQQKKVUpHhjBH8G/iwi1xtjWt0sYj86WKyUUtESmUfwF3tnsSFAlufxZ5NZsGTICAYJ1RlCdYZgoFVOjlZKqSaXyJ7FvwVOwgoE72EtS/050PoCQZo1Nl5dW0d2RrCFS6OUUgeGRNYaugAYB2w2xlwOHAFkJrVUSeINBEoppSyJBIK9xlqys1ZEcoGttMLJZFAfCKpCoQaOVEqp1JHIonNF9t7DT2JlD5UDs5JaqiTJDGqLQCmlIiUyWPwz++Y/ROR9INfehrLV0a4hpZSKFm9C2Yh4zxlj5iSnSMmT6QQCTSFVSilXvBbBg/b3LGAkMA9rUtkwYCbWstStirYIlFIqWszBYmPMycaYk4G1wAhjzEhjzFHAkcDK5ipgU9JAoJRS0RLJGhpkjFng3DHGLASGJ69IyZNhDxbrngRKKVUvkayhJSLyT+A5rH0ILgWWJLVUSaItAqWUipZIILgcuAa4wb4/Hfh70kqURO48Ag0ESinlSiR9tBJ42P5q1TRrSCmlosVLH33FGHOhiCwgfKtKAIwxw5JasiTICFrrC2nXkFJK1YvXInC6gs5ujoI0Bx0jUEqpaPH2I9hkf1/bfMVJrvpAoGsNKaWUI17XUBk+XUJYk8qMMSY3aaVKkgwdI1BKqSjxWgQ5zVmQ5pCpXUNKKRUlkfRRAESkM+E7lK1LSomSKC0giGggUEoprwZnFovIOSKyAlgDfAoUA5OTXK6kEBEyggGqtGtIKaVciSwxcTdwHLDcGHMI1m5lXyS1VEmUkRbQFoFSSnkkEghqjDHbgYCIBIwx00hgrSERyRKRWSIyT0QWicjvfI7JFJGXRWSliMwUkcJGn0EjZWogUEqpMImMEewSkXZYS0s8LyJbgdoEfq4KOMUYUy4i6cDnIjLZGDPDc8wVwE5jTH8RuRi4H7iokefQKBnBgC4xoZRSHom0CM4F9gK/AN4HVgHfaeiHjKXcvptuf0Wmo54LPGPf/h8wTkQkgTLtM+0aUkqpcDEDgYj8VUSON8ZUGGNCxphaY8wzxphH7a6iBolIUETmYm14P8UYMzPikB7AegBjTC2wG+jo8zoTRaRIRIpKSkoSPTdfGgiUUipcvBbBCuBBESkWkftFpNF7ENgBZDjQEzhGRIZGHOJX+/db1+gJe2OckQUFBY0tRpiMtIBOKFNKKY94O5T92RgzCjgR2AE8JSJLROQOERnYmF9ijNkFfAKMj3jqW6AXgIikAXn270qajKC2CJRSyqvBMQJjzFpjzP3GmCOBHwDfI4GNaUSkQETy7dvZwKnA0ojD3gJ+bN++APjYGOO3rEWTyUwLaiBQSimPRCaUpYvId0TkeayJZMuB8xN47W7ANBGZD3yNNUbwjojcJSLn2Mf8C+goIiuBm4Bb9uksGiEjTSeUKaWUV7xF504DLgEmALOAl4CJxpiKRF7YGDMfa6P7yMfv8NyuBL7fyDLvFx0sVkqpcPHmEdwKvAD8yhiT1H775mQFAl2GWimlHPFWHz25OQvSXDKDAXbvraEmVEd6MJFpFEopdXBLuSthRlqAbeXVXPPc7JYuilJKHRBSLhBU1ljdQh8t2drCJVEtZWtpJcPu/IBFG3e3dFGUOiCkXCBYs30PYM0nUKnp0+UllFbW8u/Pi1u6KFF276lh9H0fM//bXS1dFJVCUu5quHa7lfTUs0N2C5dEtRRny9KaAzCNeMaa7WzYtZdHp65s6aKoFJJygeD+84cB0KFNRguXRLUUpzV4IKYRB+w1F5M8r1KpMCkXCM44rCvjBnWmUlNID1qz1uzgy1XbYj7vZIsdiGtOBezVt0IaCFQzSnjP4oNJVkaQvdUaCA5WFz7+FQDF903wfb7Ovsgmu2uosiZEVnow4eOrakPMKram7NRpHFDNKOVaBADZ6UEqaw682qBqHjUh6yr72YptrN+xJym/Y9nmMgb95n0mL9iU8M/cN3kpj3+6GtCuIdW8UjIQtMkIsqc6kU3W1MHI2xJwWg9NbeEGKzX1/UWb4x5XG6pjR0U1ZZU1rNhS7j5eVVvHlMVbklK2fVFZE+LNuRsOigC1tzrE+EemM3tt8y+YsHhj6QH5HqZkIMhOD7K3RruGmsuOimp276lpstdbt32Pe6HdF95B4i2llU1RpChBu7O/NhT7n76qNkT/2yYz4u4pnPLgp6QH67fnmLVmB1c+WxR3rKM53fveEm54aS5frU5oT6oD2uJNpSzdXMbd78ReRLmkrIp565s2hffteRs569HP+MCncvD8zLUce89HLRYkUjIQZNldQ3XaEdssRtw9haN+P6XJXm/sH6dx9l8+930ukb+pd5C4XWZyhskq7Ban3zhEVW2Itdsr2FlRHxxLyqrctFav0r2xW663vDqfeyc3uCJ8k1hnd6HtqWr9FaiQ/RlJC4TvizVn3U73QvzDf87g3Me+cI+NZ9PuvQldwN/4ZgMApZXW33R7eRWrS6xW4G2vL2RLaVWL7aeekoEgO8MawNNN7GFPdS0/e342m3cnp2bsqE1y0K2sCTHptQWs3hZ/cdz3Fmxi46697v3IQLB+xx5CdYZnvyrmD+8u3ufy7LJbQH7n/Z+v1jL+kc+iWiN+a19lpMXewvulr9e7YwpNYfmWMu54cyEjf/9RVGqt07AJBpK6pXizqK2zzi3N0wJbuGE35/3tS2astrqLltvddGsa+DytKiln1L0f89u3FkU9Z4wJq5jM+7a+Fbuzopqjfv8Rpz70adjPlFe1TJd1SmYNZduZHHtrQm5QSFXvzt/Eews2k5Ue5KELG70baZOorAmRFhDSPBfCr4t30LdTWzq2y0zoNaYs3sKLs9a5NSwvYwyPTVtJ34J2/Oz5OWHPtcuq/xdYv2MPJzwwjRtPHcAjH60A4LYJQxp9PnV1hlVbrXKUVdawe08NeW3S3edXlZSztyZE8fbwi0yis92ra+t8Ww/76/SHp7u3d+2tZnVJBQO75NChbQYh++Lpu7lsK+MkC6QF6t/D7RXVAKzcWsbemlq65GaypbSKJZtK6d+5XczXWlNi/Q2f/Wotd51r7cT73Iy1bNq9l9qQ4fHpq1l1z1kEA0JZpVU5qKiqZeaa+uwwb6ujvLKWTgl+5ptSarYI7ECgA8b1NbyW7CYb9Jv3ufRfM937oTrDD5+cyTNfrQ077t35mzj/71+69721VqcLZlt5VdTrvzF3A3/6cDm3vr4g6jlveud8u8b2ut2EB1i5tZzCW95l9tqdvmWfsngLy7eUhT327y/W8Jr9Gl8X7+SIuz7kvQWb3H/4jbuslkDxtvCMJb/aYGR228zV2xl4+2SKisMHOhPpwmiMjbsqufiJGVzz3GyueW42a+2lWapqQtSGortV6+oMOyqqeWjKcsY+MC3q9Tbt3ut2jfhJ5v/iuu17uPl/89zPyF77dwU8rRvnsTvfXsxPni5iS6n1OVq2uYx4tpbVf96+XLWNmau3c/sbC3ls2ioen2611raXV2GMcbsk91SHwta58v7dY7UIakN1bNy1N2kpz6kZCOxWQKUOGLszWeOMacb05twNXBtRw47U0Oxdp2/VaZID7N5bQ3Wozq1BOa59YU7YBdn5p1m0cTc3vTIv7DHva6+2a23eGqDDO5/EWd/HueiB1ZUE8MrX633Lf+WzRWE1aYAvVkYP8P7s+Tk8NGUZYF0UgagWwS6fAfXKmhBVtSGe/mINtaE6/jv7WwCWeC5Qb87dQL9b32ODp8ursSIv7B8vtRZlnLlmB5MXbubbnXvt8tTR/7bJ9L31PX7w5Az3+L99spIRd0/h0akrWLdjT9TrXfLEDG58ea77fpdW1jDx2SI27d7Lmm0VDLnjA16b823C5f1y1Ta2xhjo31JayQpPcL751Xm8UvQtc+zPzh67DN4xAuexyIC6c4/VUti8u5LLnprFtGXhi1V6u/d+8ORMLnpiBpG2llWxtyaEM4xQUVXrVjrAquA4yir9A8HDHy3n+Ps+5t73Inf7bRqpGQjcFoEGAqdW5Db9G+GGl+bybgN58g31eTpNcq8dFVYtq7Kmji2llTEH4pxA8cD7y9zHSjw1NOefunSvdVxka+H8ET1ZsbWcdfaFf4lP7c/5jFT41Fj9yrW6pJyFG0t9y/vJshIANtktAqf/+bQhXazHS6Mv5JU1dTzzZTF3vr2YF79e79ZQ87Lru5qen7EOgOJtFVTWhHj2q2Iem9a4tYo2R1xUpy8v8T3Om2335ar6DKKpS8MvkDv2hP9di+332Pk8TF9ewoeLt3DnW4vcbrR35ic258IYww+enMkF//BP/R334Kec5gnOTmVE7EpPhf03DfoEgkhllbVMXrCJX7w8l0+WlfCTp78Oe35rWcNja1vLKtnpCfJbSqv4avV2uuZmAYS1VL/duYfCW97lrXkb3cd+/O9ZPDZtFQAThnVr8Pfti5QMBF3zrD/A+h37XoM6GFTWhOqXNNiProV43Up+ffZe3oHbUJ3hP18V85x9YVu4YTfH3jOVZ74s9v1Zp/bUPT+rviyeojhN8Vi1rHaZVoVg7B+trgxvEHE4j1X4BDTvRdF5/87+y+fuz/znimO47azB7jG79tRQVllDmf1ac+30xJvPOBSAraXRv/+LVdu4x64Ffrtjj/t+eluzpXZALK+qZdBv3ueONxfxxw+WRb3Woo27YwbVRXbwcj4PsQZJy2O8lzlZ6WH3/d5LqH8f22ZYYzMrtpa7g7ZVCS774gQTJ5Ppq1XbufSfM7nEro07zzvfndau8/faYz8+b/0uCm95l8UbS2N2Tc37dhfXPD/HTZs1JrwC4Pc3i/STp4uYu64+FfXteRuprq3jB8f2jjrWCQD/m13fOvrUDspnHd6Vo/q0b/D37YuUDAT9O7cjGBAWb0rd9ehrQ3UM+s373P2OlRmzP12PsbKvakN1MWttDqe/HKyLx2/eXMTT9oV/6Wbr4vTZCv9c+l17anj6izUxg5hTEyyt9J/D0MaTMTTptfm+4wvrdlgXxAo7bXLG6u089KF1kfUGmKG//YC35210a5Z9C9pywoACrhzb1z1mw6697sXLK9eu3fu9j95ug517qt3arDcwOeX4b1HsrpUvVm5jwqOf8+jUlTw0ZTmhOsP4R6bzxHSrpvn4p6vokZ/NSxNHAVb3nJ8Sn/cIICcrPO9ka4xAUF5VS12dcd+n1SUVXPaUVcuuSnC2f2QX2iVPzuDzldui5jiMe/ATXv/mW7ei8stX5jFz9Xb3PXTKOG3Z1pgtAm83ocP73mxKMNvu2hfqu1CrQ3XkZadzwoBOUcc5n/WqmhAnPPAxlz81y32uIImDyCmZNZSVHqRfQVuWbIo/ENTardhSxvUvfsMLVx5Hh7bhq63usWtHzqBY3X5MZKmMkX0VWRMP1Zmo9ENvi2Dq0vCZtE52x9SlW8P6fB1Pf7kmbIOhw7rnujVb8AYC/9qe95xfnGWNARzSqW1YbXil3W0xq3gHpZU1XPrPmdTWGU48tICSsvruj701IZ6fuZbMtABVtXUM7prrPvfqNcfz/My1vDZnQ9REuD9eMIzciNp0LMs8M4+9F0One+KjJbFnIjvv38MfLQfg0alWVtQ97y1l4th+LNlUykVH9ya/TfyyRPbLO+spZUZkPMVqEZSUVdH3L+9xaJecqOcSTef2nnttRA3mEfv8wPpsvzZng1tR2FZexUVPzOCnYw4J+5mXv17vG6Bj2birEmOswLx2ewWH98hjQYITHNtlplFeVcuxh3RwKwB+nKwib69FMhdJTMkWAcCALjkN5gi3dk9+tpqlm8t8+/EjF91rqGvoqS/WUHjLu25z29s1EWsl18iauF/Tf9PuvQQEji5szz3vxp4cdVrEgCyEtyZG9M7n4qN7hT1/zD1TueXV+e4YQaRtZdHjE95UwR752WF9u7/+33y3G+P8v3/F1RHbnc5Zt4uq2jpOGNCJ+84/3H38qD7tGX9YVwDmrg+/YHx/ZC+y0gNhA5enDu7C6P4do8rmnenqrZXWeEb6u+dlkd8mndyIGnq8eRwVVbVUVIfonJvpjp/FsiWiK8T5PESOoTjBaeDtk7nN0wfuLKq3zCewO58p5/ue6lrflsmuvfV/t1teC88Ec9J+HTv3VEdVciLHrRoKAu0jguO6HRVc8/xsTnnwUyqqQxzRKy/mz9561qCw+06W2tGFHaJaUfEEA8LPTuqf8PGNlbKBoHNOJtti1FoOFt3zrc13Nvlkk0T2ecdrESzcsJvfvW11Ie20B3e9Nb5YC/hFzoqNDD5frdrOU18UU9ixLbecOchtsifKe/Fp3yaDzrlZUce89PX6mGMEfrn4AzyBYFDX8FrrR0u2uFlWkTq1y3BbIOOHdo3qM3fKNv/bXQQEhvXM40ej+gDWIKZTOzyqT3v++eORPP/T43x/j8N7gWyXmUandlaLr3t+NheO7BVVe6yJkxbmdJF0zslscF5NZNdQUfEONu7a63adOdZt32OlTNbW8fzMde7jM+IsUbFiazmLN5Yy6Dfv8+T01Zz158844ncfus8v31LGp8tLwroKvX3pXveddzjnjejBzoqaqJZGrNaKY3ivfP75o5FuH35kv/zVz80Jy3I7omd+2PO/Obt+7snEsf2Yf+fpTL7hBB668Ah220FsWM+8hFuCAJNvOIFeHdokfHxjpWwg6NQuk7Kq2oM6hdT5oPn1Y0b2icZrEfgt57DTkxUS+R7OW7+Lm16eG1ZzA3hh5jpKK2u47KlZLNtcxiVPzqC2ztA9P5sRvRs3CCYSPlCX3yaDLj6BAGB7RVXUcgIAt5w5iLEDC8Ie6+35ZxvUzQoEOZlpvHXdaGpCJmZf8mlDurq3e7WP/octyLH6dxdtLKVzThZvXTfGnYAEuDX4RCeVed//nKw08u2Nljq1yyTL7p4yxrB8SxnGGMqrYq/1tGST1Z3WOSeLNg0EAqdr6KRDrfft6ufmcPx9H/P5ym2M7t+Rf1x6FCP7tGfJplJ3UNzrm3Xx1++5yF4E8A/vLXEzjerqDGWVNZz+8HR+/O9ZPDG94dnUBmvzqZ17qtkVkcG0pYFMn3aZaZw6pItbYx/crb6bz/s56mYnnRzp+ewu+/14rojoesrNSmdwt1zOG9HTDciH9cgj01MReeD8YVGz3LPSrecvProXfTu1jVvm/ZWygcAZeDnpj59QWROi8JZ33UHAg4VTK/TLL49cdC9WIIhsRr89fyOLN5aGBZcnp6/m8U9XufeveKaI177Z4E7Tv/rEfgA8OGU5L81axyfLSnjmq2L3+GBA3NS+RPXt1DbsHPLbpNMl138wrbKmzs0U89b487LT3X/a7nlZnHxoAScP6uw+36/AOrasqpbDe+SFBYlIHdtmcPuEwdx46gDG9I8eBHRq7AC9fLZJdVoQ6TFmDB0QoLoAABrdSURBVJ93ZA/at0knMy1Am4ygm4oKVneDs+NeQU4mmelBjLFaXKc/PJ3fvb2YHT5pujedNhCAT+3X6pybSVZa/EDgjLdMODw6jTEvO53xQ7tyRK98lmwqi9nVF++i5hc8NpdWsnCDf0qun8O65/K9I3vQvm0Ge6pDYd17gJsuDDCwSzsesHctdDitImddpQ5tM/jhsb159JIjefryYzh1cBeW3j2eab86if9ePco9nzYZQTIbeP/uPe9wxg3qTLvMNESEX542kBevPI4Lj+7FsJ7hXUxvXjuG4vsmcN/5w8Jm3SdDSg4WQ30NbXNppZtt8I/pq7np9ENbslhNyqmp+3WNJNoiiNxE/YH3l/HA+8vCai/OLNqr7Au+VR+r/9nOOfUX6PcXWisveqfRb7fnDaQFJOE1ifp0bMuqkvoxno7tMnyzKo7q057Za3e63VL9CtqxYmv9oOtgu/vnd+cO5bQhXcJSA/t0rL/wiwiv/+x43luwiYy0AL9+NbxvOjsjyE9P6EssmWlBOrbNYHtFNQN9Bkpzs2O3CFb84UzSgwFqQnWUVdZy+sOfhv39MtMCDO6Ww6ziHfbFyHqNtXbf99MR6bcFOZm8fd0Y2mYGeWjKcl4usgbKO+dkhs22jdS/czt38NzpdvRyugiHdMulOlTHSzEm4f183ABufHkuAMf36+jOR5gwrFtYlpSjeFsFCzcmnuF373mHk5UepH2M7Wi9yQMnDizgvBE9uPnV+e5jbe1A4Ixx5Wal84fv1Y/5jPFk+xxd2AGAB79/BCM8XUgzJo3znQV8yTG9ueSY+rTR68cNcG9PHNuXL1dt56oT+/L4p6vDPn/JlrItAu+FyJk8cxCspxXG+cf06/7aGzG4F2vAN1Z/aryJYs619M25Vk50Z09NfY7dNbCzotq9YN12ltWn6rfoWiyRtfNe7duQFgzw/844lFvOrB+g+/dlR3P7hMHccKr1DzcmImWvc24WxfdNcCd1eVsmkX2yHdtl8n+jCrno6N4suWs8T/5opJsFlUh+t/N6h3b1CQR2iyDTp0XgvC/pwQAd2mZE9cdnZwQZ1c8aXK4zhkx7QDLWEtuDuubQNS8rbBzj1MGdwyapAZxzRHfAynp65/oxYQPQTgvLy/l9Q7rnRj3n5R2Qf+HK+rEQ5286tEf4z6/ZXtGoDD+nkucd5O1b0JaTDy2IOrZLblZUbdtJKx5pX+QbOh+A84/qySGelk7XvKxG9+mfdGhnVt9zFpPOHEzxfRMatbvd/krZFkFHT1PdWRIg1kBgS/q6eAdH9W4ft6ZWXlXL/PW7OD6iS8LJ0om8aP9nxlpmrQlfq2bhhlLKq2qj+ikbGljzE1mn96uZba+ooiZUx/Wn9HcvYmlBgQS3LSiMqC31bG/VUK89uT9z1tUvQ5GXnc5PT+iLMYZBXXM5urA9t7+xMO5r//ni4fTq0CZu3nZ2RpDThnRh1T1nsXl3pe+FMdLx/Toyd/0u3wuEEwgSWUwuslsvKy3I6UO68ofvDeXsw7vzwWKr1eXMQh7WMy9sSQPvIOUrV42irLKGcYO7RP2eBy88gmtP7u8GLqfLJCcrLexvet95h3PLawvcVorTpRZLQY7/++oEgtMGdw3rCtoYY/5FpMHdclmyqdSt5LX3pEzfPmEwXXKzmGZ3g+VmpVFaWesO4h/ZO5/OOZl8sGgLbewL8KXH9mbcoM6+rZ9kifd/nkwpGwi65WVx/Sn9ed3Tl32gBIIPF21mZUk5h/fI4//+NYtbzxrExLH9Yh5/08tz+XDxFmbdNo7OOfUXJKdFEJkh9JsYF8IbXvyGv106wu3nDNWZqPVwAI45pENUIAEr4Pz9k5VURVyo/FoP7y2wLlbeWqi3WyQYkLgD2H06hvczey+uft0rIsIxh3SI+Xpe5w7vEXY/MnsoUiJBAODGUwdyWPc8ThoYXTPtZs+O9nZNvTTxuISSGbLSAwQCwg+PtbKQnFbFZLsbLjIl1OmGAuK+J+nBQFjrJT/burDmt0kPq20P7ZHHVSf2dVsQsYLZ/64eRUFOZtScFrA+B2cc1pV1O/YwYVhXd74DWGnC3+70DwT3nnc4w3rm8dmKbVx5Qt+weSreLsm87AxyMuvLPLBLDkVrd7rHvP6z0VTVhhj62w/cuRQi0qxBoCWlbCAQEX55+qFsK69yJxMdIHGAif+x8tP/eIE1iLW0gWaxM4mqrLKWzp5rltMi2FMdoq7ONFjbmLp0K1f/ZzZPXX6M9fs/WOYu9+A1qGuObyB4bNpK/v7JqrDH0oPC2AEFfHd4d96YuzHqZ7yBwLs+fNfcrLiLqPWOaBF09FxcnAth2xgZML/9zpCY/ceRFt91RpOtwZ+RFoi5VkyhHdi8efrH9Y2eS+AncoAy8r5zrs6Eu4bSFt++boxvjnv7ttbP5WdnhHWhdc7JZNKZg8OOfef6MW622QMXDGNPVa3b1RJp8V1nIAjZGUF+PX5Q2GKDI3rns3pbRdT8BcehXXM4rHseh3WPzuX3XsTbt0mnbWb9+zK4Wy5Fa3eGZZplpgV5aeJx9C+IH/gPRikbCBw9Pal+B0qLwOGUpybBAdTIyTfeKfsV1bVRue2OnKw0fn7KAP7w3hK36QzwSpH/YN9Qn386qJ9j4GjfJp2i208jGBAeufhIvi7eGXVxDwsEntVBu+XVB4KrxvZ1l/R19GyfTUCstYV+dfrAsAtTyK5VD/AZlAW4fPQhvo/7aZPRPP8iTmBzViZtjPFDu4bdd9IOHXd/dygnDOzEYd3zeOObDVwUMfEu0uE9/f++HdpatWenRfGvH4/k31+s8d0zYmiPPP579SimLd3KhSP9f193uyUV+R57uyd7d2jjW4F45apRGGPiph17+9jz22SEpcY6LZrOEd1UR/VJrNV4sNFA0L6+1tBU3XPvL9zEk5+t4X9Xj2p0WqSXk7UQOY0+kvMrIvOlvd0KFVUhcrLSfWf3ds7J5Mqxffm6eEfY2ip52em+aYcj+uRHPQbhqX+//+5QzhnePaw2nZke3WXgnch22pAuPP1lMRPH9mX3nhqK1u7k5+MGcNNpAzl7WHeqQyHO/7uVZ56ZFqRH+2yGds/julMGhL3mwM45XDW2L5eNLvQt54HIaREkunYNwM3jD+X/jusTFeAjWwQFOZlut9HwXv5/u0R0iJhhO25wF9+xBcfRhR3crJpI8+44PawF6OX8z/QtaEs3T63+J6MP4d9frGHeb0+PGthuSF52etj/97l2emnbJG1V2tqk/LvgzT5pqhbBNc/PwRhrNmesrQaXbymjS25W3A+0U8OevHAzb87dENV3DdYqkc5a8ZGLcXlnVDr99H57zjoNjnaZaWEDkc7YQpuMYFi6YmT/vMM7U/uInvlRXRBO3/3d3x3KiN75/OuzNZwwoL6//PYJg7lizCH06tCGz1ds4+Wi9Vw4sidQX0u98dQBDLLX8Xn4wuG+718gIEw6a3DU4wey9m3SOevwrmGphbGMHVjA9OUlMZcc8Abcxl4w45bR7n7bhxXLo+Q1sKbRzFvH0TYzjS89ezvcPmEwd3yn8TvGQf2S020zgnx/ZC/6FbRrcFA7laR8IIjVfeC3QFqinEpudSh6S8F12/e4yx4P6prD+zeOjfp5Ees1Nnpqhze8NNc3EJz8p0/c27/67zzum7yUz399CvdOXhLWDVMRsTSvl5Njn5URrF+qt7qWrWVVTBzbl+8d2YMz//yZe3x60Orr7tQ2I2wXMe9+wd6loR1O333Hthkc1j2Phy4K3xozLRhwB33HDOhE8X0Tol7jxlMHurdj9Tm3RiLC3354VELHPnXZ0XEH0r2TwqbcFP352lfO7OX9WaAwUU7f/amDuzCoaw6j+nXcp4yaN68dHbaD3KK7xjdZGQ8mSQsEItILeBboCtQBTxhj/hxxzEnAm8Aa+6HXjDF3JatMfrz9kU5Xyitfr+fmV+fz9W2nxkx1S0R1bR1E/HjR2vpB1qUxtsHLzUpn996asJU5E0krrDPWujGTF27iqS+KAasGVFEdcgOB3wYrzqYk2elBKu2gsMFuZRzWPdc3y+OxH4wACAsE3lRTv59xziEyRVU1TjAgcSspTosgJystLItsf6XbXTnNEAdcgYAw+YYT9rmL9Yhe+RyxH91hqSKZE8pqgV8aYwYDxwHXiohfu+4zY8xw+6tZg0CkiuoQf/14hTvL8Og/fMQnEVvTNYZf6mUi475O+triTfW51G0zgsxbvyssKyfWhjDefn5nIK+sqpYXZ61j/CP1NfvIFS6z04PsqQlhjHFbE93zs+MuRDZj0jh+Pm5A1ON+/7iNmTCm9p3TBde2iQe6nYlglxwbf7C5qe3POJtKTNL+M40xm4wxc+zbZcASILpv4wDw1nWj6Vdg9Xv/6cPlYc/57fSUqPP+9iWfrQjf8s9vhyhjTNhOXkH7g+/t82+bmca5j33B/e8vdaeux1o866Ep9efgTJz70wfLmBSxZO/o/p0IiJWiB9aEoVCd4doX5rj7/HbPzyY3K503rh3t+7u65mVxdGF95sZXk05h6i9P9D12mL1KY6Kpm2rfOC2vWIP6+6pzjjUL+3tH9mzS11Utr1mqaCJSCBwJzPR5epSIzBORySJyWIyfnygiRSJSVFLiv5fq/hjWMz8qBc/hlzXTGHPWhq/V41eHf2PuBk558FN3YMxvgw5v7W5LaSX3vreEL1fGXtLX4eRSr9hazjjPgmo/O6kfV43tx9K7z+S/Vx8P1KfbvbdgM3e9s5iAQBe7ayxetol3wlK3vOyYg3C/On0gL088LmZ6omoaXXKzeOHKY/nT949o6aKoViLpgUBE2gGvAjcaYyKXEJwD9DHGHAH8BXjD7zWMMU8YY0YaY0YWFETPymwKV5/Yj/NGWA0W7wXTb3N1r73VIT5YtDn283ZXC1ibu/jttOVM///FK3PdzV9yIvrR23gmw0xfvo3Hp6/ml/+dB8CHvxjrLgscybtMwrF96wdXLz66N8GAkJEWcPubI2eg5rfJSGjVQyeANJShkhYMcGyCk6TU/jm+X6dmmwOhWr+kflJEJB0rCDxvjHkt8nlvYDDGvCcifxORTsYY/01qkygnK50Hzh/GkG65nDq4C1OXWmMD1Q1sn3fXO4t4cdZ63rl+DEN75Pnm8g+8fTLDeuYze+3OGK9icWZP7t5bQ8/22WF5+d513FdFbAjfqV0m+TEuwt7t8Hq1b+NmJHlnWTqyM+ov+od0astxfRPLynH6/hva5lApdWBKWotArBGefwFLjDEPxTimq30cInKMXZ6G+zuSJC0Y4Kcn9I1aOybWKo6AuyritvIqZq3ZwfC7poQ9v7c6RE3IxAwCxhg3Q8crXj+6d9/bYEDIz053U/uuOjF8KeRunnPp1aENd9ubofjNMnZaBN3zspj2q5O497xhUcf4cZYj8FujXil14Etmi2A08H/AAhGZaz92K9AbwBjzD+AC4BoRqQX2Ahcbv9HUZha5FPAXK7dx0yvz+P13h9KvoB2bS/e6A2ZOPvfGXZXc+vqCqNcq90nX9Drt4em+2TTxatfeQNCpXQaBgLgpmdnpQd64djTffewLwFqzx9lQvVeHNgztkcelx/XxfV2niyfW8rddc7MY0CW6/797fjaf3XwyPVJkgS6lDjZJCwTGmM+BuHlfxpi/An9NVhn2lTddLSDw4aItADw+fRXrd1i19+8d2ZP53+5y5x4sirFxxpYGlgxYubXc9/F8T4vgxIEFfLq8fpDcu7evs+SuE0xCdSZsYLdLbhYvXzWKKYs3N9iH7/QpZ8YIBDNuHRfzZ5O5n6pSKrk0sbsBXXOz3N2RvBuCzFi9nXP++oW721VRsX/Xj3e2bTyR+5x6l/l10gHH9O/Eq9ccH3acMxPYWbclcpPybnlZDO+Vz/87YxANceYoZfusCaSUOnjpf3wD8ttkuGv5eFNJI2vyy3yygSJ/JvbvCF/f3fm9DicQHNo1h6P6tGfswAJ3tuRR9uqLzqbazgJ1Xe0p+o0ZwHXSVptzZySlVMvT/LIGxLqQLvHM+h3RO9/dgnFfZKUFowZvvVlAmXa3jzOY++xPrP0CtpVXufMLxg4s4N7JS90tF1+/9njWbKto1KzMows78L0je7ibmiulUoMGghg+/MVY6ozhL1NX+j6/0N4M5rLjC6mqDe1fIEgPRG0E4rceT+RSD959lwd3yw1bpK1bXjbd8ho3eJuRFuDhiIXglFIHP+0aimFglxwGdc2NuVzuwg27GdojlzvPOSxuqueLns25czLTwjKSfmqPC2SlR7cIvIvMORutRE74UkqppqCBoAFO331kSmmozrh7GfittOkY1a+juyhbbnY6C+48w33OqeFnpgejWgTeJXedFNV4i78ppdS+0kDQAGfDbr+dlob2sNbMcVoER/TM44WfHht1nFOTb5eZRkZagCtPOIS0gLiDsmkBiQoE5ZX18w+cQNBGA4FSKgk0EDTA2dqxf+d2vHrNKKb96iT3ueH2aprOgHJmetB33wBnD1mnRn/bhCGsvOessFZGRsSkstLKGk4f0oXLji90A0HkMUop1RT0ytKAQzpZM2lPG9KFo/p04JBObbn3vMPJb5POMDuF05kLnZUeZGiPvLBlmaG+RRBZo/dO3HL2Zr3c3mf3pEMLeOJHI7nznMPcQLCvO6YppVQ8mjXUgLMO78rnvz6Znu3rZ85eckxvLj66l5uaefQhHejfuR03n3EoWelB/nv18RTe8q57fFaMQJBltwgEq9vIyfr57XfCV+N2tgbUQKCUSgYNBA0QkbAg4H3ckZedzkc3hW/GMunMQQzsGr4fcuRErVhLOURyJgvvy56tSinVEA0ESXLVif3c284yEJFbB7otggau7/bqEaQHtCdPKdX09MrSDJwxhPYRaaaJtgjuPW8YPxl9SML7AyilVGNoi6AZnDeiB+t27OH6U/qHPZ5uV/Ul/iKtdM3L4o7vDEla+ZRSqU0DQTPISg9yy5nRq3+6AUC7/pVSLUi7hlqQsbey1ziglGpJGghaUFogfKKZUkq1BO0aakEj+7TnupP786Pj/beOVEqp5qCBoAUFAsKvzji0pYuhlEpx2jWklFIpTgOBUkqlOA0ESimV4jQQKKVUitNAoJRSKU4DgVJKpTgNBEopleI0ECilVIoT46yR3EqISAmwdh9/vBOwrQmL0xroOacGPefUsD/n3McYU+D3RKsLBPtDRIqMMSNbuhzNSc85Neg5p4ZknbN2DSmlVIrTQKCUUiku1QLBEy1dgBag55wa9JxTQ1LOOaXGCJRSSkVLtRaBUkqpCBoIlFIqxaVEIBCR8SKyTERWisgtLV2epiIi/xaRrSKy0PNYBxGZIiIr7O/t7cdFRB6134P5IjKi5Uq+70Skl4hME5ElIrJIRG6wHz9oz1tEskRklojMs8/5d/bjh4jITPucXxaRDPvxTPv+Svv5wpYs//4QkaCIfCMi79j3D+pzFpFiEVkgInNFpMh+LOmf7YM+EIhIEHgMOBMYAlwiIkNatlRN5mlgfMRjtwBTjTEDgKn2fbDOf4D9NRH4ezOVsanVAr80xgwGjgOutf+eB/N5VwGnGGOOAIYD40XkOOB+4GH7nHcCV9jHXwHsNMb0Bx62j2utbgCWeO6nwjmfbIwZ7pkvkPzPtjHmoP4CRgEfeO5PAia1dLma8PwKgYWe+8uAbvbtbsAy+/bjwCV+x7XmL+BN4LRUOW+gDTAHOBZrhmma/bj7OQc+AEbZt9Ps46Sly74P59rTvvCdArwDSAqcczHQKeKxpH+2D/oWAdADWO+5/6392MGqizFmE4D9vbP9+EH3PtjN/yOBmRzk5213kcwFtgJTgFXALmNMrX2I97zcc7af3w10bN4SN4lHgJuBOvt+Rw7+czbAhyIyW0Qm2o8l/bOdCpvXi89jqZgze1C9DyLSDngVuNEYUyrid3rWoT6PtbrzNsaEgOEikg+8Dgz2O8z+3urPWUTOBrYaY2aLyEnOwz6HHjTnbBttjNkoIp2BKSKyNM6xTXbOqdAi+Bbo5bnfE9jYQmVpDltEpBuA/X2r/fhB8z6ISDpWEHjeGPOa/fBBf94AxphdwCdY4yP5IuJU5rzn5Z6z/XwesKN5S7rfRgPniEgx8BJW99AjHNznjDFmo/19K1bAP4Zm+GynQiD4GhhgZxtkABcDb7VwmZLpLeDH9u0fY/WhO4//yM40OA7Y7TQ3WxOxqv7/ApYYYx7yPHXQnreIFNgtAUQkGzgVawB1GnCBfVjkOTvvxQXAx8buRG4tjDGTjDE9jTGFWP+zHxtjfshBfM4i0lZEcpzbwOnAQprjs93SgyPNNABzFrAcq1/1tpYuTxOe14vAJqAGq3ZwBVa/6FRghf29g32sYGVPrQIWACNbuvz7eM5jsJq/84G59tdZB/N5A8OAb+xzXgjcYT/eF5gFrAT+C2Taj2fZ91faz/dt6XPYz/M/CXjnYD9n+9zm2V+LnGtVc3y2dYkJpZRKcanQNaSUUioODQRKKZXiNBAopVSK00CglFIpTgOBUkqlOA0EStlEJGSv+uh8NdlKtSJSKJ5VYpU6kKTCEhNKJWqvMWZ4SxdCqeamLQKlGmCvEX+/vSfALBHpbz/eR0Sm2mvBTxWR3vbjXUTkdXv/gHkicrz9UkERedLeU+BDe5YwIvJzEVlsv85LLXSaKoVpIFCqXnZE19BFnudKjTHHAH/FWvMG+/azxphhwPPAo/bjjwKfGmv/gBFYs0TBWjf+MWPMYcAu4Hz78VuAI+3XuTpZJ6dULDqzWCmbiJQbY9r5PF6MtTHManvBu83GmI4isg1r/fca+/FNxphOIlIC9DTGVHleoxCYYqzNRRCRXwPpxpjfi8j7QDnwBvCGMaY8yaeqVBhtESiVGBPjdqxj/FR5boeoH6ObgLVmzFHAbM/qmko1Cw0ESiXmIs/3r+zbX2KtjAnwQ+Bz+/ZU4BpwN5TJjfWiIhIAehljpmFtwpIPRLVKlEomrXkoVS/b3gXM8b4xxkkhzRSRmViVp0vsx34O/FtE/h9QAlxuP34D8ISIXIFV878Ga5VYP0HgORHJw1pN8mFj7TmgVLPRMQKlGmCPEYw0xmxr6bIolQzaNaSUUilOWwRKKZXitEWglFIpTgOBUkqlOA0ESimV4jQQKKVUitNAoJRSKe7/A7mtO9Hx6fddAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(1,len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 그래프는 범위가 크고 변동이 심하기 떄문에 보기가 좀 어렵다.**\n",
    "\n",
    "> **곡선의 다른 부분과 스케일이 많이 다른 첫 10개의 데이터 포인트를 제외시킨다.**\n",
    "\n",
    "> **부드러운 곡선을 얻기 위해 각 포인트를 이전 포인트의 지수 이동 평균(exponential moving average)으로 대체한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor = 0.9):\n",
    "    smoothed_points = []\n",
    "    \n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    \n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_mae_history = smooth_curve(average_mae_history[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이 그래프를 보면 검증 MAE가 80번째 에포크 이후에 줄어드는 것이 멈추었다. 이 지점 이후로는 과대적합이 시작된다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델의 여러 매개변수에 대한 튜닝이 끝나면(에포크 수뿐만 아니라 은닉 층의 크기도 조절할 수 있다.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model() # new compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 49us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.583094596862793"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_targets, # total train_set\n",
    "         epochs = 80, batch_size = 16, verbose = 0)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data,test_target)\n",
    "\n",
    "test_mae_score # 아직 2,843달러 정도 차이가 난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.6.5 정리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **회귀는 분류에서 사용했던 것과는 다른 손실 함수를 사용한다. 평균 제곱 오차(MSE)는 회귀에서 자주 사용되는 손실 함수이다.**\n",
    "\n",
    "> - **비슷하게 회귀에서 사용되는 평가 지표는 분류와 다른다. 당연히 정확도 개념은 회귀에서 적용되지 않는다. 일반적인 회귀 지표는 평균 절대 오차(MAE)이다.**\n",
    "\n",
    "> - **입력 데이터의 특성이 서로 다른 범위를 가지면 전처리 단계에서 각 특성을 개별적으로 스케일 조정해야 한다.**\n",
    "\n",
    "> - **가용한 데이터가 적다면 K-겹 검증을 사용하는 것이 신뢰할 수 있는 모델 평가 방법이다.**\n",
    "\n",
    "> - **가용한 훈련 데이터가 적다면 과대적합을 피하기 위해 은닉 층의 수를 줄인 모델이 좋다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.7 요약**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **보통 원본 데이터를 신경망에 주입하기 전에 전처리해야 한다.**\n",
    "\n",
    "> - **데이터의 범위가 다른 특성이 있다면 전처리 단계에서 각 특성을 독립적으로 스케일 조정해야 한다.**\n",
    "\n",
    "> - **훈련이 진행됨에 따라 신경망의 과대적합이 시작되고 새로운 데이터에 대해 나쁜 결과를 얻게 된다.**\n",
    "\n",
    "> - **훈련 데이터가 많지 않으면 과대적합을 피하기 위해 1개 또는 2개의 은닉 층을 가진 신경망을 사용한다.**\n",
    "\n",
    "> - **데이터가 많은 범주로 나뉘어 있을 때 중간층을 너무 작으면 정보의 병목이 생길 수 있다.**\n",
    "\n",
    "> - **회귀는 분류와 다른 손실 함수와 평가 지표를 사용한다.**\n",
    "\n",
    "> - **적은 데이터를 사용할 때는 K-겹 검증이 신뢰할 수 있는 모델 평가를 도와준다.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
